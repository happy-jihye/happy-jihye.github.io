---
title: "[Paper Review] StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery ë…¼ë¬¸ ë¶„ì„"
excerpt: "textë¡œ ì´ë¯¸ì§€ë¥¼ ì¡°ì‘í•˜ëŠ” StyleCLIPì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤."


categories:
 - GAN
tags:
  - deeplearning
  - ai
  - GAN
  - vision
search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 

use_math: true
---

> âœğŸ» ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” textë¡œ ì´ë¯¸ì§€ë¥¼ ì¡°ì‘í•˜ëŠ” **StyleCLIP**ì— ëŒ€í•´ ì‚´í´ë³¸ë‹¤.


- Paper : [StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery](https://arxiv.org/abs/2103.17249) (arXiv 2021 /Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski)

- [Official Github](https://github.com/orpatashnik/StyleCLIP)

- [GAN-Zoos! (GAN í¬ìŠ¤íŒ… ëª¨ìŒì§‘)](https://happy-jihye.github.io/gan/)

---

## 1. Introduction

ê·¸ë™ì•ˆì˜ Image manipulationì€ ì‚¬ìš©ìê°€ ì§ì ‘ ì˜ë¯¸ìˆëŠ” latent spaceë¥¼ ì°¾ì€ í›„ sementic directionì„ ì°¾ì•„ controlí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆë‹¤. 

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/styleclip-1.PNG?raw=1' width = '800' ></p>

**StyleGLIP**ì€ StyleGANê³¼ CLIP modelì„ ê¸°ë°˜ìœ¼ë¡œ textê¸°ë°˜ì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œ ëª¨ë¸ì´ë‹¤. ì´ì „ ëª¨ë¸ë“¤ë³´ë‹¤ í›¨ì”¬ ì§ê´€ì ì´ë©°, latent spaceë¥¼ ì¼ì¼ì´ ì°¾ì§€ ì•Šì•„ë„ textì— ë”°ë¼ ì´ë¯¸ì§€ ì¡°ì‘ì´ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ë°œí‘œë˜ìë§ˆì ë§ì€ ì£¼ëª©ì„ ë°›ì€ ë…¼ë¬¸ì´ë‹¤.

- **StyleGAN**
  <p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan-2.PNG?raw=1' width = '600' ></p>
  - StyleGANì€ global attributeì™€ styleì„ ì˜ í•™ìŠµì‹œí‚¨ ëª¨ë¸ìœ¼ë¡œ, ì´ë¯¸ì§€ ìƒì„±ì— ìˆì–´ latent spaceì˜ ì—­í• ì´ ë¬´ì—‡ì¸ì§€ë¥¼ ì˜ ì„¤ëª…í–ˆë‹¤.
  - [[Paper Review] StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks ë…¼ë¬¸ ë¶„ì„](https://happy-jihye.github.io/gan/gan-6/)
  - [[Paper Review] StyleGAN2 : Analyzing and Improving the Image Quality of StyleGAN ë…¼ë¬¸ ë¶„ì„](https://happy-jihye.github.io/gan/gan-7/)


- **CLIP(Contrastive Language-Image Pre-training)**
  - [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) (2021)
  
  <p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/clip-1.PNG?raw=1' width = '700' ></p>

  - <span style='background-color: #E5EBF7;'> CLIPì€ multi-modal embedding spaceë¥¼ í•™ìŠµí•˜ì—¬ textì™€ imageì‚¬ì´ì˜ semantic similarityë¥¼ ì¶”ì •í•˜ì˜€ë‹¤. </span> ì¸í„°ë„·ì—ì„œ í¬ë¡¤ë§í•œ *400M(4ì–µê°œ)*ì˜ text-image pairë¥¼ ì´ìš©í•˜ì—¬ representational learningì„ ìˆ˜í–‰í•˜ì˜€ê³ , zero-shot image classificationì—ì„œ SOTAì˜ ê²°ê³¼ë¥¼ ëƒˆë‹¤. (ImageNetì— ëŒ€í•´ zero-shot learningë¥¼ test í•˜ì˜€ì„ ë•Œì—ëŠ” 76.2%ì˜ ë§¤ìš° ë†’ì€ ê²°ê³¼ê°€ ë‚˜ì™”ë‹¤. ì´ë¥¼ í†µí•´ í•™ìŠµ ê³¼ì •ì—ì„œ í•œë²ˆë„ ë³´ì§€ ëª»í•œ ë¬¸ì œ ë° ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ê³  ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.)

  - **Image Encoder**ë¡œëŠ” **Vision Transformer(ViT)**ë¥¼, **Text Encoder**ë¡œëŠ” **Transformer**ë¥¼ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, ì´ë¥¼ í†µí•´ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
    - Paper Review : [Vision Transformer](https://happy-jihye.github.io/nlp/nlp-9/), [Transformer](https://happy-jihye.github.io/nlp/nlp-8/)
    

## 2. Related Work

**Text-guided Image Generation and Manipulation**

- [Generative Adversarial Text to Image Synthesis](https://happy-jihye.github.io/gan/gan-4/)
  - text2imageì˜ ì´ˆê¸° ì—°êµ¬ë¡œ, conditional GANì„ ì´ìš©í•˜ì—¬ textì— ë”°ë¼ ì´ë¯¸ì§€ë¥¼ ì¡°ì‘í•˜ì˜€ë‹¤.

- [AttnGAN: Fine-grained text to image generation with attentional generative adversarial networks](https://arxiv.org/abs/1711.10485)
  - Attention mechanismì„ í†µí•´ textì™€ image ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì •ì˜í•˜ì—¬ text-to-image generationì„ ìˆ˜í–‰í•˜ì˜€ë‹¤.

- [ManiGAN: Text-guided image manipulation](https://arxiv.org/abs/1912.06203)

- [TediGAN: Text-Guided Diverse Face Image Generation and Manipulation](https://arxiv.org/abs/2012.03308)
  - textì™€ StyleGANì˜ latent spaceë¥¼ mappingí•˜ëŠ” encoderë¥¼ í•™ìŠµí•˜ì—¬ textì— ë”°ë¼ ì´ë¯¸ì§€ë¥¼ ì¡°ì‘í•˜ì˜€ë‹¤.

## 3. StyleCLIP Text-Driven Manipulation

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/styleclip-2.PNG?raw=1' width = '600' ></p>


StyleCLIPì€ Text-Driven Image Manipulationì„ ìœ„í•œ ì„¸ê°€ì§€ ë°©ë²•ì„ ì œì•ˆí•œë‹¤.

> (1) **Text-guided latent optimization** : CLIP modelì˜ loss networkë¥¼ ë„ì…í•˜ì—¬ textë¥¼ ë°”íƒ•ìœ¼ë¡œ input latent vectorë¥¼ ìˆ˜ì •í•  ìˆ˜ ìˆë„ë¡ í•¨ (StyleGANì˜ $\mathbf{W+}$ spaceì˜ Image latent vectorì™€ CLIP spaceì˜ Text latent vectorì˜ lossë¥¼ ìµœì†Œí™”í•˜ë„ë¡ optimizeë¥¼ í•¨) 
>
> *Text-guided latent optimization, where a CLIP model is used as a loss network. This is the most versatile approach, but it requires a few minutes of optimization to apply a manipulation to an image.*

 
> (2) **Latent Residual Mapper** 
>  
> *A latent residual mapper, trained for a specific text prompt. Given a starting point in latent space (the input image to be manipulated), the mapper yields a local step in latent space.*

> (3) textì— ë”°ë¼ StyleGANì˜ style spaceì—ì„œ **Input-global(agnostic) direction**ë¥¼ ì¡°ì •í•˜ëŠ” mapping network
> 
> *A method for mapping a text prompt into an inputagnostic (global) direction in StyleGANâ€™s style space, providing control over the manipulation strength as well as the degree of disentanglement.*

---

### 3.1 Latent Optimization

> 1. **Text-guided latent optimization** : CLIP modelì˜ loss networkë¥¼ ë„ì…í•˜ì—¬ textë¥¼ ë°”íƒ•ìœ¼ë¡œ input latent vectorë¥¼ ìˆ˜ì •í•  ìˆ˜ ìˆë„ë¡ í•¨ (StyleGANì˜ $\mathbf{W+}$ spaceì˜ Image latent vectorì™€ CLIP spaceì˜ Text latent vectorì˜ lossë¥¼ ìµœì†Œí™”í•˜ë„ë¡ optimizeë¥¼ í•¨) 


(1) pretrained & fixed StyleGANì˜ Generator $G$ì™€ (2) CLIPì˜ image encoderì—ì„œì˜ gradient descentë¥¼ í†µí•´ latent codeë¥¼ optimizationí•œë‹¤.   

$$\underset{w \in \mathcal{W}+}{\arg \min } D_{\mathrm{CLIP}}(G(w), t)+\lambda_{\mathrm{L} 2}\left\|w-w_{s}\right\|_{2}+\lambda_{\mathrm{ID}} \mathcal{L}_{\mathrm{ID}}(w)$$

- $D_{\mathrm{CLIP}}(G(w), t)$ : ìš°ë¦¬ê°€ ì…ë ¥í•œ *text prompt* $t$ì™€ ìœ ì‚¬í•œ ì´ë¯¸ì§€ê°€ ìƒì„±í•˜ë„ë¡ ì¡°ì‘í•˜ëŠ” **CLIP Loss**ì´ë‹¤. 
  - $G$ : pretrainëœ StyleGANì˜ Generator
  - $D_{\mathrm{CLIP}}$ : ìƒì„±ëœ ì´ë¯¸ì§€ $G(w)$ì™€ textì˜ CLIP embeddingê°„ì˜ cosine distance

- $\lambda_{\mathrm{L} 2}\left\|w-w_{s}\right\|_{2}+\lambda_{\mathrm{ID}} \mathcal{L}_{\mathrm{ID}}(w)$ : L2 Distanceì™€ Identity Lossë¡œ, ì¡°ì‘ëœ imageê°€ input ì´ë¯¸ì§€ì™€ ë¹„ìŠ·í•˜ë„ë¡ lossë¥¼ updateí•˜ëŠ” ë¶€ë¶„

$$\mathcal{L}_{\mathrm{ID}}(w)=1-\left\langle R\left(G\left(w_{s}\right)\right), R(G(w))\right\rangle$$

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/styleclip-3.PNG?raw=1' width = '600' ></p>

- optimizationì„ 200~300íšŒ ë°˜ë³µí•œ í›„ì˜ ê·¸ë¦¼. 
- $\mathcal{L}_{\mathrm{ID}}$ë¥¼ ë‚®ê²Œ ì„¤ì •í•˜ë©´ ì´ë¯¸ì§€ì˜ identityë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŒ

---

### 3.2 Latent Mapper

- **3.1 Latent Optimization**ì€ ì´ë¯¸ì§€ ì¡°ì‘ì€ ì˜ í•˜ì§€ë§Œ, ì´ë¯¸ì§€ë¥¼ ì¡°ì‘í•˜ë ¤ë©´ (image, text) single pair ë§ˆë‹¤ ëª‡ë¶„ì˜ optimizationê³¼ì •ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ì¥ì˜ ì´ë¯¸ì§€ë¥¼ í•™ìŠµì‹œí‚¤ê¸°ì—” ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ê±¸ë¦¼ 
- $\lambda$ ì™€ ê°™ì€ parameterì— sensitiveí•¨
â†’ í•œë²ˆ í›ˆë ¨ì„ í•´ë†“ìœ¼ë©´ ê·¸ ì´í›„ë¶€í„°ëŠ” inferenceë§Œ í•˜ë©´ ë˜ëŠ” **3.2 Latent Mapper**ë¥¼ ì œì•ˆ !


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/styleclip-4.PNG?raw=1' width = '700' ></p>

**Architecture**
- ì„¸ê°€ì§€ì˜ layer group : Coarse, Medium, Fine
- ê°ê°ì˜ ê·¸ë£¹ì€ ì„œë¡œ ë‹¤ë¥¸ latent vectorì™€ FC-layerë¥¼ ê°€ì§
- StyleGANì—ì„œëŠ” 8ê°œì˜ FC-layerë¥¼ í†µí•´ latent code $z$ì—ì„œ immediate latent code $w$ë¥¼ ë§Œë“¤ì—ˆë‹¤ë©´ StyleCLIPì—ì„œëŠ” 4ê°œì˜ FC-layerë¥¼ ì‚¬ìš©í•˜ì—¬ latent code $w$ë¥¼ ë§Œë“¦
$$ w = (w_c, w_m, w_f) $$

- ê°ê°ì˜ Latent vectorë“¤ì„ Mappingí•˜ì—¬ Manipulation step $M_{t}$ë¥¼ ë§Œ
  
  $$M_{t}(w)=\left(M_{t}^{c}\left(w_{c}\right), M_{t}^{m}\left(w_{m}\right), M_{t}^{f}\left(w_{f}\right)\right)$$

**Loss**
MapperëŠ” input imageì˜ global attributeë¥¼ ìœ ì§€í•˜ë©´ì„œ *text prompt*, $t$ì— ë”°ë¼ manipulationë˜ë„ë¡ í›ˆë ¨ëœë‹¤. 

$$\mathcal{L}(w)=\mathcal{L}_{\text {CLIP }}(w)+\lambda_{L 2}\left\|M_{t}(w)\right\|_{2}+\lambda_{\text {ID }} \mathcal{L}_{\mathrm{ID}}(w)$$

- **CLIP loss** 

  $$\mathcal{L}_{\text {CLIP }}(w)=D_{\text {CLIP }}\left(G\left(w+M_{t}(w)\right), t\right)$$

---

Latent Mapperì€ ì´ˆê¸° í›ˆë ¨ì‹œê°„ì´ 10ì‹œê°„ ì´ìƒìœ¼ë¡œ ì˜¤ë˜ ê±¸ë¦¬ì§€ë§Œ, í•œë²ˆ í›ˆë ¨ì‹œì¼œë†“ìœ¼ë©´ ì´ë¥¼ ì¶”ë¡ ì‹œì— ê³„ì† ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/styleclip-5.PNG?raw=1' width = '700' ></p>

í›ˆë ¨ ê²°ê³¼ë„ ì„±ê³µì ì´ë‹¤. identityë¥¼ ìœ ì§€í•˜ë©´ì„œ textì— ë”°ë¼ visual attributeë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/styleclip-6.PNG?raw=1' width = '700' ></p>

ë˜í•œ, text promptë¥¼ í†µí•´ ì—¬ëŸ¬ ê°€ì§€ì˜ ì†ì„±ì„ í•œë²ˆì— ë°”ê¿€ ìˆ˜ë„ ìˆë‹¤.

---

### 3.3 Global Directions

3.2ì˜ Latent MapperëŠ” inference timeì€ ë¹ ë¥´ì§€ë§Œ, ì„¸ë°€í•˜ê²Œ disentangle manipulationì„ ì¡°ì ˆí•˜ëŠ”ë° ë¶€ì¡±í•¨ì´ ìˆë‹¤. ë˜í•œ, text promptê°€ ì£¼ì–´ì¡Œì„ ë•Œ manipulation stepì˜ ë°©í–¥ì´ ë‹¤ì–‘í•˜ì§€ ì•Šê³  ë¹„ìŠ·í•  ë•Œë„ ë§ë‹¤.

ë”°ë¼ì„œ ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ <span style='background-color: #E5EBF7;'> text promptë¥¼ StyleGANì˜ *style space* $S$ì˜ **single, global direction**ì™€ mappingí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•˜ì˜€ë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ë‹¤ë¥¸ latent spaceë³´ë‹¤ ì¡°ê¸ˆ ë” disentangleí•´ì§€ê¸° ë•Œë¬¸ì— ì„¸ë°€í•œ ì¡°ì ˆì´ ê°€ëŠ¥í•˜ë‹¤. </span>

---

1. *style code* $s \in \mathcal{S}$ì— ë”°ë¥¸ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œë‹¤. $G(s)$
2. *text prompt*ê°€ ì£¼ì–´ì§€ë©´, CLIP text encoderë¥¼ ì‚¬ìš©í•˜ì—¬ CLIP's joint language-image embedding vectorì¸ $\Delta t$ë¥¼ êµ¬í•œë‹¤.
3. $\Delta t$ë¥¼ manipulation directionì— ë”°ë¼ $\Delta s$ vectorë¡œ mappingí•œë‹¤.
4. manipulation strengthëŠ” $\alpha$ë¥¼ ì¡°ì ˆí•¨ìœ¼ë¡œì¨ ì¡°ì •í•  ìˆ˜ ìˆìœ¼ë©°, ìƒì„±ëœ ì´ë¯¸ì§€ëŠ” $G(s+\alpha \Delta s)$ ì´ë‹¤.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/styleclip-7.PNG?raw=1' width = '700' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> ì¶œì²˜ : ë‚˜ë™ë¹ˆë‹˜ StyleCLIP ìë£Œ </p></i></font>

directionì— ë”°ë¼ ê°ê¸° ë‹¤ë¥¸ styleë¡œ imageë¥¼ manipulationí•  ìˆ˜ ìˆë‹¤.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/styleclip-8.PNG?raw=1' width = '700' ></p>

darker hairë¼ëŠ” direction($\Delta t$)ì´ ìˆì„ ë•Œ, manipulation strength $\alpha$ë¥¼ ì¡°ì •í•¨ìœ¼ë¡œì¨ ìŠ¤íƒ€ì¼ì˜ ì •ë„ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤. 

---
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/styleclip-9.PNG?raw=1' width = '700' ></p>

ë‹¤ì–‘í•œ datasetì—ì„œ global directionì„ í†µí•´ Text-Driven Manipulationë¥¼ í•  ìˆ˜ ìˆë‹¤.

## 4. Comparisons & Evaluations

<span style='background-color: #E5EBF7;'> **Text-Driven Manipulation Methods** </span>

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/styleclip-10.PNG?raw=1' width = '800' ></p>

TediGANê³¼ ë¹„êµí–ˆì„ ë•Œ StyleCLIPì´ ë” ì¢‹ì€ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œë‹¤.

<span style='background-color: #E5EBF7;'> **Other StyleGAN manipulation methods** </span>

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/styleclip-11.PNG?raw=1' width = '700' ></p>

StyleCLIPì€ ë‹¤ë¥¸ styleGAN manipulation methodë³´ë‹¤ ê°„ë‹¨í•œ ë°©ì‹ìœ¼ë¡œ latent spaceë¥¼ ì¡°ì •í•˜ì§€ë§Œ ìƒì„±ëœ ì´ë¯¸ì§€ì˜ qualityëŠ” ë¹„ìŠ·í•˜ë‹¤.

<span style='background-color: #E5EBF7;'> **Limitations** </span>

StyleCLIPì€ **pretrained StyleGAN generator**ì™€ **CLIP model for a joint language-vision embedding**ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§Œë“¤ì–´ì¡Œë‹¤. ë”°ë¼ì„œ Generatorê°€ pretrainingë˜ì§€ ì•Šì€ ì˜ì—­ì— ëŒ€í•´ì„œëŠ” ì´ë¯¸ì§€ ì¡°ì‘ì´ ì–´ë µë‹¤. ë˜í•œ, *text prompt* ì—­ì‹œ CLIP spaceì™¸ì˜ ì˜ì—­ì— mappingëœë‹¤ë©´ ì˜ë¯¸ìˆëŠ” visual manipulationì„ í•  ìˆ˜ ì—†ì„ ê²ƒì´ë‹¤.

## 5. Conclusions & Opinion

> âœğŸ» ê·¸ë˜ë„ StyleCLIPì€ ë¹„êµì  ê°„ë‹¨í•œ ë°©ì‹ìœ¼ë¡œ textì— ë”°ë¼ ì´ë¯¸ì§€ë¥¼ manipulationí•œ íš¨ê³¼ì ì¸ ëª¨ë¸ì´ë‹¤. ìµœê·¼ ë“¤ì–´ì„œëŠ” pretrained modelì„ fine-tuningí•˜ì—¬ ì˜ë¯¸ìˆëŠ” ê²°ê³¼ë¥¼ ë‚´ëŠ” ê²ƒì´ íŠ¸ë Œë“œì´ê¸° ë•Œë¬¸ì— ë” ì£¼ëª©ì„ ë°›ì§€ ì•Šì•˜ë‚˜ì‹¶ë‹¤.

> **Conclusion** : ë³¸ ë…¼ë¬¸ì€ StyleGANê³¼ CLIPì„ ê²°í•©í•œ ì„¸ê°€ì§€ì˜ image manipulation methodë¥¼ ì œì•ˆí–ˆë‹¤. CLIPì„ ì˜ í™œìš©í•˜ë©´ ì„¸ë°€í•œ controlë„ ê°€ëŠ¥í•˜ë©°, StyleCLIPì—ì„œëŠ” disentanglement ì •ë„ì™€ manipulation strengthë„ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤.

---

**Reference**
- [(CLIP) í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì´ìš©í•œ Visual Model Pre-training](https://inforience.net/2021/02/09/clip_visual-model_pre_training/)