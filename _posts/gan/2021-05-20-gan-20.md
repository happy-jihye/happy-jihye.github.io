---
title: "[Paper Review] StyleGAN2-ADA: Training Generative Adversarial Networks with Limited Data 논문 분석 및 코드 리뷰"
excerpt: " "


categories:
 - GAN
tags:
  - deeplearning
  - ai
  - GAN
  - vision
search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---

<p align='right'> 
 <a href='https://github.com/NVlabs/stylegan2-ada-pytorch' role='button' target='_blank'> <img class='notebook-badge-image' src='/assets/badges/github.svg' alt='View On GitHub'> </a> 
</p>


> ✍🏻 최근에는 이미 있는 모델(`pretrained model`)을 잘 `fine tuning`하여 의미있는 결과를 내는 연구가 대세이다. (FreezeD, GANSpace, StyleCLIP 등등) 
> 
> **StyleGAN2-ADA**도 이러한 흐름에서 나온 연구로, loss function이나 network의 architecture를 건들이지 않고 이미 학습이 된 GAN을 `finetuning`하거나 training과정에서 `scratch`를 내는 식으로 학습을 한다. 또한, 적은 데이터로 학습을 해도 discriminator가 overfitting 되지 않도록  `Adaptive Discriminator Augmentation Mechanism`을 제안하였다.


- Paper : [Training Generative Adversarial Networks with Limited Data](https://arxiv.org/abs/2006.06676) (NeurlPS 2020 /Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, Timo Aila)

- StyleGAN2-ADA는 Nvidia팀의 StyleGAN, StyleGAN v2 후속 논문이다. StyleGAN 관련 리뷰는 다음 글을 참고 : 
    [`StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks`](https://happy-jihye.github.io/gan/gan-6/)
    [`StyleGAN2: Analyzing and Improving the Image Quality of StyleGAN`](https://happy-jihye.github.io/gan/gan-7/)

- [GAN-Zoos! (GAN 포스팅 모음집)](https://happy-jihye.github.io/gan/)

---



## Generation Images

truncation 없이 이미지를 생성하였다. truncation metric은 immediate latent space $W$에서 한다. 이 metric을 사용하면 다양성은 떨어지지만, quality가 높은 이미지를 생성할 수 있다.
  
```bash
# Generate curated MetFaces images without truncation (Fig.10 left)
$ python generate.py --outdir=out --trunc=1 --seeds=85,265,297,849 \
    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl
```

| seed 85  | seed 265  | seed 297  |  seed 849  |
| ---- | ---- | ---- | ---- |
| <img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan2-ada-ge1.png?raw=1' width = '700' >     |  <img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan2-ada-ge2.png?raw=1' width = '700' >     | <img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan2-ada-ge3.png?raw=1' width = '700' >     | <img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan2-ada-ge4.png?raw=1' width = '700' >     |


```bash
# Generate uncurated MetFaces images with truncation (Fig.12 upper left)
$ python generate.py --outdir=out --trunc=0.7 --seeds=600-605 \
    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl
```
<script src="https://gist.github.com/happy-jihye/b9849c7432375bb2595b436fa375c760.js"></script>

- 또한, pickle file은 세가지의 network, `G`, `D`, `G_ema`를 포함하고 있다. 
  - `G_ema`는 Generator weight의 평균을 exponential moving한 것으로, EMA(Exponential Moving Average)의 방식을 활용하면 GAN을 안정적으로 학습할 수 있다.


- stylegan2-ada에서는 conditional generation도 가능하다. (stylegan2는 x)
- class label을 또다른 mapping network에 넣어 embedding 한 후에 그 embedding을 $w$와 concate하는 방식으로 진행된다.

```bash
# Generate class conditional CIFAR-10 images (Fig.17 left, Car)
$ python generate.py --outdir=out --seeds=0-35 --class=1 \
    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/cifar10.pkl

# Style mixing example
$ python style_mixing.py --outdir=out --rows=85,100,75,458,1500 --cols=55,821,1789,293 \
    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl
```

---
## Projecting images to latent space

`projector.py` 함수를 사용하여 원하는 이미지의 latent vector를 구할 수 있다.

```bash
$ python projector.py --outdir=out --target=~/mytargetimg.png \
    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl
```

|                         target image                         |                        rojection imag                        |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan2-ada-target.png?raw=1' width = '700' > | <img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan2-ada-proj.png?raw=1' width = '700' > |




<script src="https://gist.github.com/happy-jihye/1b62d46fededfd0cf8df305f3093faa2.js"></script>

위에서 생성한 latent vector를 바탕으로 이미지를 생성할 수도 있다.
```bash
$ python generate.py --outdir=out --projected_w=out/projected_w.npz \
    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl
```

## Training new networks

```bash
# Train with custom dataset using 1 GPU.
# dry-run : 중간에 error가 없는지 확인하기 위한 용도
$ python train.py --outdir=~/training-runs --data=~/mydataset.zip --gpus=1 --dry-run
$ python train.py --outdir=~/training-runs --data=~/mydataset.zip --gpus=1

# Train class-conditional CIFAR-10 using 2 GPUs.
$ python train.py --outdir=~/training-runs --data=~/datasets/cifar10.zip \\
    --gpus=2 --cfg=cifar --cond=1

# Reproduce original StyleGAN2 config F.
$ python train.py --outdir=~/training-runs --data=~/datasets/ffhq.zip \\
    --gpus=8 --cfg=stylegan2 --mirror=1 --aug=noaug

# Transfer learn MetFaces from FFHQ using 4 GPUs.
$ python train.py --outdir=~/training-runs --data=~/datasets/metfaces.zip \\
    --gpus=4 --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10

```

```
Base configs (--cfg):
    auto       Automatically select reasonable defaults based on resolution
                and GPU count. Good starting point for new datasets.
    stylegan2  Reproduce results for StyleGAN2 config F at 1024x1024.
    paper256   Reproduce results for FFHQ and LSUN Cat at 256x256.
    paper512   Reproduce results for BreCaHAD and AFHQ at 512x512.
    paper1024  Reproduce results for MetFaces at 1024x1024.
    cifar      Reproduce results for CIFAR-10 at 32x32.


Transfer learning source networks (--resume):
    ffhq256        FFHQ trained at 256x256 resolution.
    ffhq512        FFHQ trained at 512x512 resolution.
    ffhq1024       FFHQ trained at 1024x1024 resolution.
    celebahq256    CelebA-HQ trained at 256x256 resolution.
    lsundog256     LSUN Dog trained at 256x256 resolution.
    <PATH or URL>  Custom network pickle.
```

### train.py

- `train.py`의 `main` function이다.

<script src="https://gist.github.com/happy-jihye/8725dbd531d3812d58963150a9fbfd58.js"></script>

- `setup_training_loop_kwargs` : `train.py`에서 여러 parameter들을 정의하는 파트이다.

<script src="https://gist.github.com/happy-jihye/51e1d6069b5f35890599d766e006df03.js"></script>

- `subprocess_fn` 함수에서 gpu의 개수에 맞게 세팅을 조정한 후, `training_loop`함수를 호출하여 학습을 시작한다.
<script src="https://gist.github.com/happy-jihye/fdff76e0c080b52de3a002ec4d3a2eb9.js"></script>

### training_loop.py

<script src="https://gist.github.com/happy-jihye/d2e5db5b821143bb1ddb870be8102d1e.js"></script>