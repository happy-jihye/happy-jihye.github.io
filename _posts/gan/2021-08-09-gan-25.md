---
title: "2021 생성모델 연구 동향 및 주요 논문 / AI Content Creation: Deep Generative Model"
excerpt: "최신 GAN Architecture부터 "


categories:
 - GAN
tags:
  - deeplearning
  - ai
  - GAN
  - vision
search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---

<p align='center'>
  <iframe src="https://www.youtube.com/embed/rfx3whKgFVo"
    frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen style="width: 42.5em; height: 25em;"></iframe>
</p>

# Progress for Image Generation

## GAN-based Model

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-1.png?raw=1' width = '800' ></p>

- `GAN`: Generative Adversarial Networks (NIPS 2014) : [arxiv](https://arxiv.org/abs/1406.2661), [review](https://happy-jihye.github.io/gan/gan-1/)

- `DCGAN`: Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (ICLR 2016)  : [arxiv](https://arxiv.org/abs/1511.06434), [review](https://happy-jihye.github.io/gan/gan-2/)

- `PG-GAN`: Progressive Growing of GANs for Improved Quality, Stability, and Variation (ICLR 2018) : [arxiv](https://arxiv.org/abs/1710.10196), [review](https://happy-jihye.github.io/gan/gan-5/)

- `BigGAN`: Large Scale GAN Training for High Fidelity Natural Image Synthesis (2019) : [arxiv](https://arxiv.org/abs/1809.11096) 

- `StyleGAN`: A Style-Based Generator Architecture for Generative Adversarial Networks (CVPR 2019) : [arxiv](https://arxiv.org/abs/1812.04948), [review](https://happy-jihye.github.io/gan/gan-6/)

- `StyleGAN v2`: Analyzing and Improving the Image Quality of StyleGAN (2020) : [arxiv](https://arxiv.org/abs/1912.04958), [review](https://happy-jihye.github.io/gan/gan-7/)

- `StyleGAN-ADA`: Training Generative Adversarial Networks with Limited Data (NeurlPS 2020) : [arxiv](https://arxiv.org/abs/2006.06676)  : review [#01](https://happy-jihye.github.io/gan/gan-19/), [#02](https://happy-jihye.github.io/gan/gan-20/)

---
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-2.png?raw=1' width = '800' ></p>

- `NeRF`: Representing Scenes as Neural Radiance Fields for View Synthesis (ECCV 2020) : [arxiv](https://arxiv.org/abs/2003.08934), [project page](https://www.matthewtancik.com/nerf)
  - 여러 각도에서 촬영한 이미지들을 input으로 사용하여 새로운 view에서의 이미지를 만들어낸 모델
- `DALLE`: Zero-Shot Text-to-Image Generation (ICML 2021) : [arxiv](https://arxiv.org/abs/2102.12092), [project page](https://openai.com/blog/dall-e/)
  - `Text-to-Image Generation` : OpenAI에서 공개한 모델로, text로 부터 이미지를 생성하는 모델 (transformer 기반)

---

## Generative Adversarial Networks (GANs)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-3.png?raw=1' width = '800' ></p>

- `GAN`: Generative Adversarial Networks (NIPS 2014) : [arxiv](https://arxiv.org/abs/1406.2661), [review](https://happy-jihye.github.io/gan/gan-1/)
- Ian-Goodfellow가 2014년에 공개한 모델로, 현재 대부분의 생성모델은 GAN의 network를 따르고 있음
- `Generator`가 random vector `z`로 부터 fake image `G(z)` 를 생성하면, `Discriminator`가 생성된 이미지가 진짜인지 가짜인지를 판별
- `G` 와 `D` 가 싸우면서 학습하는 방식

## How to Steer Neural Image Generation?

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-4.png?raw=1' width = '800' ></p>

>침대라는 이미지를 생성한다고 할때, 다양한 스타일의 침대나 다양한 각도에서 찍은 이미지를 생성하려면 어떻게 할까?
> → latent space나 conv filter를 조절 !

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-5.png?raw=1' width = '800' ></p>

- Deep Generative Model은 Latent vector를 Convolutional Neural Network의 input으로 넣어 이미지를 생성
- 이미지의 주요 feature는 **(1) Conv filters와 (2) latent space**에서 결정됨
- 이 두가지의 요소에 따라 이미지가 어떻게 변화하는지를 이해한다면, 원하는 대로 이미지를 editing할 수 있을 것 !

# Interpretation Approaches

> - **Supervised Approach** : Label이나 훈련된 classifier로 Generator가 이미지를 잘 생성하고 있는지에 대한 ground truth를 제공하는 방식
> - **Unsupervised Approach** : Label이나 훈련된 classifier없이 Generator를 학습
> - **Zero-Shot Approach** : align language embedding with
generative representations

---

## 1. Supervised Approach

>  use labels or trained classifiers to probe the representation of the generator

### 1.1 Manipulating Conv filters

#### GAN Dissection

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-6.png?raw=1' width = '800' ></p>


- `GAN Dissection`: Visualizing and Understanding Generative Adversarial Networks (ICLR 2019) : [arxiv](https://arxiv.org/abs/1811.10597), [project page](https://gandissect.csail.mit.edu/)
- Supervised Aprroach의 초기 연구
- GAN의 feature map과 이미지의 semantic segmentation이 matching되도록 학습
- 특정 feature map이 이미지 내에 어떤 object를 생성하는지를 연구하였음.
- interactive하게 이미지 내의 특정 object를 지울 수도 있고, 생성할 수도 있음.


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-7.png?raw=1' width = '800' ></p>

예를 들어, 교회 이미지가 생성됐을 때
1. 같은 object끼리 grouping(b). (ex. 나무는 나무끼리 grouping) object unit의 featuremap이 semantic segmentation과 match되도록 해야함.
2. 이미지에서 특정 object를 사라게 하거나(c), 다시 생성할 수 있어야 함(d)
3. Generator가 배경과 object 사이에 관계를 이해해야함 (ex. 건물에 문은 생길 수 있어도, 건물 object에 구름이나 나무가 있으면 안됨)

---

### 1.2 Manipulating Latent Space

> 각각의 Conv Filter이 어떤 object를 생성하는지를 찾은 후에 이를 editing하는 방식도 있지만, latent space가 disentangle하다면 이를 조절함으로써 이미지를 editing하는게 더 수월함

#### HiGAN

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-8.png?raw=1' width = '800' ></p>

- `HiGAN`: Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis (IJCV 2020) : [arxiv](https://arxiv.org/abs/1911.09267), [project page](https://genforce.github.io/higan/)
- 훈련된 classifier로 생성된 이미지의 object들을 분류한 후(category, attribute), 각각의 object들이 latent vector와 어떻게 연관이 되어있는지를 학습 → 특정 feature를 생성하는 latent vector를 조절하면서 image editing


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-9.png?raw=1' width = '800' ></p>

$$\Delta s_{i}=\frac{1}{K} \sum_{k=1}^{K} \max \left(F_{i}\left(G\left(\mathbf{z}^{k}+\lambda \mathbf{n}_{i}\right)\right)-F_{i}\left(G\left(\mathbf{z}^{k}\right)\right), 0\right)$$

**Result**
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-10.png?raw=1' width = '800' ></p>

---
#### InterFaceGAN

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-11.png?raw=1' width = '800' ></p>


- `InterFaceGAN`: Interpreting the Latent Space of GANs for Semantic Face Editing (CVPR 2020) : [arxiv](https://arxiv.org/abs/1907.10786), [project page](https://genforce.github.io/interfacegan/)
- `HiGAN` 모델을 연구한 `genforce`에서 발표한 모델
- latent vector의 특정 방향이 특정 attribute를 조절함을 찾은 후, latent manipulation을 통해 face image를 editing하는 모델 (PGGAN, StyleGAN 등에 접목 가능)

---
#### StyleFlow

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-15.png?raw=1' width = '800' ></p>


- `StyleFlow`: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows (ACM TOG 2021) : [arxiv](https://arxiv.org/abs/2008.02401), [project page](https://rameenabdal.github.io/StyleFlow/)

- StyleFlow: StyleGAN + Flow-based conditional model
  - 17가지의 face-attribute에 관여하는 특정 latent vector를 찾는 모델
  - `attribute classifier` 를 사용하여 StyleGAN을 통해 생성된 이미지의 attribute들을 뽑은 후, 이를 label로 사용하여 conditional 하게 이미지 editing에 관여하는 latent vector를 학습하여 찾음
- 기존의 latent mainpulation model들은 linear하게 latent vector를 수정함으로써 이미지를 editing 했다면, 이 모델은 **non-linear 하게 latent vector를 조절**
- 이 모델은 두가지 task를 할 수 있음 : **(1) attribute-conditioned sampling**: target attribute를 가지고 있는 high-quality 이미지를 생성 **(2) attribute-controlled editing**: real image를 target attribute를 가진 이미지로 editing

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-12.png?raw=1' width = '800' ></p>

- ⭐️ **contribution** : (1) 기존의 모델들(ex. GANSpace) 보다 성능 향상 (2) entangle한 latent space를 conditional하게 살펴봄으로써 disentangle하게 조절할 수 있도록 함
- (Fig2) left가 stylegan에서 생성된 이미지, middle이 Image2StyleGAN으로 웃는 얼굴로 editing한 이미지, right가 StyleFlow의 결과
  - StyleFlow는 non-linear path를 찾기 때문에 feature를 뽑은 후 disentangle하게 latent vector를 수정할 수 있다고 주장 

**model aritecture**
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-13.png?raw=1' width = '800' ></p>

- `CNF block` : Conditional Conitinous Normalizing Flows
- input latent vector $z_k$ 를 attribute variable $a^+_t$을 반영하도록 학습

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-14.png?raw=1' width = '800' ></p>

**Joint Reverse Encoding(JRE)**
- real image에서 시작을 한다고 가정하면, encoder를 통해 이미지를 $w \in \mathbb{R}^{1 \times 512}$ vector로 projection한 후, 생성된 이미지 $I(w)$의 `attribute classifier` $\mathcal{A}$ 로 attribute를 추정 
  - `attribute classifier`: face classifier API(MS) & lighting predction DRP network)
- 이후 reverse inference로 $w, a_t$에서 $z_0$ 을 추정

**Conditional Foward Editing (CFE)**
- image를 projection하여 얻은 고정된 $z_0$ vector에서 시작하여 target attribute $a_t'$를 반영하는 intermediate latent vector $w'$ 를 inference

---

### 1.3 Parsing 3D Information from 2D Image Generator

#### StyleGANRender

<p align='center'><img src='https://nv-tlabs.github.io/GANverse3D/figures/ICLR_teaser.png?raw=1' width = '800' ></p>

- `StyleGANRender`: Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering (ICLR 2021) : [arxiv](https://arxiv.org/abs/2010.09125), [project page](https://nv-tlabs.github.io/GANverse3D/)
- nvidia에서 낸 논문으로, 2D image를 바탕으로 3D image를 생성. SoTA inverse graphics network.
- StyleGAN으로 생성된 multi-view image를 토대로 Inverse Graphics Network를 학습한 후, 이 network로 latent code를 disentangle하게 구해 3D 이미지를 생성

<p align='center'><img src='https://nv-tlabs.github.io/GANverse3D/figures/reconstruction.gif?raw=1' width = '800' ></p>

---

### 1.4 Challenges for Supervised Approach

- How to expand the annotated dictionary size?
- How to further disentangle the relevant attributes? 
- How to align latent space with image region attributes?


---

## 2. Unsupervised Approach

> Identify the controllable dimensions of generator without labels/classifiers

### SeFA

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-17.png?raw=1' width = '800' ></p>

- `sefa`: Closed-Form Factorization of Latent Semantics in GANs (CVPR 2021) : [arxiv](https://arxiv.org/abs/2007.06600), [project page](https://genforce.github.io/sefa/), [code](https://github.com/happy-jihye/GAN/tree/main/SEFA)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-16.png?raw=1' width = '800' ></p>

- latent vector 중 특정 feature를 변화시키는 layer가 어디인지 찾은 후 이를 조절함으로써 image를 editing
- Human-in-the-loop AI content creation


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/cartoongan/sefa-pose.gif?raw=1' width = '800' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> Cartoon-StyleGAN (https://github.com/happy-jihye/Cartoon-StyleGAN) </p></i></font>

### GANspace

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-18.png?raw=1' width = '800' ></p>

- `GANSpace`: Discovering Interpretable GAN Controls (NeurIPS 2020) : [arxiv](https://arxiv.org/abs/2004.02546), [code](https://github.com/harskish/ganspace)
- [`PCA(Principal Component Analysis)`](https://darkpgmr.tistory.com/110)의 방식으로 StyleGAN의 latent space와 BigGAN의 feature space의 주요 direction을 찾음


### Hessian Penalty


- `Hessian Penalty`: A weak prior for unsupervised disentanglement (ECCV 2020) : [arxiv](https://arxiv.org/abs/2008.10599), [project page](https://www.wpeebles.com/hessian-penalty)

- `Hessian Penalty` 라는 간단한 regularization term을 도입하여 생성 모델의 입력에 대한 대각선을 유도

- **Hessian Matrix** : $i$와 $j$라는 두 attribute가 서로 disentangle하다면 $H_{ij}$는 0일 것

$$H_{i j}=\frac{\partial^{2} G}{\partial z_{i} \partial z_{j}}=\frac{\partial}{\partial z_{j}}\left(\frac{\partial G}{\partial z_{i}}\right)=0$$

- **Hessian panalty in training** : latent space가 disentangle해지도록 훈련과정에서 Hessian panalty를 추가 (만약 $i$와 $j$가 다른 방향이지만 서로 비슷한 attribute를 생성한다면, hessain penalty term의 값이 커질 것)
$$\mathcal{L}_{H}(G)=\sum_{i=1}^{|z|} \sum_{j \neq i}^{|z|} H_{i j}^{2}$$
$$\mathcal{L}_{\mathrm{G}}=\underbrace{\mathbb{E}_{z \sim p_{z}(z)}[f(1-D(G(z)))]}_{\text {Standard Adversarial Loss }}+\underbrace{\lambda \underset{z \sim p_{z}(z)}{\mathbb{E}}\left[\mathcal{L}_{\mathbf{H}}(G)\right]}_{\text {The Hessian Penalty }}$$

```python
def hessian_penalty(G, z, k, epsilon):
    # Input G: Function to compute the Hessian Penalty of
    # Input z: Input to G that the Hessian Penalty is taken w.r.t.
    # Input k: Number of Hessian directions to sample
    # Input epsilon: Finite differences hyperparameter
    # Output: Hessian Penalty loss
    G_z = G(z)
    vs = epsilon * random_rademacher(shape=[k, *z.size()])
    finite_diffs = [G(z + v) - 2 * G_z + G(z - v) for v in vs]
    finite_diffs = stack(finite_diffs) / (epsilon ** 2)
    penalty = var(finite_diffs, dim=0).max()
    return penalty
```

### EigenGAN

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-19.png?raw=1' width = '800' ></p>

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-20.png?raw=1' width = '800' ></p>

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-21.png?raw=1' width = '800' ></p>


- `EigenGAN`: Layer-Wise Eigen-Learning for GANs : [arxiv](https://arxiv.org/abs/2104.12476), [code](https://github.com/bryandlee/eigengan-pytorch)
- Design inductive bias of disentanglement in the generator
- 보통 generator는 초기에 coarse한 특징들(ex. 자세, 성별)을 학습하고, 마지막 layer로 갈수록 fine한 특징들(ex. 시선, 빛)을 학습. 이 모델은 이러한 generator의 특징을 이용하여 각 generator에 injection되는 latent vector들이 어떤 특징들을 결정하는지까지 같이 학습하겠다는 컨셉
  - stylespace 이 eigengan 모델의 컨셉을 stylegan에 적용시켰다고 보면 됨
- $t$-layer의 generator와 $t$개의 latent set $z_i$를 mapping : generator의 각 layer마다 latent vector를 injection
- [`BlockGAN`](https://github.com/thunguyenphuoc/BlockGAN), [`HoloGAN`](https://github.com/thunguyenphuoc/HoloGAN)과 비슷한 컨셉

### Challenges for UnSupervised Approach

- How to evaluate the results?
- How to annotate each disentangled dimensions?
- How to improve the disentanglement in GAN training?

---

## 3. Zero-Shot Approach

### StyleCLIP

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/styleclip-1.PNG?raw=1?raw=1' width = '800' ></p>


- `StyleCLIP`: Text-Driven Manipulation of StyleGAN Imagery (arXiv 2021) : [arxiv](https://arxiv.org/abs/2103.17249), [review](https://happy-jihye.github.io/gan/gan-15/), [code](https://github.com/orpatashnik/StyleCLIP)
- `StyleGAN`과 `CLIP`을 baseline으로 삼아 text기반의 이미지를 생성한 모델

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-22.png?raw=1' width = '800' ></p>

세가지 방식으로 CLIP 기반의 생성모델을 제안. 자세한 architecutre 설명은 [`이 링크`](https://happy-jihye.github.io/gan/gan-15/)를 참고
1. Latent Optimization
2. Latent Mapper
3. Global Direction

### Paint by Word
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-23.png?raw=1' width = '800' ></p>

- Paint by Word (2021) : [arxiv](https://arxiv.org/abs/2103.10951)
- brush로 특정 영역을 색칠한 후, text를 입력하면 이를 바탕으로 이미지를 수정 (CLIP model의 joint-embedding space를 사용)

### DALL.E

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-24.png?raw=1' width = '800' ></p>


- `DALLE`: Zero-Shot Text-to-Image Generation (ICML 2021) : [arxiv](https://arxiv.org/abs/2102.12092), [project page](https://openai.com/blog/dall-e/), [mini-dalle](borisdayma/dalle-mini)
- `Text-to-Image Generation` : OpenAI에서 공개한 모델로, text로 부터 이미지를 생성하는 모델 (transformer 기반)
- 250M(2.5억개)의 text-image pair로 학습. 12B(120억개)의 parameter

1. Train a discrete variational autoencoder (`dVAE`)
2. Train an autoregressive transformer to model the joint distribution of text and image tokens

## Latent Spaces of GAN’s Generator

GAN Inversion 및 Encoder에 관한 건 이 글 참고 : [GAN Inversion / Encoder : Image2stylegan, IDInvert, pSp, e4e](https://happy-jihye.github.io/gan/gan-23/)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-25.png?raw=1' width = '800' ></p>

- `z space` : stochastic한 분포에서 뽑은 random vector
- StyleGAN이 나오면서 latent space들이 더 확장되었음
  - `w space` : MLP를 거쳐 얻은 vector. w vector를 AdaIN 한 후, Generator의 각 layer에 injection되었었음
  - `S space` : `w`를 AdaIN한 후에 얻은 style vector (Layer-wise codes)
  - `w+ space` : GAN inversion을 하면서 도입된 space. `w vector`는 모든 AdaIN에 들어가는 vector들이 같았다면, `w+ vector`는 다름
  - `p/p+ space`

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-26.png?raw=1' width = '800' ></p>

> StyleSpace 가 가장 disentangle


- `StyleSpace`: StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation (CVPR 2021) : [arxiv](https://arxiv.org/abs/2011.12799), [code](https://github.com/xrenaa/StyleSpace-pytorch), [code2](https://github.com/happy-jihye/GAN/tree/main/StyleSpace)

- `GHFeat`: Generative Hierarchical Features from Synthesizing Images (CVPR 2021) : [arxiv](https://arxiv.org/abs/2007.10379), [project page](https://genforce.github.io/ghfeat/)


## Encoding Real Image into StyleGAN space

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-27.png?raw=1' width = '800' ></p>

> 최근에는 GAN을 기반으로 생성된 이미지를 Inversion한 후 이를 manipulation하여 image를 editing하는 연구가 대세
> 
> - [GAN Inversion / Encoder : Image2stylegan, IDInvert, pSp, e4e](https://happy-jihye.github.io/gan/gan-23/)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-28.png?raw=1' width = '800' ></p>

- 이미지 reconstruction에도 StyleSpace를 이용한게 가장 성능이 좋음
- `ALAE`: Adversarial latent autoencoders (CVPR 2020) : [arxiv](https://arxiv.org/abs/2004.04467), [code](https://github.com/podgorskiy/ALAE)
- `IdInvert` : In-Domain GAN Inversion for Real Image Editing (ECCV 2020) : [arxiv](https://arxiv.org/abs/2004.00049), [review](https://happy-jihye.github.io/gan/gan-23/), [code](https://github.com/happy-jihye/GAN/tree/main/In-Domain-GAN)
- `GHFeat`: Generative Hierarchical Features from Synthesizing Images (CVPR 2021) : [arxiv](https://arxiv.org/abs/2007.10379), [project page](https://genforce.github.io/ghfeat/)

## Applying the pretrained GAN model to image processing tasks

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ai-content-creation-29.png?raw=1' width = '800' ></p>

- GAN-Inversion을 통해 다양한 task도 가능