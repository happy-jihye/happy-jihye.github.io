---
title: "[Paper Review] CycleGAN : Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks 논문 분석"
excerpt: "unpaired dataset에 대해서도 Image-to-Image translation을 하는 CycleGAN model에 대해 알아본다."


date: 2021-04-14
categories:
 - GAN
tags:
  - deeplearning
  - ai
  - pytorch
  - GAN
  - vision
search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---

> ✍🏻 이번 포스팅에서는 unpaired dataset에 대해서도 Image-to-Image translation을 하는 **CycleGAN model**에 대해 살펴본다.


- Paper : [[CycleGAN] Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593) (ICCV 2017/ Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros)

- [GAN-Zoos! (GAN 포스팅 모음집)](https://happy-jihye.github.io/gan/)

---

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/cyclegan1.PNG?raw=1' width = '800' ></p>

그동안의 Image-to-image translation은 쌍이 있는 dataset에 대해서만 학습이 되었다. <span style='background-color: #E5EBF7;'> 하지만 실제로 많은 task들에서는 *paired training data*가 존재하지 않는다. 본 논문에서는 mapping network $G: X \rightarrow Y$와 inverse mapping $F: Y \rightarrow X$를 학습시켜 unpaired training data에 대해서도 image-to-image가 가능하도록 하였다. </span>

<span style='background-color: #E2F0D9;'> 또한, $F(G(X)) \approx X$ 의 식이 만족하도록 **cycle consistency loss** 를 도입하였으며, 이 모델은 style transfer, object transfiguration, season transfer, photo enhancement 등 다양한 task의 image-to-image 변환을 가능하게 했다. </span>

- 본 논문은 [Pix2Pix](https://arxiv.org/abs/1611.07004)의 후속 연구이다. (Image-to-Image Translation with Conditional Adversarial Networks 논문 리뷰는 [이 글](https://happy-jihye.github.io/gan/gan-8/) 참고)

## 1. Introduction

> 본 논문에서는 **(1) image collection에서 특별한 특성을 capture하는 방법**과 **(2) paired training dataset이 없어도 이 특성을 다른 image collection으로 변환하는 법**을 제안한다.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/cyclegan2.PNG?raw=1' width = '450' ></p>

paired training data를 찾는건 어렵고 cost가 많이 든다. 따라서 CycleGAN에서는 input-output의 pair 없이도 domain간 변환이 가능한 알고리즘을 찾고자 한다.

이때, 단순히 $G: X \rightarrow Y$로 mapping을 한 후, 생성된 이미지 $\hat{y}=G(x)$가 $y \in Y$와 비슷하도록 학습을 시키면 안된다. 이런식으로 학습을 시킬 경우에는 $x$가 $y$랑 잘 mapping이 될지 명확하지 않기 때문이다. 
- ✔ (문제) 대다수의 $G$가 하나의 $\hat{y}$로 mapping될 수도 있고, mode collapse로 빠질 수도 있음

따라서 본 논문에서는 translation이 반드시 **Cycle consistent**되어야만 한다는 특징을 추가하였다. (영어를 독일어로 번역할 수 있으면, 독일어도 영어로 번역할 수 있어야하는게 cycle consistent!)

$$G: X \rightarrow Y$$

$$F: Y \rightarrow X$$

또한, <span style='background-color: #E5EBF7;'> $F(G(x)) \approx x$, $G(F(y)) \approx y$의 **cycle consistency loss** </span>도 추가하였다.

## 2. Related Work

- Generative Adversarial Networks (GANs)
- Image-to-Image Translation
- Unpaired Image-to-Image Translation
- Cycle Consistency
- Neural Style Transfer
- 자세한 내용은 생략

## 3. Formulation

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/cyclegan5.PNG?raw=1' width = '700' ></p>

> **CycleGAN의 object function**은 생성된 이미지와 실제 이미지간의 loss인 **(1) Adversarial losses**와 G와 F가 잘 matching되도록 학습시키는 **(2) Cycle consistency losses**로 구성된다.

### 3.1 Adversarial Losses

- $G: X \rightarrow Y$와 $D_Y$에 대한 object function
  
$$\begin{aligned}
\mathcal{L}_{\mathrm{GAN}}\left(G, D_{Y}, X, Y\right) &=\mathbb{E}_{y \sim p_{\text {data }}(y)}\left[\log D_{Y}(y)\right] \\
&+\mathbb{E}_{x \sim p_{\text {data }}(x)}\left[\log \left(1-D_{Y}(G(x))\right]\right.
\end{aligned}$$

$$\min _{G} \max _{D_{Y}} \mathcal{L}_{\mathrm{GAN}}\left(G, D_{Y}, X, Y\right)$$

- $F$에 대해서도 유사한 adversarial loss를 적용한다.
$$\min_{F} \max _{D_{X}} \mathcal{L}_{\mathrm{GAN}}\left(F, D_{X}, Y, X\right)$$

---

### 3.2 Cycle Consistency Loss

단순히 adversarial loss만을 학습하면 mode collapse 문제가 생길 수 있다. 따라서 unpaired dataset에 대해서도 data들이 잘 mapping되도록 Cycle consistency loss(L1 loss)를 추가적으로 학습시킨다.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/cyclegan3.PNG?raw=1' width = '800' ></p>

- forward cycle-consistency에서는 $x \rightarrow G(x) \rightarrow F(G(x)) \approx x$를
- backward cycle-consistency에서는 $y \rightarrow F(y) \rightarrow G(F(y)) \approx y$를 학습한다.

- **Cycle Consistency Loss**
  
$$\begin{aligned}
\mathcal{L}_{\text {cyc }}(G, F) &=\mathbb{E}_{x \sim p_{\text {data }}(x)}\left[\|F(G(x))-x\|_{1}\right] \\
&+\mathbb{E}_{y \sim p_{\text {data }}(y)}\left[\|G(F(y))-y\|_{1}\right]
\end{aligned}$$


---

**Reconstruct Image**

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/cyclegan4.PNG?raw=1' width = '500' ></p>

> ✍🏻 CycleGAN은 unpaired dataset에 대해 학습을 시키는 모델이다. 따라서 train dataset에서 pairing을 하기 위해 A에서 B로 이미지를 생성했을 때, B에서 A로 **복원**할 수 있도록 학습을 시켰다. 
> 
> 다음 그림을 보면 input $x$에서 이미지를 생성하고$G(x)$, 다시 한번 A로 복원하는 것($F(G(x))$)을 확인할 수 있다.

---

**Result**

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/cyclegan7.PNG?raw=1' width = '800' ></p>
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/cyclegan6.PNG?raw=1' width = '500' ></p>

**GAN + forward cycle loss**나 **GAN + backward cycle loss**만을 하면 학습이 불안정해지고 mode collapse가 발생한다. 

- GAN + forward cycle loss : 
$$\mathbb{E}_{x \sim p_{\text {data }}(x)}\left[\|F(G(x))-x\|_{1}\right]$$

- GAN + backward cycle loss : 
$$\mathrm{E}_{y \sim p_{\text {data }}(y)}\left[\|G(F(y))-y\|_{1}\right]$$

따라서 양방향의 cycle loss를 통해 학습을 시켜야한다.

---

### 3.3 Full Object

$$\begin{aligned}
\mathcal{L}\left(G, F, D_{X}, D_{Y}\right) &=\mathcal{L}_{\mathrm{GAN}}\left(G, D_{Y}, X, Y\right) +\mathcal{L}_{\mathrm{GAN}}\left(F, D_{X}, Y, X\right) \\
&+\lambda \mathcal{L}_{\mathrm{cyc}}(G, F)
\end{aligned}$$

$\lambda$는 adversarial loss의 중요도에 따라 상대적으로 결정한다. 논문의 실험에서는 $\lambda = 10$의 값을 사용하였다.

$$G^{*}, F^{*}=\arg \min _{G, F} \max _{D_{x}, D_{Y}} \mathcal{L}\left(G, F, D_{X}, D_{Y}\right)$$

## 4. Implementation

Generator 모델의 baseline으로는 [Pix2pix](https://happy-jihye.github.io/gan/gan-8)를 사용한다.

- 3개의 convolution
- 여러개의 residual block(=skip connection)
- 2개의 fractionally-strided convolution (stride = $\frac{1}{2}$)
  - Transposed convolution, deconvolution으로도 불린다. 
  - [DCGAN](https://happy-jihye.github.io/gan/gan-2/#dcgan-architecture)에서도 사용되었음
- `128 x 128` images에는 6개의 block을, `256 x 256` 이상의 high-resolution image에는 9개의 block을 사용
- instance normalization
- Discriminator는 `70 x 70`의 [PatchGAN](https://happy-jihye.github.io/gan/gan-8/#322-markovian-discriminator-patchgan)을 사용

또한, 기존에는 GAN의 loss function으로 negative log likelihood loss를 사용했다면,

$$\begin{aligned}
\mathcal{L}_{\mathrm{GAN}}\left(G, D_{Y}, X, Y\right) &=\mathbb{E}_{y \sim p_{\text {data }}(y)}\left[\log D_{Y}(y)\right] \\
&+\mathbb{E}_{x \sim p_{\text {data }}(x)}\left[\log \left(1-D_{Y}(G(x))\right]\right.
\end{aligned}$$

CycleGAN에서는 이를 **Least-squares Loss**로 바꿨다.

- minimize G :
$$\mathbb{E}_{x \sim p_{\text {data }}(x)}\left[(D(G(x))-1)^{2}\right]$$
- minimize D :
$$\mathbb{E}_{y \sim p_{\text {data }}(y)}\left[(D(y)-1)^{2}\right]+\mathbb{E}_{x \sim p_{\text {data }}(x)}\left[D(G(x))^{2}\right]$$

- 이렇게 하면 vanishing gradient문제를 해결하여 학습을 안정적으로 할 수 있다.

## 5. Experiments

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/cyclegan8.PNG?raw=1' width = '700' ></p>
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/cyclegan9.PNG?raw=1' width = '500' ></p>

다른 model보다 CycleGAN이 좋다.

---

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/cyclegan10.PNG?raw=1' width = '700' ></p>

paired dataset에 대해 실험을 한 pix2pix와 비교해봐도 cyclegan은 비슷한 성능을 낸다 !😮

## 6. Applications

**Style Transfer**
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/cyclegan11.PNG?raw=1' width = '800' ></p>

style을 변형시키는 것도 가능하다.

---

**Object transfiguration**
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/cyclegan12.PNG?raw=1' width = '800' ></p>

비슷한 카테고리로 transfiguration도 가능하다.

---

**Phto generation from paintings**
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/cyclegan13.PNG?raw=1' width = '800' ></p>
---
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/cyclegan14.PNG?raw=1' width = '600' ></p>

그림을 사진으로 바꿔줄 때 추가적인 loss를 도입하였다.

$$\mathcal{L}_{\text {identity }}(G, F)=\mathbb{E}_{y \sim p_{\text {data }}(y)}\left[\|G(y)-y\|_{1}\right]+\mathbb{E}_{x \sim p_{\text {dta }}(x)}\left[\|F(x)-x\|_{1}\right]$$

## 7. Limitations and Discussion

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/cyclegan15.PNG?raw=1' width = '800' ></p>
위의 그림처럼 학습이 잘 안된 경우도 존재한다. 보통은 training dataset에 없는 data가 주어졌을 때 학습에 실패를 하곤 한다. 

`horse -> zebra`를 보면, ImageNet에는 말과 얼룩말의 데이터는 많아 이에 대해 학습은 했지만, 말을 타고 있는 사람에 대해서는 학습을 하지 못해 학습에 실패했다.


> 그래도 **본 논문은 unpaired data(unsupervised setting)에 대해 효과적인 image-to-image task를 수행했다.**

## 8. Opinion

> ✍🏻 기존의 pix2pix의 논문에 새로운 아이디어 하나를 더해 좋은 결과를 낸 논문이다. 이전 논문부터 하나씩 발전해나가는 모델을 보니 재밌었다.
>
> 논문 자체도 쉽게 쓰여진 편이라 읽기 편했던 것 같다. cyclegan은 관련 application이 다양하던데, 이를 토대로 재미있는 project를 만들어보고 싶다!😊

---
**Reference**
- Naver AI LAB 최윤제 연구원님 발표자료