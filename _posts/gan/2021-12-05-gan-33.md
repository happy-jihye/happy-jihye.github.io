---
title: "[Paper Review] StyleGAN3: Alias-Free Generative Adversarial Networks ë…¼ë¬¸ ë¦¬ë·°"
excerpt: ""


categories:
 - GAN
tags:
  - deeplearning
  - ai
  - GAN
  - vision
search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 

use_math: true
---


- Paper : Alias-Free Generative Adversarial Networks (NeurIPS 2021) ([arxiv](https://arxiv.org/abs/2106.12423), [code](https://github.com/NVlabs/stylegan3), [project](https://nvlabs.github.io/alias-free-gan/))

- ğŸ˜ StyleGAN Review Series
    - [`[Paper Review] StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks ë…¼ë¬¸ ë¶„ì„`](https://happy-jihye.github.io/gan/gan-6/)
    - [`[Paper Review] StyleGAN2 : Analyzing and Improving the Image Quality of StyleGAN ë…¼ë¬¸ ë¶„ì„`](https://happy-jihye.github.io/gan/gan-7/)
    - [`[Paper Review] StyleGAN2-ADA #01: Training Generative Adversarial Networks with Limited Data ë…¼ë¬¸ ë¶„ì„`](https://happy-jihye.github.io/gan/gan-19/)
    - [`[Paper Review] StyleGAN2-ADA #02: Training Generative Adversarial Networks with Limited Data ì½”ë“œ ë¦¬ë·°`](https://happy-jihye.github.io/gan/gan-20/)

- [GAN-Zoos! (GAN í¬ìŠ¤íŒ… ëª¨ìŒì§‘)](https://happy-jihye.github.io/gan/)

---

<p align='center'>
  <iframe src="https://nvlabs-fi-cdn.nvidia.com/_web/alias-free-gan/videos/video_0_ffhq_cinemagraphs.mp4#t=0.001"
    frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen style="width: 42.5em; height: 25em;"></iframe>
</p>


# 0. Abstract

> - **Probloem**: ê¸°ì¡´ì˜ `StyleGAN2`ì€ **texture sticking** ì´ë¼ëŠ” ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆë‹¤. ì´ë¯¸ì§€ëŠ” êµ¬ì¡°ì ìœ¼ë¡œ í•™ìŠµë˜ì–´ì•¼ í•˜ëŠ”ë°(ex. í„±ì— í•´ë‹¹í•˜ëŠ” ìœ„ì¹˜ì— ìˆ˜ì—¼ì´ ìˆì–´ì•¼í•¨), styleganì˜ generatorëŠ” ì´ë¯¸ì§€ì˜ ê° íŠ¹ì§•ë“¤ì„ hierarchical ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•˜ì§€ ì•Šê³  ê³ ì •ëœ í”½ì…€ ë‹¨ìœ„ë¡œ í•™ìŠµì„ í•œë‹¤.
>   - interpolation ì˜ìƒì„ ë³´ë©´, StyleGAN2ì˜ ê²°ê³¼ëŠ” í„± ìˆ˜ì—¼ì´ ì¸ë¬¼ì„ ë”°ë¼ê°€ì§€ ì•Šê³ , í”½ì…€ ë‹¨ìœ„ë¡œ ê³ ì •ë˜ì–´ ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ.
> - **Cause**: careless signal processing that causes aliasing in the generator network
> - **Problem Solving**: hiearchical í•˜ê²Œ ì´ë¯¸ì§€ë¥¼ í•©ì„±í•  ìˆ˜ ìˆë„ë¡ alias-free í•œ network ì œì•ˆ
>   - translationì´ë‚˜ rotationì— ëŒ€í•´ equivarianceë¥¼ ë§Œì¡±
>   - videoë‚˜ animationì„ ë§Œë“¤ê¸°ì— ì í•©

# 1. Introduction

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-1.png?raw=1' width = '700' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> figure1 </p></i></font>

- **ê¸°ì¡´ì˜ StyleGAN generator**: coarse, low-resolution featureì—ì„œ ì‹œì‘í•˜ì—¬ upsamplingí•˜ê³ , convolutionìœ¼ë¡œ localí•˜ê²Œ mixingí•˜ê³ , non-linear functionì„ ê±°ì³ detailë“¤ì„ ì°¾ì•„ê°€ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œë‹¤.
  - coarse featureë“¤ì´ finer featureì˜ ì—¬ë¶€ì— ëŒ€í•´ì„œëŠ” ì¡°ì ˆì„ í•˜ì§€ë§Œ, ì •í™•í•œ ìœ„ì¹˜ê¹Œì§€ controlí•˜ì§€ëŠ” ëª»í•¨
  - ê²°ê³¼ì ìœ¼ë¡œ, fine detailì´ hiearchicalí•˜ê²Œ í•™ìŠµë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ pixel coordinateì— ê³ ì°©í™”ëœ ìƒíƒœë¡œ í•™ìŠµì´ ë¨
- ì´ëŸ¬í•œ **texture sticking** ë¬¸ì œëŠ” fig1ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŒ
  - latent interpolationì„ í†µí•´ ìì—°ìŠ¤ëŸ¬ìš´ transformationì„ ë§Œë“¤ì—ˆì„ ë•Œ, ì´ transformationì´ hierarchyí•˜ê²Œ ì¡°ì ˆë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê° featureë“¤ì´ íŠ¹ì • pixelì— ê³ ì°©í™”ë˜ì–´ ìˆìŒ
- â­ï¸ Our goal is an architecture that exhibits **a more natural transformation hierarchy**, where the exact sub-pixel position of each feature is exclusively inherited from the underlying coarse features.



## 1.1 The cause of the problem 

í˜„ì¬ networkê°€ ì´ìƒì ì¸ hierachical constructionì„ ê°€ì§€ì§€ ëª»í•˜ëŠ” ì´ìœ ëŠ” ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆë‹¤.
1. **image borders**: styleganì€ image borderê°€ spatial informationì„ ì¤˜ì„œ generatorê°€ texture stickingì´ ë˜ë„ë¡ í•™ìŠµì´ ë˜ê³  ìˆìŒ â†’ (sol) image ë¥¼ ì¢€ ë” í¬ê²Œ ì¡ì€ ë‹¤ìŒì— ë‚˜ì¤‘ì— cropí•˜ëŠ” ì‹ìœ¼ë¡œ í•™ìŠµ ([`3.4.1`](https://happy-jihye.github.io//gan/gan-33/#341-boundaries-and-upsampling-config-e))
2. **per-pixel noise inputs**: styleganì—ì„œëŠ” ê° pixelë§ˆë‹¤ independantí•œ gaussian noiseê°€ ë“¤ì–´ê°. ìš°ë¦¬ëŠ” pixelì— ë“¤ì–´ê°€ëŠ” transformì— ë”°ë¼ ì´ë¯¸ì§€ê°€ ë‹¤ë¥´ê²Œ ìƒì„±ë˜ê¸°ë¥¼ ì›í•˜ê¸° ë•Œë¬¸ì— translationê³¼ independantí•œ noiseë¥¼ ë„£ì–´ì£¼ë©´ ì•ˆë¨ â†’ (sol) alias-free ganì—ì„œëŠ” ì´ë¥¼ ì œê±° ! ([`3.3`](https://happy-jihye.github.io//gan/gan-33/#33-baseline-simplification)) 
3. **positional encoding** ([`3.2`](https://happy-jihye.github.io//gan/gan-33/#32-fourier-features)) 
4. **aliasing** â­ï¸

ì €ìë“¤ì€ ì´ ì¤‘ì—ì„œ <u>aliasingì´ ê°€ì¥ criticalí•œ issue</u>ë¼ê³  ì£¼ì¥í•œë‹¤. networkëŠ” aliasingì´ ì¡°ê¸ˆë§Œ ì¡´ì¬í•´ë„ ì´ë¥¼ ì¦í­í•˜ëŠ” ê²½í–¥ì´ ìˆì–´ì„œ í•™ìŠµì´ ì§„í–‰ë˜ë©´ì„œ scaleì´ ì»¤ì§ˆ ìˆ˜ë¡ í”½ì…€ì— íŠ¹ì • textureê°€ ê³ ì°©ë˜ê³¤ í•œë‹¤.


> ğŸ¤” Aliasingì€ ì™œ ë°œìƒí•˜ë‚˜?
> 1. **non-ideal upsampling filters** (ex. nearest, bilinear, strided conv): generatorì—ì„œ upsamplingì„ í•˜ëŠ” ê³¼ì •ì—ì„œ low-path filteringì„ í•˜ì§€ ì•Šì•˜ìŒ. ì¦‰ idealí•˜ì§€ ì•Šì€ upsampling filterë•Œë¬¸ì— ì›ì¹˜ ì•Šì€ high-frequencyë“¤ì´ ê³„ì† ë”í•´ì ¸ì„œ aliasingì´ ì¼ì–´ë‚˜ëŠ” ê²ƒ (sol in [`3.4.1`](https://happy-jihye.github.io/gan/gan-33/#341-boundaries-and-upsampling-config-e))
> 2. **pointwise application nonlinearities such as ReLU**: ì˜ˆë¥¼ ë“¤ì–´ ìŒìˆ˜ì¼ë•Œ 0ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” reluê°€ ìˆìœ¼ë©´ ê°‘ìê¸° ê°’ì´ í™• íŠ€ê²Œ ë¨ (sol in [`3.4.2`](https://happy-jihye.github.io/gan/gan-33/#342-filtered-nonlinearities-config-f))

ë˜, ì €ìë“¤ì€ ì´ëŸ¬í•œ aliasingì—ì„œ ë¹„ë¡¯ë˜ëŠ” ë¬¸ì œê°€ stylegan ë¿ë§Œ ì•„ë‹ˆë¼ deep learningì—ì„œ ì „ë°˜ì ìœ¼ë¡œ ë°œìƒí•œë‹¤ê³  ë³´ê³ í•˜ê³  ìˆë‹¤.

ê·¸ë ‡ë‹¤ë©´, aliasingì€ ì–´ë–»ê²Œ í•´ê²°í•  ìˆ˜ ìˆì„ê¹Œ?

ì´ë¡ ì ìœ¼ë¡œ aliasingì€ [Nyquist-Shannon sampling theorem](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem) ë¡œ í•´ê²°ëœë‹¤. ì €ìë“¤ì€ StyleGAN2ì˜ Generatorë¥¼ ì‹ í˜¸ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ upsampling filterë‘ pointwise nonlineartiesì—ì„œ ìƒê¸°ëŠ” aliasingì„ í•´ê²°í•˜ê³ ì í•˜ì˜€ë‹¤.

> (ì¡ë‹´ ğŸ¥¸) stylegan3ëŠ” ìœ„ì— ì–¸ê¸‰í•œ 4ê°€ì§€ì˜ ë¬¸ì œë¥¼ í•´ê²°í•¨ìœ¼ë¡œì¨ ëª¨ë¸ì„ equivarianceí•˜ê²Œ ë§Œë“œëŠ”ê²Œ í•µì‹¬ì¸ ë…¼ë¬¸ì´ë‹¤. ì™œ ë…¼ë¬¸ì˜ ì´ë¦„ì´ equivariance-ganê°€ ì•„ë‹ˆë¼ alias-free ganì¸ì§€ì— ëŒ€í•´ ê³ ë¯¼ì„ í•´ë´¤ëŠ”ë°, (4ê°€ì§€ ë¬¸ì œ ì¤‘ í•˜ë‚˜ê°€ aliasì´ë‹ˆê¹Œ..) ì €ìë“¤ì´ ì£¼ì²´ì ìœ¼ë¡œ í•´ê²°í•œ ë¬¸ì œê°€ aliasingì´ì–´ì„œ ì¸ ê²ƒ ê°™ë‹¤.
> 
> 1, 2, 3ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì œë“¤ì€ ë‹¤ë¥¸ ë…¼ë¬¸ë“¤ì—ì„œ ë¬¸ì œì œê¸°ë¥¼ í•˜ê³  í•´ê²°ì„ í–ˆë‹¤ë©´, 4ì— í•´ë‹¹í•˜ëŠ” aliasing ë¬¸ì œëŠ” ì´ ë…¼ë¬¸ì—ì„œ ì²˜ìŒìœ¼ë¡œ ë¬¸ì œë¥¼ ì œê¸°í•˜ê³  í•´ê²°ì„ í–ˆê¸° ë•Œë¬¸ì— ë…¼ë¬¸ì˜ ì œëª©ì´ alias-free ganì´ ì•„ë‹ê¹Œ ì¶”ì¸¡í•œë‹¤.
> 
> ê·¸ë˜ì„œì¸ì§€ 1, 2, 3 ë¬¸ì œì˜ í•´ê²°ì±…ì€ ë…¼ë¬¸ì—ì„œ ë¶ˆì¹œì ˆí•˜ê²Œ ì„¤ëª…ë˜ì–´ìˆë‹¤. 1, 2, 3ë²ˆ ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í•´ê²°í–ˆëŠ”ì§€ì— ëŒ€í•´ ìì„¸í•˜ê²Œ ì´í•´ë¥¼ í•˜ë ¤ë©´ reference ë…¼ë¬¸ì„ ì½ì–´ë³´ê³  ì´í•´í•  í•„ìš”ê°€ ìˆë‹¤.


---

## 1.2 Equivariance ì˜ ì˜ë¯¸

**About translation equivariant in CNN**

ìµœê·¼ image classification ì—°êµ¬ì—ì„œëŠ” CNNì´ translation equivariant í•˜ì§€ ëª»í–ˆì„ ë•Œì˜ ë¬¸ì œì ì— ëŒ€í•´ ë§ì€ ì—°êµ¬ê°€ ì§„í–‰ë˜ê³  ìˆë‹¤. (CNN ìì²´ëŠ” equivarianceí•´ì•¼ì§€ Gloval average pooling ì„ í†µê³¼í–ˆì„ ë•Œ, final representationì´ translation invariantí•˜ê²Œ ë¨)

- ì°¸ê³ í• ë§Œí•œ ê¸€
  - [`CNNê³¼ ì´ë¯¸ì§€ê°€ ì°°ë–¡ê¶í•©ì¸ ì´ìœ `](https://seoilgun.medium.com/cnn%EC%9D%98-stationarity%EC%99%80-locality-610166700979)
  - [`translation invariance ì„¤ëª… ë° ì •ë¦¬`](https://ganghee-lee.tistory.com/43)

ì €ìë“¤ì€ CNN representationì´ translation equivariant í•˜ë ¤ë©´, CNNì—ì„œ ë‚˜ì˜¤ëŠ” feature mapì´ Nyquist frequency ë¥¼ ë„˜ì–´ì„œëŠ” ë¹ ë¥¸ íŒ¨í„´ë“¤ì„ ê°€ì§€ë©´ ì•ˆëœë‹¤ê³  ë³´ê³ í•œë‹¤. ì¦‰, aliasingì´ ë°œìƒí•˜ë©´ ì•ˆëœë‹¤.

**Equivariance**


ìš°ë¦¬ëŠ” 2D planeìƒì—ì„œ ì–´ë–¤ operation $f$ ê°€ íŠ¹ì • transformation $t$ ì— ëŒ€í•´ êµí™˜ ë²•ì¹™ì´ ì„±ë¦½ë˜ë©´ **equivariant** í•˜ë‹¤ê³  ë§í•œë‹¤.

$$\mathbf{t} \circ \mathbf{f}=\mathbf{f} \circ \mathbf{t}$$


> ğŸ¥¸ ë³¸ ë…¼ë¬¸ì€ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ operation(ex, CNN, ReLU, Upsampling / Downsampling..)ì—ì„œ Equivariance í•´ì•¼í•œë‹¤ê³  ê³„ì†í•´ì„œ ì£¼ì¥í•œë‹¤. **Generator ëª¨ë¸ì„ Equivariance í•˜ê²Œ ë§Œë“œëŠ” ê²ƒ**ì´ ë…¼ë¬¸ì˜ í•µì‹¬ì´ë‹¤.

Equivarinace í•´ì•¼í•œë‹¤ëŠ” ê²ƒì€ ì§ê´€ì ìœ¼ë¡œ êµ‰ì¥íˆ ë‹¹ì—°í•œ ê°œë…ì´ë‹¤. 
 
ì˜ˆë¥¼ ë“¤ì–´, ìš°ë¦¬ê°€ ì–´ë–¤ ì´ë¯¸ì§€ë¥¼ íšŒì „ì‹œí‚¤ê³  ì‹¶ì„ ë•Œ,
 - latent code $z$ ë¡œ ë¶€í„° ìƒì„±ëœ ì´ë¯¸ì§€ë¥¼ íšŒì „ì‹œì¼°ì„ ë•Œë‘
 - íšŒì „ëœ latent code $z$ ì—ì„œ ìƒì„±ëœ ì´ë¯¸ì§€ê°€
 
ê°™ë‹¤ë©´, ì´ê²ƒì´ ë°”ë¡œ rotationì— ëŒ€í•´ equivariant í•œ ê²ƒì´ë‹¤.

> ìš°ë¦¬ì˜ ëª©í‘œëŠ” ê°ê°ì˜ íŠ¹ì§•ë“¤ì„ hierarchicalí•˜ê²Œ í•™ìŠµí•˜ê²Œ ë§Œë“œëŠ” ê²ƒì´ë‹¤.
> 
> ë”°ë¼ì„œ ê°ê°ì˜ layerë“¤ì„ translation equivariantí•˜ê²Œ ë§Œë“¤ì–´ì„œ `ë³€í˜•ëœ inputìœ¼ë¡œ ìƒì„±ëœ output`ì´ `ì¼ë°˜ outputì„ ë³€í˜•í•œ ê²ƒ`ê³¼ ê°™ë„ë¡ ë§Œë“ ë‹¤ë©´, **ì´ë¯¸ì§€ì˜ ê° íŠ¹ì§•ë“¤ì´ ìì—°ìŠ¤ëŸ½ê²Œ hierarchicalí•˜ê²Œ í•™ìŠµ**ë  ê²ƒì´ë‹¤.
 

**plus ğŸ˜Š**

ì´ì „ì— `BDInvert`ë¼ëŠ” í¬ìŠ¤í…ì—ì„œ ë‚˜ì˜¨ ë…¼ë¬¸ì„ ë¦¬ë·°í•œ ì ì´ ìˆë‹¤. ì´ ë…¼ë¬¸ì€ styleganì˜ generatorì—ì„œ base codeë¼ëŠ” ì¤‘ê°„ feature map $f$ ì„ ë½‘ì•„ ë³€í˜•ì‹œí‚¤ë©´ output imageë„ ë³€í˜•ëœ ìƒíƒœë¡œ ìƒì„±ëœë‹¤ëŠ” ë…¼ë¬¸ì´ë‹¤.

- [[Paper Review] BDInvert: GAN Inversion for Out-of-Range Images with Geometric Transformations ë…¼ë¬¸ ë¦¬ë·°](https://happy-jihye.github.io/gan/gan-32/)

stylegan3ì˜ equivaraianceê°€ input feature mapì„ ë³€í˜•ì‹œí‚¤ë©´ ìƒì„±ëœ output imageë„ ë™ì¼í•˜ê²Œ ë³€í˜•ëœë‹¤ëŠ” ê±´ë°, ì´ ìì²´ê°€ BDInvertì˜ ì»¨ì…‰ê³¼ ë¹„ìŠ·í•œ ê²ƒ ê°™ë‹¤. 

---

# 2. Equivariance via continuous signal interpretation


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-3.jpg?raw=1' width = '800' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> figure2 </p></i></font>

> ë³¸ ë…¼ë¬¸ì—ì„œëŠ” discrete domainì™€ continuous domain ì‚¬ì´ë¥¼ ììœ ë¡­ê²Œ ë„˜ë‚˜ë“¤ ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” operationì— ëŒ€í•´ ì†Œê°œí•œë‹¤. (`figure2` ì°¸ê³ )
> - **sampling**: continuous â†’ discrete (by Dirac Comb)
> - **interpolation**: discrete â†’ continuous (by ideal interpolation filter)


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-2.jpeg?raw=1' width = '800' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> figure3 </p></i></font>

- **figure3**: discrete â†”ï¸ continuous domain ìœ¼ë¡œì˜ ë³€í™˜ ê³¼ì •ì—ì„œ aliasingì´ ì•ˆìƒê¸´ ì±„ ì‹ í˜¸ë¥¼ sampling & interpolation í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ì´ë¡  ì •ë¦¬

- **Nyquist-Shannon sampling Theorem** ([`#01`](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=guburi&logNo=221369911121), [`#02`](https://linecard.tistory.com/20))
  - ë§Œì•½ ì‹ í˜¸ê°€ ëŒ€ì—­ì œí•œ(bandlimited)ì‹ í˜¸ì´ê³ , í‘œë³¸í™” ì£¼íŒŒìˆ˜ê°€ ì‹ í˜¸ì˜ ëŒ€ì—­ì˜ ë‘ ë°° ì´ìƒì´ë¼ë©´ í‘œë³¸ìœ¼ë¡œë¶€í„° ì—°ì† ì‹œê°„ ê¸°ì € ëŒ€ì—­ ì‹ í˜¸ë¥¼ ì™„ì „íˆ ì¬êµ¬ì„±í•  ìˆ˜ ìˆë‹¤.
  - ì…ë ¥ ì‹ í˜¸ì˜ ìµœê³  ì£¼íŒŒìˆ˜ $f_{max}$ ì˜ 2ë°° ì´ìƒìœ¼ë¡œ ëª¨ë“  ì‹ í˜¸ë“¤ì„ ê· ì¼í•˜ê²Œ sampling í•œë‹¤ë©´, ì›ë˜ ì‹ í˜¸ë¥¼ ì™„ë²½í•˜ê²Œ ë³µì›í•  ìˆ˜ ìˆë‹¤.
  - `aliasing í˜„ìƒ`: ì•„ë‚ ë¡œê·¸ ì‹ í˜¸ë¥¼ ë””ì§€í„¸ ì‹ í˜¸ì— ì ìš©í•  ë•Œ, sampling ì†ë„ê°€ $2f_{max}$ ë³´ë‹¤ ì‘ì„ ê²½ìš° ì•„ë‚ ë¡œê·¸ ì…ë ¥ ì‹ í˜¸ì—ì„œ ì¼ë¶€ ìµœê³  ì£¼íŒŒìˆ˜ ì„±ë¶„ì´ ë””ì§€í„¸ ì¶œë ¥ì— ì˜¬ë°”ë¥´ê²Œ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤. ë”°ë¼ì„œ ì´ ë””ì§€í„¸ ì‹ í˜¸ë¥¼ ë‹¤ì‹œê¸ˆ ì•„ë‚ ë¡œê·¸ ì‹ í˜¸ë¡œ ë³€í™˜í•˜ê³ ì í•  ë•Œ, ì›ë˜ ì£¼íŒŒìˆ˜ì— ì—†ë˜ ì˜ëª»ëœ ì£¼íŒŒìˆ˜ ì„±ë¶„ì´ ë‚˜íƒ€ë‚œë‹¤. 
  - â­ï¸ ì´ ì´ë¡ ì— ë”°ë¼ ì‹ í˜¸ë¥¼ sampling í•˜ê³  ë‚˜ë©´, samplingëœ discrete feature map $Z(x)$ ì´ ë‚˜ì¤‘ì— continuous domainìœ¼ë¡œ ë³µì›í•˜ê¸° ìœ„í•œ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆìŒ

- **Whittaker-Shannon interpolation Theorem**
  - sinc interpolation
  - ideal band-limited inteporlation
  - â­ï¸ discreteí•˜ê²Œ samplingëœ Dirac grid $Z(x)$ ì™€ ideal interpolation filter $\phi_{s}$ ë¥¼ convolutioní•˜ê³  ë‚˜ë©´, continuous í•œ ì‹ í˜¸ë¥¼ ë³µì›í•  ìˆ˜ ìˆìŒ

$$z(\boldsymbol{x})=\left(\phi_{s} * Z\right)(\boldsymbol{x})$$


<span style='background-color: #E5EBF7;'> <b>Discrete and continuous representations of network layers</b> </span>

ë˜í•œ, continuous domainê³¼ discrete domainê°„ì˜ ë³€í™˜ì´ ììœ ë¡œìš°ë ¤ë©´ ê°ê°ì˜ domainì—ì„œ í–‰í•´ì§€ëŠ” operationê°„ì˜ ë³€í™˜ë„ ììœ ë¡œì›Œì•¼í•œë‹¤. (ë‹¨, ì´ë•Œ frequencyê°€ bandlimitì„ ë„˜ì–´ì„œë©´ ì•ˆë¨)

$$\mathbf{f}(z)=\phi_{s^{\prime}} * \mathbf{F}\left(\mathrm{W}_{s} \odot z\right), \quad \mathbf{F}(Z)=\mathrm{W}_{s^{\prime}} \odot \mathbf{f}\left(\phi_{s} * Z\right)$$

- discrete domain
  - practical neural networkëŠ” discretely sampled feature mapì—ì„œ ë™ì‘
  - discrete feature mapì—ì„œ convolution, nonlinearityì™€ ê°™ì€ operation $F$ ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë¨

$$ Z' = F(Z)$$

- continuous domain

$$ z' = f(z)$$


## 2.1 Equivariant network layers

> â­ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **2ê°€ì§€ tranformation(translation, rotation)** ê³¼ ì „í˜•ì ì¸ generator networkì˜ 4ê°€ì§€ **operations(convolution, upsampling, downsampling, nonlinearity)** ì— ëŒ€í•´ì„œ equivariantí•œì§€ í™•ì¸í•œë‹¤. 
> 
> ë˜í•œ, **aliasingì´ ì—†ìœ¼ë ¤ë©´** nyquist samplingì„ í–ˆì„ ë•Œ ì´ìƒí•œ high frequencyê°€ ì—†ì–´ì•¼í•œë‹¤. ì¦‰ low-path filteringì´ outputê¹Œì§€ ìœ ì§€ë˜ê³  ìˆëŠ”ì§€ë¥¼ í™•ì¸í•´ì¤˜ì•¼í•œë‹¤.

### 2.1.1 Convolution

<span style='background-color: #E5EBF7;'> <b>discrete domain</b> </span>

ìš°ì„ , discrete domainì—ì„œë¶€í„° ì‚´í´ë³´ì. discrete kernel $K$ ì—ì„œì˜ standard convolutionì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤.

$$\mathbf{F}_{\text {conv }}(Z)=K * Z$$

<span style='background-color: #E5EBF7;'> <b>continuous domain</b> </span>

discrete domainì—ì„œì˜ convolution ì‹ì„ continuous domainì—ì„œì˜ ì‹ìœ¼ë¡œ ë³€í™˜í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$\mathbf{f}_{\mathrm{conv}}(z)=\phi_{s} *\left(K *\left(\text { Ğ¨ }_{s} \odot z\right)\right)$$

(1) convolutionì€ commutativity í•˜ë¯€ë¡œ $\phi_{s} * K = K *\phi_{s}$ 

$$\phi_{s} *\left(K *\left(\text { Ğ¨ }_{s} \odot z\right)\right)=K *\left(\phi_{s} *\left(\text { Ğ¨ }_{s} \odot z\right)\right)$$

(2) ideal low path filterë¥¼ ì‚¬ìš©í•œë‹¤ë©´, $z$ ë¥¼ dirac combë¥¼ í†µí•´ sampling í•œ í›„ ideal interpolation filterë¡œ interpolation í•˜ë©´ ë‹¤ì‹œ $z$ ê°€ ë¨

$$\phi_{s} *(\text { Ğ¨ }_{s} \odot z) = z$$

$$\mathbf{f}_{\mathrm{conv}}(z) = K *\left(\phi_{s} *\left(\text { Ğ¨ }_{s} \odot z\right)\right)=K * z$$

ì¦‰, convolutionì˜ commutativityí•œ ì„±ì§ˆ (1) ë•Œë¬¸ì— **translation equivariance ëŠ” ë§Œì¡±**í•˜ë©°, convolutionê³¼ì •ì—ì„œ ìƒˆë¡œìš´ frequencyê°€ ì¶”ê°€ë˜ê±°ë‚˜ í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ (2) domainê°„ì˜ ë³€í™˜ì—ì„œ ì¶”ê°€ì ìœ¼ë¡œ **aliasing ì—­ì‹œ ìƒê¸°ì§€ ì•ŠëŠ”ë‹¤**.

- translation ì˜ ê²½ìš° convolutionì€ ë‹¹ì—°íˆ equivarianceí•˜ë‹¤
- rotation equivarianceì˜ ê²½ìš°ì—ëŠ”, discrete kernel $K$ ê°€ radially symmetricí•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ì•½ê°„ ì£¼ì˜í•  í•„ìš”ê°€ ìˆë‹¤. â†’ ì‹¤ì œë¡œ `stylegan3` ì—ì„œëŠ” rotation equivarianceë¥¼ ë§Œì¡±ì‹œí‚¤ê¸° ìœ„í•´ symmetricí•œ 1x1 convë¥¼ ì‚¬ìš©


---

### 2.1.2 Upsampling & Downsampling

<span style='background-color: #E5EBF7;'> <b>Continuous domainì—ì„œì˜ Upsampling & Downsampling</b> </span>

continuous domainì—ì„œì˜ upsamplingì€ ì•„ë¬´ëŸ° ì˜ë¯¸ê°€ ì—†ë‹¤. (ì´ë¯¸ infinite domainì´ë‹ˆê¹Œ) 

$$f_{up}(z) = z$$

ì¦‰, translation or rotationì„ í•˜ê³  upsampling í•œ ê²ƒê³¼ upsampling í•˜ê³  translation or rotation í•˜ëŠ” ê²ƒì´ ë™ì¼í•˜ë‹¤ (**equivariance**)

<span style='background-color: #E5EBF7;'> <b>Discrete domainì—ì„œì˜ Upsampling & Downsampling</b> </span>

ê·¸ëŸ¬ë‚˜ discreteí•œ domainì—ì„œì˜ upsampling filterëŠ” ideal í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— upsamplingì„ í•˜ëŠ” ê³¼ì •ì—ì„œ aliasingì´ ìƒê¸°ê³ , ì´ ë•Œë¬¸ì— ë“±ë³€ì„±ì´ ì‚¬ë¼ì§„ë‹¤. ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì€ upsampling ê³¼ì •ì—ì„œ low path filteringì„ í•˜ì—¬ upsamplingê³¼ downsamplingì´ ì´ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë˜ê²Œ í•œë‹¤. (â†’ equivariance !)


---

### 2.1.3 Nonlinearity

nonlinearityëŠ” styleganì˜ generatorì—ì„œ ìœ ì¼í•˜ê²Œ high-frequencyë¥¼ í•™ìŠµí•  ìˆ˜ ìˆëŠ” blockì´ë‹¤. ê·¸ëŸ¬ë‚˜ non-linearityë¥¼ ê·¸ëƒ¥ ì ìš©í•´ë²„ë¦¬ë©´ ì˜ë„ì¹˜ ì•Šì€ aliasingì´ ìƒê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ low-path filteringì„ í•´ì„œ ìƒˆë¡­ê²Œ ì¶”ê°€ë˜ëŠ” ì •ë³´ì˜ ì–‘ì„ ì¡°ì ˆí•´ì•¼í•œë‹¤.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-11.jpeg?raw=1' width = '800' ></p>


**continuous domain**ì—ì„œëŠ” ReLUê°€ pointwise operationì´ê¸° ë•Œë¬¸ì— equivarianceê°€ ë‹¹ì—°íˆ ì„±ë¦½ëœë‹¤. ê·¸ëŸ¬ë‚˜ bandlimit constraintëŠ” ë§Œì¡±ë˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆë‹¤.

ì¦‰, continuous domainì—ì„œ ReLU operationì„ í•˜ê³  ë‚˜ë©´ outputì—ì„œ ì˜ë„ì¹˜ ì•Šì€ high-frequncyê°€ ìƒê¸¸ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. (aliasing)

ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ aliasingì„ ì œê±°í•˜ê¸° ìœ„í•´ non-linearityì˜ ê²°ê³¼ê°’ì— low-path filteringì„ í•´ì¤€ë‹¤.

$$\mathbf{f}_{\sigma}(z)=\phi_{s} * \sigma(z) \quad \mathbf{F}_{\sigma}(Z)=\text { Ğ¨ }_{s} \odot\left(\phi_{s} * \sigma\left(\phi_{s} * Z\right)\right)$$




**ì •ë¦¬ â­ï¸**

> - Generatorê°€ low-resolutionì—ì„œ ì‹œì‘í•˜ì—¬ **upsampling**ì„ í•œ í›„
> - **non-linearity function**ì„ í†µí•´ detailí•œ ë¶€ë¶„(high-frequency)ì„ ë§Œë“¤ì–´ë‚˜ê°€ëŠ”ë°, 
> - ì´ë•Œ ìƒì„±ë˜ëŠ” high-frequency ì˜ì—­ë“¤ì„ **low-path filterë¥¼ í†µí•´ cut-off** í•˜ë©´ì„œ 
> - **high-frequencyë¥¼ ì ì ˆí•˜ê²Œ í•™ìŠµ**í•˜ë„ë¡ í•¨


---

# 3. Practical application to generator network

> âœğŸ» **2ì ˆ**ì—ì„œëŠ” ì£¼ìš” operationì—ì„œ ì–´ë–¤ ë¬¸ì œê°€ ìƒê¸°ëŠ”ì§€ì™€ ê·¸ê²ƒì„ ì–´ë–»ê²Œ í•´ê²°í•˜ëŠ”ì§€ì— ëŒ€í•´ ì†Œê°œí•˜ì˜€ë‹¤. **3ì ˆ**ì—ì„œëŠ” ì‹¤ì§ˆì ì¸ ë¬¸ì œë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì–´ë–¤ ì‹ìœ¼ë¡œ networkë¥¼ ë°”ê¿¨ëŠ”ì§€ì— ëŒ€í•´ í•˜ë‚˜ì”© ì„¤ëª…í•œë‹¤. 

## 3.1 StyleGAN2 

**Discriminator**
  - alias-free-ganì—ì„œëŠ” stylegan2ì˜ discriminator êµ¬ì¡°ë¥¼ ìœ ì§€ ğŸ˜Š


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan2-5.PNG?raw=1' width = '500' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> figure4: stylegan2 architecture </p></i></font>

**Generator**

1. **mapping Network**: initial, normally distributed latent code $z$ ë¥¼ intermediate latent code $w \sim \mathcal{W}$ ë¡œ transform
2. **synthesis network G**: learned constant input `4x4x412` $Z_0$ ì—ì„œ Nê°œì˜ layerë¥¼ ê±°ì³ output image $Z_n = G(Z_0;w)$ ë¥¼ ìƒì„±
  - Nê°œì˜ layer:  consisting of `convolutions, nonlinearities, upsampling, and per-pixel noise`
  - skip connection, mixing regularization, path length regularization ê¸°ë²•ë“¤ë„ ë„ì…

> â­ï¸ Our goal is **to make every layer of G equivariant w.r.t. the continuous signal**, so that all finer details transform together with the coarser features of a local neighborhood
>
> - ì¦‰, transformation $t$ (translation & rotation)ì— ëŒ€í•´ equvariant í•˜ë„ë¡ synthesis networkì˜ operation $g$ ë¥¼ continuous í•˜ê²Œ ë§Œë“¤ì–´ì•¼í•¨ 
>
> $$\mathbf{g}\left(\mathbf{t}\left[z_{0}\right] ; \mathbf{w}\right)=\mathbf{t}\left[\mathbf{g}\left(z_{0} ; \mathbf{w}\right)\right]$$

Generatorì˜ operationë“¤ì„ equivarianceí•˜ê²Œ ë§Œë“œëŠ” ê²ƒì´ ì´ ë…¼ë¬¸ì˜ í•µì‹¬ì´ë‹¤.

ì €ìë“¤ì€ ê° operationì´ ì–¼ë§ˆë‚˜ equivariance í•œì§€ í‰ê°€í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë„ í•¨ê»˜ report í•˜ì˜€ë‹¤. 
- the peak signal-to-noise ratio (PSNR) in decibels (dB) between two sets of images
- EQ-T / EQ-R : ì´ scoreê°€ ë†’ì„ ìˆ˜ë¡ translation / rotationì— ëŒ€í•´ equivarianceí•œ ê²ƒ
- ìì„¸í•œ ë‚´ìš©ì€ ë…¼ë¬¸ì˜ p5 ë¥¼ ì°¸ê³ 

---

**Result**

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-4.png?raw=1' width = '800' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> figure5 </p></i></font>


## 3.2 Fourier features 

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-6.png?raw=1' width = '800' ></p>

> ì´ë¯¸ì§€ë¥¼ continuousí•˜ê²Œ transformation(t&r) í•˜ê¸° ìœ„í•´ **learned constant inputì„ Fourier feartureë¡œ ë³€ê²½**í•˜ì˜€ë‹¤.


StyleGAN3ì—ì„œëŠ” `fourier-feature-networks [56]`ì™€ `Positional Encoding as Spatial Inductive Bias in GANs [66]` ë…¼ë¬¸ì˜ ì•„ì´ë””ì–´ë¥¼ ì°¨ìš©í•˜ì—¬ <u>learned constant inputì„ Fourier feartureë¡œ ëŒ€ì²´</u>í•˜ì˜€ë‹¤. ì–´ë–¤ ì´ì ì„ ì–»ê¸° ìœ„í•´ Fourier Featureë¥¼ ì‚¬ìš©í•˜ì˜€ëŠ”ì§€ë¥¼ ë¶„ì„í•˜ê³ ì ë‹¤ìŒ ë…¼ë¬¸(`[56, 66]`)ì„ ê°„ë‹¨í•˜ê²Œ ìš”ì•½í•˜ì˜€ë‹¤.

### 3.2.1 Fourier Features Let Networks [56]

> Paper: Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains (NeurIPS2020): [project](https://bmild.github.io/fourfeat/)


- **Fourier featuring**ë€, coordinate space pointë¥¼ frequency spaceë¡œ embeddingí•˜ëŠ” functionì˜ ì´ì¹­ì´ë‹¤. 
- *transformer ê³„ì—´ì˜ ëª¨ë¸ë“¤*ì—ì„œëŠ” featureì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ í¬í•¨ì‹œí‚¤ê¸° ìœ„í•´ sinusoidal function(fourier featuring function)ì„ ì‚¬ìš©í•˜ì—¬ coordinate spaceì—ì„œ frequency spaceë¡œì˜ embeddingì„ ì§„í–‰í•˜ëŠ”ë°, ì´ ì—­ì‹œ ì¼ì¢…ì˜ Positional Encodingì´ë‹¤ ([ì°¸ê³  ë§í¬](https://happy-jihye.github.io/nlp/nlp-8/#positional-encoding))
- ì¦‰, Fourier Featuringì„ í†µí•´ frequency domainìœ¼ë¡œ embeddingí•˜ëŠ” ê²ƒ ìì²´ê°€ **Positional Encoding**ì˜ íš¨ê³¼ë¥¼ ì¤€ë‹¤.
- Fourier-Feature mapping function $\gamma$


$$\gamma(v)=\left[a_{1} \cos \left(2 \pi b_{1}^{T} v\right), a_{1} \sin \left(2 \pi b_{1}^{T} v\right), \ldots ., a_{m} \cos \left(2 \pi b_{m}^{T} v\right), a_{m} \sin \left(2 \pi b_{m}^{T} v\right)\right]^{T}$$

- Transformerë“±ì˜ attention-based architectureë“¤ì€ PE functionìœ¼ë¡œ $\gamma$ ë¥¼ ì‚¬ìš©í•˜ë©°, ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.

$$a_{i}=1, b_{m}=10000^{m / d}, d: \text { dimension }$$

- ì´ ë…¼ë¬¸ì—ì„œëŠ” Fourier-Featureë¡œ ë¶€í„° PE ì •ë³´ë¥¼ ì¤€ í›„, MLPë¥¼ í†µí•´ high-frequencyë¥¼ í•™ìŠµí•´ë‚˜ê°€ë©´ì„œ ê³ í•´ìƒë„ì˜ ì´ë¯¸ì§€ ìƒì„±ì„ í•™ìŠµí•œë‹¤.

---

### 3.2.2 Positional Encoding [66]

> Paper: Positional Encoding as Spatial Inductive Bias in GANs (CVPR 2021): [project](https://nbei.github.io/gan-pos-encoding.html), [arxiv](https://arxiv.org/abs/2012.05217)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/pe-gan-1.png?raw=1' width = '550' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> figure-pe-1 </p></i></font>

- **Problem**: SinGAN, StyleGAN2ê³¼ ê°™ì€ translation-invariant convolutional generatorëŠ” ì–´ë–»ê²Œ spatialí•œ global structureë¥¼ í•™ìŠµí•˜ëŠ”ê°€?
- **Cause**:  zero padding - unbalanced spatial biasë¥¼ ì œê³µí•˜ì—¬ ìœ„ì¹˜ ì •ë³´ë¥¼ implicití•˜ê²Œ í•™ìŠµí•˜ë„ë¡ ë„ì›€
- **Contribution**: ì´ ë…¼ë¬¸ì€ ë” íš¨ê³¼ì ì¸ spatial inductive biasë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ **(1) multi-scale training strageì™€ (2) explicit positional encoding** ë°©ì‹ì„ ì œì•ˆ


`translation-invariant convolutional generator`ê°€ ì •ë§ë¡œ ìœ„ì¹˜ì— invariantí•˜ë‹¤ë©´, `figure-pe-1 (b)`ì˜ ê²°ê³¼ì²˜ëŸ¼ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì–´ì•¼ í•œë‹¤. ê·¸ëŸ¬ë‚˜ ì‹¤ì œë¡œëŠ” `figure-pe-1 (a)` ì²˜ëŸ¼ ì´ë¯¸ì§€ê°€ ìƒì„±ëœë‹¤. ( = Generatorê°€ ì–´ëŠì •ë„ ìœ„ì¹˜ì— ëŒ€í•œ ì •ë³´ë¥¼ í•™ìŠµí•œë‹¤ëŠ” ê²ƒ )

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/pe-gan-2.png?raw=1' width = '570' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> figure-pe-2 </p></i></font>


- ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ë¬¸ì œê°€ **zero padding** ë•Œë¬¸ì´ë¼ê³  ì„¤ëª…í•œë‹¤. border ë¶€ë¶„ì˜ zero paddingì€ conv layerê°€ feature mapì˜ distributionì—ì„œ location-aware biasë¥¼ í•™ìŠµí•˜ê²Œ ë§Œë“¤ê¸° ë•Œë¬¸ì—, border ë¶€ë¶„ì—ì„œëŠ” PE ì •ë³´ë¥¼ í•™ìŠµí•˜ê³  centerë¡œ ê°ˆìˆ˜ë¡ PE ì •ë„ë¥¼ í•™ìŠµí•˜ì§€ ëª»í•˜ëŠ” ê²ƒì´ë‹¤. 
- ì¦‰, zero paddingì€ ì´ë¯¸ì§€ ê³µê°„ì— ëŒ€í•´ unbalanced spatial biasë¥¼ ì£¼ì–´ generatorê°€ implicití•˜ê²Œ positional encoding ì •ë³´ë¥¼ í•™ìŠµí•˜ë„ë¡ í•œë‹¤.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/pe-gan-3.png?raw=1' width = '700' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> figure-pe-3 </p></i></font>

- `top` - standard StyleGAN2ì˜ ê²°ê³¼, `bottom` - padding-free StyleGAN2ì˜ ê²°ê³¼ 
- `ì˜¤ë¥¸ìª½ ì„¸ê°œì˜ ê·¸ë¦¼` - learned constant inputì´ ì•„ë‹ˆë¼ identical const inputì—ì„œ ìƒì„±ëœ ì´ë¯¸ì§€
- `top`: zero paddingì´ border ë¶€ë¶„ì˜ frozen structureë¥¼ í•™ìŠµí•˜ë„ë¡ í•˜ì—¬ identical const inputì—ì„œ ìƒì„±ëœ ì´ë¯¸ì§€ê°€ ê¸°ì¡´ ì´ë¯¸ì§€ì™€ ë¹„ìŠ·í•œ borderë¥¼ ê°€ì§
- `bottom`: zero paddingì„ ì œê±°í•˜ë‹ˆ ë‹¤ì–‘í•œ colorì™€ patternì´ ìƒê¹€
- StyleGAN2 ì—ì„œë„ implicit positional encodingì˜ ë¬¸ì œê°€ ë‚˜íƒ€ë‚œë‹¤.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/pe-gan-4.png?raw=1' width = '550' ></p>

- `(a)`: zero paddingìœ¼ë¡œ ì¸í•œ implicit spatial anchorëŠ” border ë¶€ë¶„ì— ëŒ€í•œ PE ì •ë³´ë¥¼ í•™ìŠµí•˜ë„ë¡ ë„ì›€
- `(b)`: StyleGAN2ì˜ generatorëŠ” **4 x 4 x 512ì˜ learned constant vectorë¥¼ input**ìœ¼ë¡œ ë°›ëŠ”ë‹¤. ì´ëŠ” positional encodingì˜ ì •ë³´ë¥¼ explicití•˜ê²Œ ì£¼ê¸°ëŠ” í•˜ì§€ë§Œ, ì´ë¡œ ì¸í•œ spatial inductive biasê°€ unclearí•˜ë©° ì´ë¯¸ì§€ ê³µê°„ì„ explicití•˜ê²Œ í‘œí˜„í–ˆë‹¤ê³  ë³´ê¸°ì—ë„ ë¶€ì¡±í•¨ì´ ë§ë‹¤.
- `(d)`: SPE (Sinusoidal Positional Encoding) ë°©ì‹ = fourier featuringê³¼ ë™ì¼í•œ ë°©ì‹
  - nlpì—ì„œ positional encoding ì„ ì£¼ê¸° ìœ„í•´ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°©ì‹ì´ë‹¤.
  - ì´ë¥¼ ì´ìš©í•˜ë©´ ì´ë¯¸ì§€ì— ëŒ€í•œ positional encoding ì •ë³´ë¥¼ explicití•˜ê²Œ ì¤„ ìˆ˜ ìˆë‹¤.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/pe-gan-5.png?raw=1' width = '800' ></p>

---

**ì •ë¦¬ â­ï¸**

> - ê¸°ì¡´ì˜ learned const inputì€ positional encoding ì •ë³´ë¥¼ ì£¼ê¸°ëŠ” í•˜ì§€ë§Œ, ì¢Œí‘œê³„ê°€ ì¢‹ì§€ ì•Šì•„ ì–´ë– í•œ ë°©ì‹ìœ¼ë¡œ transformationì´ ì‘ë™ë˜ëŠ”ì§€ ì•Œê¸° ì–´ë ¤ì› ìœ¼ë©° ì´ë¯¸ì§€ ê³µê°„ì„ explicití•˜ê²Œ í‘œí˜„í–ˆë‹¤ê³  í•˜ê¸°ì—ë„ ë¶€ì¡±í•œ ì ì´ ë§ì•˜ë‹¤. 
> - stylegan2ì—ì„œëŠ” signalì˜ í¬ê¸°ë§Œì„ encodingí–ˆë‹¤ë©´, stylegan3ì—ì„œëŠ” ìƒˆë¡œìš´ coordinate system (SPE)ì„ ë„ì…í•˜ì—¬ signal ë¿ë§Œ ì•„ë‹ˆë¼ phaseì— ëŒ€í•œ ì •ë³´ë„ ì˜ encodingí•˜ê³ ì í•˜ì˜€ë‹¤.
> 
> stylegan3ëŠ” **Fourier Feature** (in continousí•œ frequency domain)ìœ¼ë¡œ inputì„ ë³€ê²½í•˜ì—¬ **infinit domainìœ¼ë¡œ í™•ì¥**í•˜ì˜€ê³ , ë™ì‹œì— **Postional Encoding** ì •ë³´ë„ explicití•˜ê²Œ ì¤„ ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤. 

- stylegan2ì˜ constant learned inputê³¼ ë§ˆì°¬ê°€ì§€ë¡œ í•™ìŠµê³¼ì •ì—ì„œëŠ” ì´ ì…ë ¥ê°’ì€ ê³ ì •ëœë‹¤
- Fourier Featureë¥¼ ë„ì…í•˜ë©´ FIDê°€ ì•½ê°„ ê°œì„ ë¨: 5.14 â¡ 4.79 (`figure5 (ë…¼ë¬¸ì˜ figure3)`)
- ë˜í•œ, transformation(t&r) ê°€ ê°€ëŠ¥í•´ì§€ëŠ”ë°, ëŒ€ì‹  FIDëŠ” ë§¤ìš° ë–¨ì–´ì§„ë‹¤. (`figure6`)
- fourier featureë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ equivarianceë¥¼ ì‰½ê²Œ ì¸¡ì •í•  ìˆ˜ ìˆë‹¤. (`figure5`ì—ì„œì˜ EQ-T, EQ-R)

- âœğŸ» ì°¸ê³ í• ë§Œí•œ ë§í¬ 
  - [Position Encodingì˜ ì¢…ë¥˜ì™€ ë¶„ì„](https://blog-deepest.medium.com/position-encoding%EC%9D%98-%EC%A2%85%EB%A5%98%EC%99%80-%EB%B6%84%EC%84%9D-ab1816b0f62a)
  - [gjghks950.log/Fourier Features Let Networks Learn High-Frequency Functions in Low Dimensional Domains Review](https://velog.io/@gjghks950/Fourier-Features-Let-Networks-Learn-High-Frequency-Functions-in-Low-Dimensional-Domains-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0)
  - [[ë…¼ë¬¸ ë¦¬ë·°] Positional Encoding as Spatial Inductive Bias in GANs](https://deep-math.tistory.com/m/7)

---

### 3.2.3 Code

`Fourier features`ëŠ” stylegan3 ì˜ ì½”ë“œì—ì„œ [`SyntehsisInput`](https://github.com/NVlabs/stylegan3/blob/b1a62b91b18824cf58b533f75f660b073799595d/training/networks_stylegan3.py#L169)ì— êµ¬í˜„ë˜ì–´ìˆë‹¤.
- `SyntehsisInput` block
  1. intermediate latent code $w$ ë¥¼ inputìœ¼ë¡œ ë°›ì•„ affine ë³€í™˜ì„ í•œ í›„, 
  2. ì´ ê°’ì„ learned transformation: (1) ë¨¼ì € imageë¥¼ rotationí•œ í›„ (2) translation (3) ë§ˆì§€ë§‰ìœ¼ë¡œëŠ” user-specified transform
  3. sampling gridë¥¼ ë§Œë“¤ì–´ì„œ fourier featureë¡œ ë³€í™˜

```python
# Compute Fourier features.
x = (grids.unsqueeze(3) @ freqs.permute(0, 2, 1).unsqueeze(1).unsqueeze(2)).squeeze(3) # [batch, height, width, channel]
x = x + phases.unsqueeze(1).unsqueeze(2)
x = torch.sin(x * (np.pi * 2))
x = x * amplitudes.unsqueeze(1).unsqueeze(2)
```

---

### 3.2.4 Transformed Fourier Features(config H)

- ë…¼ë¬¸ì˜ 3.2 ì ˆì— í•´ë‹¹

StyleGAN3 Generatorì˜ layerë“¤ì€ equivariantí•˜ê¸° ë•Œë¬¸ì— unaligned datasetì´ë‚˜ ì„ì˜ë¡œ ë³€í˜•ì‹œí‚¨ datasetì— ëŒ€í•´ì„œë„ ì˜ í•™ìŠµì´ ëœë‹¤. (ë§Œì•½ intermediate feature $z_i$ë¥¼ ë³€í˜•ì‹œí‚¤ë©´ final image $z_N$ ë„ ë³€í˜•ë˜ì–´ ìƒì„±)

ê·¸ëŸ¬ë‚˜ layer ìì²´ì—ì„œ globalí•˜ê²Œ transformation í•˜ê¸°ì—ëŠ” layerì˜ capabilityê°€ ì‘ë‹¤. ë”°ë¼ì„œ Input Fourier Features ìì²´ë¥¼ ë³€í˜•ì‹œí‚¤ëŠ” ë°©ì‹ìœ¼ë¡œ ìƒì„±ë˜ëŠ” ì´ë¯¸ì§€ë„ transformationë˜ë„ë¡ í•œë‹¤.
- learned affine layerë¥¼ í†µí•´ input Fourier Features ê°€ global translation or rotation ë˜ë„ë¡ ë§Œë“¦
- `SyntehsisInput` codeì˜ ì´ ë¶€ë¶„
  
```python
# Apply learned transformation.
t = self.affine(w) # t = (r_c, r_s, t_x, t_y)
t = t / t[:, :2].norm(dim=1, keepdim=True) # t' = (r'_c, r'_s, t'_x, t'_y)
m_r = torch.eye(3, device=w.device).unsqueeze(0).repeat([w.shape[0], 1, 1]) # Inverse rotation wrt. resulting image.
m_r[:, 0, 0] = t[:, 0]  # r'_c
m_r[:, 0, 1] = -t[:, 1] # r'_s
m_r[:, 1, 0] = t[:, 1]  # r'_s
m_r[:, 1, 1] = t[:, 0]  # r'_c
m_t = torch.eye(3, device=w.device).unsqueeze(0).repeat([w.shape[0], 1, 1]) # Inverse translation wrt. resulting image.
m_t[:, 0, 2] = -t[:, 2] # t'_x
m_t[:, 1, 2] = -t[:, 3] # t'_y
transforms = m_r @ m_t @ transforms # First rotate resulting image, then translate, and finally apply user-specified transform.

# Transform frequencies.
phases = phases + (freqs @ transforms[:, :2, 2:]).squeeze(2)
freqs = freqs @ transforms[:, :2, :2]
```

**Transformation Result**

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-5.png?raw=1' width = '800' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> figure6 </p></i></font>

---
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-4.png?raw=1' width = '800' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> figure5 </p></i></font>

## 3.3 Baseline Simplification

> (1) **per-pixel noise inputs**ë¥¼ ì œê±°í•˜ì˜€ë‹¤.

- stylegan2ì— ì‚½ì…ë˜ëŠ” per-pixel noiseëŠ” ì´ë¯¸ì§€ì˜ ì„¸ë¶€ì ì¸ ìš”ì†Œë“¤ì„ ë…ë¦½ì ì´ê²Œ í•™ìŠµí•˜ë„ë¡ ë§Œë“¤ê¸° ë•Œë¬¸ì—, ì´ë¯¸ì§€ê°€ hierarchicalí•˜ê²Œ í•™ìŠµë˜ì§€ ëª»í•œë‹¤.
- noiseë¥¼ ì œê±°í•˜ë©´, `figure5 (ë…¼ë¬¸ì˜ figure3)`ë¥¼ ë³´ë©´ FIDê°€ ê·¸ë‹¥ ê°œì„ ë˜ì§€ëŠ” ì•Šì§€ë§Œ í›¨ì”¬ equivariance í•´ì§„ë‹¤.

> (2) `StyleGAN2-ADA`ì—ì„œ ì²˜ëŸ¼ **the mapping network depth** ë¥¼ ì¤„ì„
>
> (3) disable **mixing regularization and path length regularization**
>
> (4) **output skip connections** ì œê±°

- FID scoreë¥¼ ë†’ì´ê¸° ìœ„í•´ 2,3,4ë¥¼ í–ˆì—ˆì§€ë§Œ, ëª¨ë¸ì„ ë‹¨ìˆœí™”í•˜ê¸° ìœ„í•´ FIDëŠ” ì•½ê°„ í¬ê¸°í•˜ê³  2,3,4ë¥¼ ì œê±°í•˜ì˜€ë‹¤.


## 3.4 Step-by-step redesign motivated by continuous interpretation

### 3.4.1 Boundaries and Upsampling (config E)

> **Boundaries**: ë³¸ ë…¼ë¬¸ì—ì„œëŠ” feature mapì„ ë¬´í•œí•œ ê³µê°„ìœ¼ë¡œ í™•ì¥í–ˆë‹¤ê³  ê°€ì •í•œë‹¤. ë”°ë¼ì„œ target canvasì— ì–´ëŠì •ë„ì˜ marginì„ ì¤€ í›„, high-layerë¡œ ê°ˆìˆ˜ë¡ ì´ í™•ì¥ëœ canvasë¥¼ crop í•˜ì˜€ë‹¤.

- border paddingì´ ë‚´ë¶€ ì´ë¯¸ì§€ì˜ coordinateì˜ ê°’ì„ ì–´ëŠì •ë„ ê°–ê³  ìˆê¸° ë•Œë¬¸ì— borderë¥¼ explicití•˜ê²Œ extensioní•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.
- ì‹¤í—˜ ê²°ê³¼, 10-pixel margin ì •ë„ë©´ ì¶©ë¶„í•˜ì—¬ ì´ë¥¼ ì‚¬ìš©í–ˆë‹¤ í•œë‹¤.

> **Upsampling**: ê¸°ì¡´ì˜ bilinear 2X upsampling filterë¥¼ windowed sinc filterë¡œ ëŒ€ì²´í•˜ì—¬ low-pass filteringë„ í•¨ê»˜ í•˜ë„ë¡ í•˜ì˜€ë‹¤.

- ì°¸ê³ âœğŸ»: [windowed sinc filter](https://www.analog.com/media/en/technical-documentation/dsp-book/dsp_book_Ch16.pdf) 
- $n=6$ì˜ large Kaiser window : upsamplingì˜ ê³¼ì •ì—ì„œ output pixelì€ 6ê°œì˜ input pixelì—ë§Œ ì˜í–¥ì„ ë°›ê³ , downsamplingì˜ ê³¼ì •ì—ì„œ input pixelì€ 6ê°œì˜ output pixelì—ë§Œ ì˜í–¥ì„ ì¤Œ
- `figure5 (ë…¼ë¬¸ì˜ figure3)`: resampling filterë¥¼ ì‘ê²Œ ì„¤ì •í•˜ë©´($n=4$) translation equivarianceê°€ ì•ˆì¢‹ì•„ì§€ê³ , ì´ë¥¼ í¬ê²Œ ì„¤ì •í•˜ë©´($n=8$) training ì†ë„ê°€ ëŠë ¤ì§

### 3.4.2 Filtered nonlinearities (config F)

[`2.1.3`](https://happy-jihye.github.io/gan/gan-33/#213-nonlinearity) ì—ì„œ ReLUê°€ ë‹¹ì—°íˆ equivarianceëŠ” ë§Œì¡±í•˜ì§€ë§Œ, bandlimitë¥¼ ì§€í‚¤ì§€ ì•Šìœ¼ë©´ aliasingì´ ìƒê¸¸ ìˆ˜ë„ ìˆë‹¤ê³  ë³´ê³ í•˜ì˜€ë‹¤. ë”°ë¼ì„œ non-linearity functionì„ ì§€ë‚  ë•Œ low-path filteringì„ ê¼­ í•´ì•¼í•œë‹¤.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-7.png?raw=1' width = '800' ></p>

- ì €ìë“¤ì€ upsample-leaky ReLU-downsampleì˜ sequenceê°€ CUDA kernelì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ ì—°ì‚°ë˜ë„ë¡ ìµœì í™”ë¥¼ í–ˆë‹¤ê³  í•œë‹¤. (10ë°° ë¹¨ë¼ì§ + memory saving)
- upsampling + downsampling ì •ë„ëŠ” ì‹¤í—˜ê²°ê³¼ $m=2$ ë©´ ì¶©ë¶„í•˜ë‹¤ê³  í•œë‹¤.

### 3.4.3 Non-critical sampling (config G)  & Flexible layer (config T)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-8.png?raw=1' width = '500' ></p>

<span style='background-color: #E5EBF7;'> <b>Non-critical sampling (config G)</b> </span>

aliasingì€ generatorì˜ equivarianceë¥¼ ë§ì¹˜ëŠ” ì›ì¸ì´ê¸°ë„ í•˜ë‹¤. ë”°ë¼ì„œ ê°ê°ì˜ layerë¥¼ ì§€ë‚  ë•Œ aliasingì´ ìƒê¸°ì§€ ì•Šë„ë¡ í•´ì•¼í•œë‹¤.

- **config G**ì—ì„œëŠ” ì €í•´ìƒë„ì˜ layerì—ì„œ aliasingì´ ì•ˆìƒê¸°ë„ë¡ cutoff frequencyë¥¼ $f_{c}=s / 2-f_{h}$ ë¡œ ë‚®ì¶¤ !

<span style='background-color: #E5EBF7;'> <b>Flexible layer (config T)</b> </span>

ì´ë ‡ê²Œ aliasingì„ ì—†ì• ëŠ” ê²ƒì€ ì¤‘ìš”í•˜ë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë¯¸ì§€ë¥¼ í•™ìŠµí•  ë•Œ ìƒìœ„ layerë¡œ ê°ˆìˆ˜ë¡ detailì„ í•™ìŠµí•˜ëŠ” ê²ƒë„ ì¤‘ìš”í•˜ë‹¤. ì¦‰, high-frequencyë„ ì ì ˆíˆ í•™ìŠµì„ í•´ì•¼í•˜ëŠ”ë°, low-path filteringì„ ë„ˆë¬´ ê°•í•˜ê²Œ ê±¸ì–´ì£¼ë‹¤ë³´ë©´ aliasingì€ ì•ˆìƒê¸°ê² ì§€ë§Œ high-frequency (detail)ê°€ í•™ìŠµë˜ì§€ ëª»í•œë‹¤. 

- ë”°ë¼ì„œ **config T**ì—ì„œëŠ” layerë¥¼ flexibleí•˜ê²Œ ì¡°ì ˆí•œë‹¤. 

> ì •ë¦¬í•˜ìë©´, 
> - ì €í•´ìƒë„ì˜ layerì—ì„œëŠ” aliasingì´ ì•ˆìƒê¸°ë„ë¡ **lower cutoff frequencyë¥¼ í†µí•´ low-path filteringì„ ê°•í•˜ê²Œ ê±¸ì–´ì£¼ê³ **
> - ê³ í•´ìƒë„ì˜ layerì—ì„œëŠ” ì´ë¯¸ì§€ì˜ detailì„ í•™ìŠµí•˜ëŠ”ê²Œ ì¤‘ìš”í•˜ë¯€ë¡œ **flexibleí•˜ê²Œ ì¡°ì ˆí•˜ì—¬ high-frequencyë¥¼ í•™ìŠµ**í•˜ë„ë¡ í•œë‹¤.


### 3.4.4 Rotation equivariance (config R)

networkë¥¼ rotation equivariantí•˜ê²Œ ë³€í˜•í•˜ê³ ì í• ë•Œì—ëŠ” 2ê°€ì§€ë¥¼ ë³€ê²½í•œë‹¤.

1. `3x3 conv`ë¥¼ `1x1 conv` ë¡œ ë³€ê²½. ëŒ€ì‹  feature mapì˜ ìˆ˜ë¥¼ 2ë°°ë¡œ ëŠ˜ë¦°ë‹¤
2. sinc-based downsampling filterë¥¼ radially symmetric jinc-based filterë¡œ ë³€ê²½
  - í•™ìŠµê³¼ì •ì—ì„œ trainable parameterê°€ 56% ì¤„ì–´ë“œëŠ” íš¨ê³¼
  - FIDëŠ” ë¹„ìŠ·í•˜ë©° EQ-Rì€ ì•½ê°„ í–¥ìƒë¨


# 4. Results

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-9.png?raw=1' width = '800' ></p>

- ì‹¤í—˜ê²°ê³¼ëŠ” í‘œ ì°¸ê³ 
- FIDë„ ê´œì°®ìœ¼ë©° equivarinceë„ ì¢‹ë‹¤ê³  ë³´ê³ 
- training ì†ë„ ê°œì„  + ì—°ì‚° ìµœì í™” ì§„í–‰
- unaligned imageì— ëŒ€í•´ ì‹¤í—˜í–ˆë‹¤ëŠ” ì ì´ ì¸ìƒê¹Šì—ˆë‹¤. unaligned imageì— ëŒ€í•´ projection í•´ë´ì•¼ì§€ ğŸ˜‰

**Internal representations**

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-10.png?raw=1' width = '800' ></p>

fourier featureë¼ëŠ” ìƒˆë¡œìš´ ì¢Œí‘œê³„ì—ì„œ ì´ë¯¸ì§€ ìƒì„±ì„ ì‹œì‘í•˜ê¸° ë•Œë¬¸ì— signalë¿ë§Œ ì•„ë‹ˆë¼ phase ì •ë³´ë„ encodingí•  ìˆ˜ ìˆê²Œ ë˜ì—ˆê³ , positional encoding ì •ë³´ë„ explicití•˜ê²Œ ì¤„ ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.


---

# ë§ˆì¹˜ë©°..

> ì˜¤ë˜ì „ë¶€í„° ì½ê¸° ì‹œì‘í•œ ë…¼ë¬¸ì´ì—ˆëŠ”ë°, backgroundë¥¼ í•˜ë‚˜ì”© ì±„ìš°ë©´ì„œ ì½ë‹¤ë³´ë‹ˆ ì™„ë…í•˜ëŠ”ë° ì‹œê°„ì´ ê±¸ë¦° ë…¼ë¬¸ì´ë‹¤ ğŸ˜‚ 
> ê·¸ë§Œí¼ ì‹ í˜¸ì²˜ë¦¬ì— ëŒ€í•œ ë°°ê²½ì§€ì‹ì´ ë§ì´ í•„ìš”í–ˆë˜ ë…¼ë¬¸ì´ë¼ ì½ê¸° ì–´ë ¤ì› ë‹¤.
>
> ëª¨ë¸ ìì²´ì˜ architectureê°€ í˜ì‹ ì ìœ¼ë¡œ ë°”ë€ ê±´ ì—†ì§€ë§Œ, stylegan2ê°€ ê°€ì§€ê³  ìˆì—ˆë˜ ë‹¤ì–‘í•œ ë¬¸ì œë“¤ì„ ì—¬ëŸ¬ ë…¼ë¬¸ì˜ ëª¨ë¸ë“¤ì˜ ì•„ì´ë””ì–´ë¥¼ í†µí•´ í’€ì–´ë‚˜ê°„ì ì´ í¥ë¯¸ë¡œì› ë‹¤. ë˜, NVIDIAê°€ ìŠ¬ìŠ¬ imageë¥¼ ë„˜ì–´ videoë‚˜ animationì„ ìœ„í•œ ëª¨ë¸ì„ ë§Œë“œë ¤ê³  ì‹œë„í•˜ëŠ” ê²ƒ ê°™ë‹¤ëŠ” ì¸ìƒì„ ë°›ì•˜ë‹¤.
>
> ì—¬ëŸ¬ëª¨ë¡œ í¥ë¯¸ë¡œì› ë˜ ë…¼ë¬¸ì´ë‹¤. official ì½”ë“œê°€ ê³µê°œëœ ë§Œí¼ ë‹¤ì–‘í•˜ê²Œ ì‹¤í—˜ì„ í•´ë´ì•¼ê² ë‹¤ ğŸ˜Š

---

# Reference

- [gjghks950.log/Fourier Features Let Networks Learn High-Frequency Functions in Low Dimensional Domains Review](https://velog.io/@gjghks950/Fourier-Features-Let-Networks-Learn-High-Frequency-Functions-in-Low-Dimensional-Domains-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0)
- [PR-338 Alias-Free Generative Adversarial Networks (StyleGAN3)](https://www.youtube.com/watch?v=BZwUR9hvBPE&t=3122s)
- [Alias-Free GAN [20210802, Ha Hyunwoo]](https://www.youtube.com/watch?v=73HPb6fu_LY&t=658s)