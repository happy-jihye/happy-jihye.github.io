---
title: "[Paper Review] CLIPDraw & StyleCLIPDraw 논문 리뷰"
excerpt: ""

categories:
 - GAN
tags:
  - deeplearning
  - ai
  - pytorch
  - vision
search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---


## CLIPDraw

- Paper: `CLIPDraw`: Exploring Text-to-Drawing Synthesis through Language-Image Encoders (arxiv 2021): [arxiv](https://arxiv.org/abs/2106.14843), [code](https://arxiv.org/abs/2106.14843)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/CLIPDraw/Untitled.png?raw=1' width = '800' ></p>

> - 별도의 훈련 없이 pre-trained CLIP language-image encoder를 사용하여 text로부터 이미지를 생성하는 모델
>   - description과 generated drawing간의 smilarity가 최대화되도록 optimize

- CLIP model
    - image encoder와 text encoder, 이 2가지의 network로 구성되며
    - 이들은 512 dim의 encoding space를 공유한다

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/CLIPDraw/Untitled%201.png?raw=1' width = '800' ></p>

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/CLIPDraw/Untitled%202.png?raw=1' width = '600' ></p>

- Method
    1. **Initialize Curves**: N개의 RGBA Bezier curves를 random으로 initialize
        - 각 curve들은 thickness, RGBA color vector와 함께 3~5개의 control point로 parameterize된다고 함
        - 처음에는 curve들이 random으로 initialize
        - 흰 배경, default color
        - optimization 동안 curve와 control point의 수는 고정, thickness와 color vector는 GD를 통해 optimize될 수 있음
    2. **Render Curves to Pixels**: 매 iteration마다 curve를 pixel image로 rendering (by differentiable vector render)
    3. **Augment the Image**: 2의 이미지를 D번 augmentation
        - random perspective shift
        - random crop-and-resize
    4. **Encoding Image**: 3의 augmented image batch를 CLIP model로 encoding
    5. **Compute Loss**: text encoding값과의 cosine similarity 계산
    6. **Backprop**
    7. 2,3,4,5,6 반복

**Result**

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/CLIPDraw/Untitled%203.png?raw=1' width = '800' ></p>


---

## StyleCLIPDraw

- Paper: `StyleCLIPDraw`: Coupling Content and Style in Text-to-Drawing Synthesis (NeurIPS workshop 2021): [arxiv](https://arxiv.org/abs/2111.03133), [code](https://github.com/pschaldenbrand/StyleCLIPDraw)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/CLIPDraw/Untitled%204.png?raw=1' width = '800' ></p>

> - CLIPDraw에 style loss를 추가해서 style control까지 가능하도록 한 모델
>   - CLIPDraw의 결과물에 style-transfer를 하면 단순히 texture만 포함이 되지만
>   - StyleCLIPDraw를 통해 이미지를 생성하면 style image의 texture와 shape이 반영된 이미지 생성이 가능하다.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/CLIPDraw/Untitled%205.png?raw=1' width = '800' ></p>

- Method
    1. **Initialize Curves** & **Render Curves to Pixels**: Differentiable Renderer로 brush strokes(Bezier curve)를 raster image로 rendering
        - StyleCLIPDraw는 CLIPDraw와 마찬가지로 randomized Bezier curves를 initialize하여 시작한다.
    2. **Content Loss**
        1. image를 augmentation
        2. CLIPDraw에서처럼 CLIP model을 활용하여 augmented image와 text를 embedding한 후 cosine distance로 loss를 계산한다.
    3. **StyleLoss**
        - VGG-16 model로 raster image와 style image사이의 loss를 계산

**Result**

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/CLIPDraw/Untitled%206.png?raw=1' width = '800' ></p>
