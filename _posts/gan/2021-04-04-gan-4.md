---
title: "[Paper Review] Generative Adversarial Text to Image Synthesis"
excerpt: " "

date: 2021-04-04
categories:
 - GAN
tags:
  - deeplearning
  - ai
  - pytorch
  - GAN
  - vision
  - nlp
search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---


> 이번 포스팅에서는 text로 image를 생성하는 연구인 **Generative Adversarial Text to Image Synthesis**에 대해 살펴본다.

- Paper : [Generative Adversarial Text to Image Synthesis](https://arxiv.org/abs/1605.05396)
          (2016 / Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee)
          
- [GAN-Zoos! (GAN 포스팅 모음집)](https://happy-jihye.github.io/gan/)


이 논문이 나왔을 때만 해도 text에서 image를 합성하는 연구가 활발하지 않았다고 한다. 이 논문에서는 GAN과 deep architectrue를 사용하여 text to image synthesis 모델을 만들었다.

대표적인 실험으로는 새와 꽃을 설명한 text로 실제 이미지를 생성한 것이 있다.


## 1. Introduction


## 2. Related Work

## 3. Background
### 3.1 Generative Adversarial Networks

### 3.2 Deep symmetric structured joint embedding

text에서 visually-dscriminative vector를 얻는 방식으로는 [Learning Deep Representations of Fine-grained Visual Descriptions(2016)](https://arxiv.org/abs/1605.05395) 논문의 **Deep Convolutional and Recurrent Text Encoder**를 사용했다. **Deep symmetric structured joint embedding**는 text를 encoding하는 것과 image를 encoding하여 얻은 것이 같은 embedding vector이다.

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/gantti1.PNG?raw=1" width = "450" ></p>

- 맨 아래의 사진이 text와 가장 비슷하므로 loss가 0에 가까움

- **Classfier**
  - $f_{t}$ : text classifier

    $$\left.f_{v}(v)=\underset{y \in \mathcal{Y}}{\arg \max } \mathbb{E}_{t \sim \mathcal{T}(y)}\left[\phi(v)^{T} \varphi(t)\right)\right]$$

  - $f_{v}$ : visual classifier

    $$\left.f_{t}(t)=\underset{y \in \mathcal{Y}}{\arg \max } \mathbb{E}_{v \sim \mathcal{V}(y)}\left[\phi(v)^{T} \varphi(t)\right)\right]$$

  - $v_n$ : image / $\phi$ : image encoder(ex CNN)
  - $t_n$ : corresponding text description / $\varphi$ : text encoder (ex LSTM) 
  - $y_n$ : cass label
  - 특정 이미지가 들어갔을 때 가장 비슷한 text를 이끌어내는 분류기

- **Structure Loss**

    $$ \frac{1}{N} \sum_{n=1}^{N} \Delta\left(y_{n}, f_{v}\left(v_{n}\right)\right)+\Delta\left(y_{n}, f_{t}\left(t_{n}\right)\right) $$
    
    이미지 분류기로부터 나온 값($f_{v}\left(v_{n}\right)$, text값이 나옴)과 실제 값($y_{n}$)간의 loss를 계산하고 text 분류기로 나온 값(image값이 나옴)과 실제 값과의 비교하여 loss를 계산한다.

## 4. Method

> ⭐ 본 논문에서는 Convolutional RNN으로 encoding한 text features와 DCGAN을 활용해서 이미지를 합성한다.

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/dcgan1.PNG?raw=1" width = "800" ></p>

- [DCGAN 설명](https://happy-jihye.github.io/gan/gan-2/)


---
### 4.1 Network Architecture

- Generator : $\mathbb{R}^{Z} \times \mathbb{R}^{T} \rightarrow \mathbb{R}^{D}$
- Discriminator : $\mathbb{R}^{D} \times \mathbb{R}^{T} \rightarrow\{0,1\}$
  - T : dim of text description embedding
  - D : dim of text image embedding
  - Z : dim of noise ($z \in \mathbb{R}^{Z} \sim \mathcal{N}(0,1))


<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/gantti2.PNG?raw=1" width = "800" ></p>

**Generator**

1. text encoder $\varphi$를 사용해서 text query $t$를 encoding 한 후, 얻은 결과값($\varphi(t)$)을 FC layer에 넣어 compress한 후 Leaky-ReLU를 사용해서 128-dim의 작은 차원으로 compression한다.

2. 이 값을 noise vector `z`와 concate한후 deconvolutional network를 통해 generate image를 얻는다.


**Discriminator**

1. 여러개의 stride-2 convolution layer와 BN기법, leaky ReLU function을 이용해서 학습을 한다.

2. 1번의 과정을 4x4 conv layer가 될때까지 반복한다.

3. 4x4 conv layer가 되면 compressiong된 embedding vector $\varphi$를 여러개 복사해서 conv layer 뒤에 이어붙인다.(depth concatenation)

4. 1x1 conv layer가 되도록 연산을 한 후 final score를 얻는다.

- 이때 모든 conv layer에 대해서 BN을 해준다.

### 4.2 Matching-aware discriminator (GAN-CLS)

conditional GAN은 discriminator가 (text, images) pair가 진짜인지 가짜인지 판단하도록 학습한다. 이때 문제점은 discriminator는 real training image가 어떤 text embedding context와 match되는지 모른다는 점이다.

즉, real image가 자기를 설명하지 않은 text와 match될 수도 있다.(mismatch)

> 따라서 (real image, mismatched text term)을 추가하도록 GAN training algorithm을 수정해서 discriminator가 fake에 대해서도 학습을 할 수 있게 한다.

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/gantti4.PNG?raw=1" width = "600" ></p>

기존의 GAN은 위의 algorithm에서 7,9의 score만 있었다면 이제는 line 8에 있는 score도 추가된다.

### 4.3 Learning with manifold interpolation (GAN-INT)

- 딥러닝 네트워크는 embedding pair사이의 interpolation에서 representation을 학습한다.
  - interpolation(보간)은 [data manifold](https://greatjoy.tistory.com/51) 근처에서 생김
  - 즉, (text1, image1), (text2, image2)가 있을 때 text1과 text2가 space내에서 가까이에 있다면 image1을 학습할 때 text1의 feature외에도 text2의 feature가 사용될 수 있다.

- 따라서 **interpolated text embedding($\beta t_{1}+(1-\beta) t_{2}$)을 사용**하기 위해 아래의 식을 사용한다.
  - 이때 interpolated text embedding은 사람이 쓴 text는 아니다. text1과 text2의 보간된 embedding값이 text3라면 text3를 사용하게 되는 것!

    $$\mathbb{E}_{t_{1}, t_{2} \sim p_{\text {data }}}\left[\log \left(1-D\left(G\left(z, \beta t_{1}+(1-\beta) t_{2}\right)\right)\right)\right]$$

  - 보통 $\beta$로는 0.5를 사용한다.

### 4.3 Inverting the generator for style transfer

text encoding $\varphi(t)$는 image content(ex. flower shape, colors)를 찾아내는 역할을 한다.(text에서 이미지로 표현할 만한 feature들을 추출) 진짜 같은 이미지를 만들려면 noise sample `z`는 배경이나 자세와 같은 **style factor**를 잘 만들어야하기 때문에 이를 위해 **style transfer**를 학습시킨다.

> Style transfer : GAN을 훈련시킬 때 query image를 특정한 text 설명으로 전환하려고 하는 것

즉, 기존에는 $\hat{x} \leftarrow G(z, \varphi(t))$를 학습했다면, style transfer에서는 $\hat{x}$에서 $z$로 거꾸로 훈련을 한다.

loss함수로는 다음의 **simple squared loss**를 사용한다. (S는 style encoder network)
$$\mathcal{L}_{\text {style }}=\mathbb{E}_{t, z \sim \mathcal{N}(0,1)}\|z-S(G(z, \varphi(t)))\|_{2}^{2}$$

