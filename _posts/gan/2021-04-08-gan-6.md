---
title: "[Paper Review] StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks ë…¼ë¬¸ ë¶„ì„"
excerpt: "style transferë¥¼ PGGANì— ì ìš©í•œ nvidia researchì˜ StyleGAN modelì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤."

categories:
 - GAN
tags:
  - deeplearning
  - ai
  - pytorch
  - GAN
  - vision
search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 

use_math: true
---

> âœğŸ» ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” style transferë¥¼ PGGANì— ì ìš©í•œ nvidia researchì˜ **StyleGAN** modelì— ëŒ€í•´ ì‚´í´ë³¸ë‹¤.


- Paper : [A Style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/abs/1812.04948) (2019 / Tero Karras, Samuli Laine, Timo Aila)
          
- [GAN-Zoos! (GAN í¬ìŠ¤íŒ… ëª¨ìŒì§‘)](https://happy-jihye.github.io/gan/)

---

ë³¸ modelì€ discrimintatorë‚˜ loss functionì€ ê±´ë“¤ì´ì§€ ì•Šê³ , **styleì„ ë” ì˜ í•™ìŠµì‹œí‚¤ë„ë¡ generatorì˜ architectureë¥¼ ë°œì „ì‹œí‚¨ ëª¨ë¸**ì´ë‹¤. ê¸°ì¡´ì˜ PGGAN(ProGAN) ëª¨ë¸ì„ ë³€í˜•ì‹œì¼œ imageí•©ì„±ì„ <span style="background-color: #D5E0EF;">**style scale-specific cotrol**</span> í•  ìˆ˜ ìˆë„ë¡ ë°œì „ì‹œì¼°ë‹¤.

ì•„ë˜ì˜ ì‚¬ì§„ì²˜ëŸ¼ íŠ¹ì • styleì„ scalingí•  ìˆ˜ ìˆëŠ” ê²ƒì´ **style scale-specific control** ì´ë‹¤.

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan-1.gif?raw=1" width = "700" ></p>

ë˜í•œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” interpolation qualityì™€ disentanglementë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆëŠ” 2ê°€ì§€ ë°©ì‹ì— ëŒ€í•´ ì†Œê°œí•˜ê³  ìˆë‹¤.(Section 4)


## 1. Introduction

GANì˜ generatorëŠ” ì•„ì§ë„ black boxê°™ë‹¤ëŠ” ë¹„íŒì„ ë°›ê³  ìˆë‹¤. latent spaceì˜ interpolationì— ëŒ€í•œ ì—°êµ¬ëŠ” ì–´ëŠ ì •ë„ ì§„í–‰ì´ ë˜ì—ˆì§€ë§Œ, ì•„ì§ë„ latent spaceì— ê´€í•œ ì´í•´ê°€ ë¶€ì¡±í•˜ë‹¤. ([GAN-interpolation](https://happy-jihye.github.io/gan/gan-4/#43-learning-with-manifold-interpolation-gan-int)ì€ ì´ ê¸€ ì°¸ê³ )

ë³¸ ë…¼ë¬¸ì€ style transfer literatureë¥¼ ê¸°ì¡´ì˜ generatorì— ì ìš©í•˜ì˜€ë‹¤. <span style='background-color: #FFF2CC;'> **latent vetor** $z$ì— styleì„ ë„£ì„ ìˆ˜ ìˆë„ë¡ **learned constant input $w$**(mapping networkë¥¼ í†µí•´ í•™ìŠµëœ parmeter)ì„ ì¡°ê¸ˆì”© ì¡°ì •í•˜ì˜€ë‹¤.(style : w1, w2, w3...) </span>

<span style='background-color: #FFF2CC;'> ë˜í•œ, style vectorë¥¼ ë„£ì„ ë•Œ noiseì™€ í•¨ê»˜ ë„£ì–´ì£¼ì–´ì„œ styleê°„ì— correlationì´ ì—†ë„ë¡ í–ˆê³ , ìì„¸ë‚˜ identityì™€ ê°™ì€ unsupervised separationë¶€í„° ì£¼ê·¼ê¹¨ë‚˜ ë¨¸ë¦¬ê°™ì€ stochastic variationê¹Œì§€ ìë™ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ë‹¤. </span>

ì¦‰, **scale-specific mixingê³¼ interpolationì´ ê°€ëŠ¥í•´ì¡Œë‹¤.**

---

> ì •ë¦¬í•˜ìë©´, StyleGANì€ **latent spaceë¥¼ disentangleí•˜ë„ë¡ êµ¬ì¡°ë¥¼ ì•½ê°„ ìˆ˜ì •**í•˜ì˜€ìœ¼ë©°, ì¶”ê°€ì ìœ¼ë¡œ latent space disentanglementë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆë„ë¡ **perceptual path lengthì™€ linear separability**ì˜ 2ê°€ì§€ metricì„ ì œì•ˆí•˜ì˜€ë‹¤. 

### Disentanglement

StyleGANì˜ architectureì— ëŒ€í•´ ì†Œê°œí•˜ê¸° ì•ì„œ, ì´ ë…¼ë¬¸ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” í‘œí˜„ì¸ Disentanglementì— ëŒ€í•´ ì„¤ëª…í•˜ê³ ì í•œë‹¤. (ë…¼ë¬¸ì˜ section4)

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan-5.PNG?raw=1" width = "600" ></p>

ë‹¤ì–‘í•œ styleì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë ¤ë©´, variationì˜ factorë¥¼ ì¡°ì •í•  ìˆ˜ ìˆì–´ì•¼í•œë‹¤. <span style='background-color: #E5EBF7;'> ì¦‰, **styleì„ ì¡°ê¸ˆì”© ì¡°ì •í•  ë•Œ latent spaceì—ì„œëŠ” linearí•˜ê²Œ ë³€í•˜ëŠ” ê²ƒì²˜ëŸ¼ í‘œí˜„ë˜ì–´ì•¼í•œë‹¤.** </span>

ìœ„ì˜ ê·¸ë¦¼ì—ì„œ ì´ìƒì ì¸ ê²½ìš°ëŠ” **(a)**ì´ë‹¤. (a)ì˜ latent spaceì—ì„œëŠ” ë‚¨ìì—ì„œ ì—¬ìë¡œ ê°ˆë•Œ linearí•˜ê²Œ attributeì´ ë³€í•˜ê²Œ ëœë‹¤.

ë°˜ë©´, **(b)**ëŠ” ê¸°ì¡´ì˜ generatorë“¤ì´ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, fixed distributionì—ì„œ latent spaceë¥¼ êµ¬ì„±í•œë‹¤.(**entangle**ëœ ìƒíƒœ). ì´ ê²½ìš°ì—ëŠ” ì„±ë³„ê³¼ ë¨¸ë¦¬ì˜ styleë“¤ì´ entangleë˜ì–´ ë‚¨ìì—ì„œ ì—¬ìë¡œ ê°ˆë•Œ ë¨¸ë¦¬ì˜ ê¸¸ì´ë„ ë³€í•˜ê²Œ ëœë‹¤. ì¦‰ styleë“¤ì´ ì„œë¡œ ë…ë¦½ì ì´ì§€ ì•Šê¸° ë•Œë¬¸ì— ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ë•Œì—ë„ ì˜í–¥ì„ ë¼ì¹˜ê²Œ ëœë‹¤.

ë”°ë¼ì„œ <span style='background-color: #FFF2CC;'> ëª©í‘œë¡œ í•˜ëŠ” ë°”ëŠ” **(c)ì²˜ëŸ¼ disentangleí•˜ê²Œ latent spaceë¥¼ ë§Œë“œëŠ” ê²ƒ**ì´ë‹¤. (c)ì™€ ê°™ì´ latent spaceë¥¼ $\mathcal{W}$ë¡œ mappingí•œë‹¤ë©´, ìš°ë¦¬ëŠ” ê° íŠ¹ì„±ì˜ ë³€í™”ë¥¼ linearí•˜ê²Œ ì¡°ì ˆí•  ìˆ˜ ìˆê²Œ ë  ê²ƒì´ë‹¤. </span>


## 2. Style-based Generator

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” style-based generatorë¥¼ ì‚¬ìš©í•œë‹¤. 

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan-2.PNG?raw=1" width = "600" ></p>

**Traditional GAN** (ê·¸ë¦¼ (a)) : <span style="background-color: #D5E0EF;">**latent vectorê°€ normalize ë˜ëŠ” êµ¬ì¡°**</span> ë¥¼ ë”°ë¥¸ë‹¤.
  - PGGANì´ë‚˜ BigGAN ë“±ì´ ì´ëŸ¬í•œ êµ¬ì¡°ë¥¼ ë”°ë¥¸ë‹¤.
    - PGGANì— ëŒ€í•œ ì„¤ëª…ì€ [ì´ ê¸€](https://happy-jihye.github.io/gan/gan-5/) ì°¸ê³ 
  - ì´ëŸ¬í•œ ë°©ì‹ì„ ë”°ë¥´ê²Œ ë˜ë©´ training dataê°€ latent spaceì˜ probability densityë¥¼ ë”°ë¼ì•¼ í•˜ê¸° ë•Œë¬¸ì— **entanglement**í•˜ê²Œ ëœë‹¤. ì¦‰, ì£¼ê·¼ê¹¨ë‚˜ ë¨¸ë¦¬ì²˜ëŸ¼ stochastic variationì„ ë³€ê²½í•˜ëŠ”ë° ì œí•œì´ ìƒê¸°ê²Œ ëœë‹¤. (ê° featureë“¤ì´ correlationì„ ê°–ê²Œ ë˜ì–´ ì£¼ê·¼ê¹¨ë¥¼ ë„£ìœ¼ë ¤ê³  í–ˆëŠ”ë°, ë¨¸ë¦¬ìƒ‰ê¹”ì´ ë°”ë€Œê²Œ ë˜ê±°ë‚˜ í•˜ëŠ” í˜„ìƒì´ ë°œìƒ)


> StyleGANì€ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì latent vector `z`ë¥¼ generatorì˜ inputìœ¼ë¡œ ë°”ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³ , **latent vectorë¥¼ non-linear mapping networkì— ë„£ì–´ disentanglement**í•˜ê²Œ ë§Œë“¤ê³ ì í•˜ì˜€ë‹¤. (ê° featureë“¤ì´ ìƒê´€ê´€ê³„ë¥¼ ê°–ì§€ ì•Šë„ë¡ ë³´ì •í•¨)

---

**Style-based Generator** (ê·¸ë¦¼ (b))

- ì„¤ëª…ì˜ codeëŠ” [Style-Based GAN in PyTorch](https://github.com/rosinality/style-based-gan-pytorch)ë¥¼ ì°¸ê³ í•˜ì˜€ë‹¤.

> ìš°ì„  latent space $\mathcal{Z}$ì—ì„œ non-linear mapping network  $f: \mathcal{Z} \rightarrow \mathcal{W}$ë¥¼ ê±°ì³ style code $\mathbf{w} \in \mathcal{W}$ë¥¼ ìƒì„±í•œë‹¤. ($\mathcal{W}$ : intermediate latent space)

---

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan-6.PNG?raw=1' width = '800' ></p>


> ì´í›„ì—ëŠ” ê° convolutional layerì— ìˆëŠ” **adaptive instance normalization(AdaIN)**ì„ ì‚¬ìš©í•˜ì—¬ generatorë¥¼ ì¡°ì ˆí•œë‹¤.

- <span style='background-color: #FFF2CC;'> **A-block** (learned affine transform) </span> : style codeì¸ $w$ë¥¼ ë½‘ê³ ë‚˜ë©´, [affine transformation](https://ko.wikipedia.org/wiki/%EC%95%84%ED%95%80_%EB%B3%80%ED%99%98)ì„ í†µí•´ $w$ë¥¼ style $y = (y_s, y_b)$ë¡œ ë°”ê¿”ì¤€ë‹¤. 

  (ì•„í•€ ë³€í™˜ì€ ì¼ì¢…ì˜ fc layerì´ë©°, ìœ„ì˜ ëª¨ë¸ì—ì„œ A-blockì€ ê°ê¸° ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ fc layer)

- AdaIN : feature map $x_i$ë¥¼ normalizeí•œ í›„, style vector $y$ë¡œ scaling & biasing
  
    $$\operatorname{AdaIN}\left(\mathbf{x}_{i}, \mathbf{y}\right)=\mathbf{y}_{s, i} \frac{\mathbf{x}_{i}-\mu\left(\mathbf{x}_{i}\right)}{\sigma\left(\mathbf{x}_{i}\right)}+\mathbf{y}_{b, i}$$


  ```python
  class AdaptiveInstanceNorm(nn.Module):
      def __init__(self, in_channel, style_dim):
          super().__init__()

          self.norm = nn.InstanceNorm2d(in_channel)
          # A-block : ê° convë§ˆë‹¤ channelì˜ ê°œìˆ˜ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— A-blockë“¤ì€ ì„œë¡œ ë‹¤ë¥¸ fc layer
          self.style = EqualLinear(style_dim, in_channel * 2)

          self.style.linear.bias.data[:in_channel] = 1
          self.style.linear.bias.data[in_channel:] = 0

      def forward(self, input, style):
          style = self.style(style).unsqueeze(2).unsqueeze(3)
          gamma, beta = style.chunk(2, 1)

          # Instance Normalize
          out = self.norm(input)
          # scaling & bias
          out = gamma * out + beta

          return out

  ```
  - [EqualLinear Code](https://github.com/rosinality/style-based-gan-pytorch/blob/b01ffcdcbca6d8bcbc5eb402c5d8180f4921aae4/model.py#L195)
  - Instance Normalizationì— ëŒ€í•œ ì„¤ëª…ì€ [ì´ ê¸€](https://m.blog.naver.com/chrhdhkd/222014776700)ì„ ì°¸ê³ 

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan-3.PNG?raw=1' width = '500' ></p>

---
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan-7.PNG?raw=1' width = '800' ></p>

> ë˜í•œ, íŠ¹ì´í•œ ì ì€ ë”ì´ìƒ ì²«ë²ˆì§¸ convolution layerì˜ inputì„ latent codeì—ì„œ feedingí•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì´ë‹¤. **ë³¸ ë…¼ë¬¸ì—ì„œëŠ” traditional input layerë¥¼ ì œê±°í•˜ê³  learned constant tensor `4 x 4 x 512`ì—ì„œ ì´ë¯¸ì§€ í•©ì„±ì„ ì‹œì‘í•œë‹¤.** 

- ì‰½ê²Œ ë§í•˜ìë©´, latent codeë¥¼ synthesis network $g$ì— ë„£ì–´ í•™ìŠµì„ ì‹œí‚¤ë©´ synthesis networkì˜ ì…ë ¥ìœ¼ë¡œ ì£¼ì—ˆë˜ latent code $z$ëŠ” ì˜ë¯¸ ì—†ì–´ì§„ë‹¤.
- input ìì²´ë¥¼ í•™ìŠµì‹œì¼°ê¸° ë•Œë¬¸ì— **Learned**ë¼ëŠ” í‘œí˜„ì„ ì“°ë©°, inferenceì‹œì— learned valueë“¤ì´ ê³ ì •ë˜ì–´ìˆê¸° ë•Œë¬¸ì— **Constant**ë¼ëŠ” í‘œí˜„ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.
- ì—¬ëŸ¬ ê°€ì§€ ì‚¬ì§„ì„ ë§Œë“¤ë•Œ, ì™¼ìª½ì˜ ë„¤íŠ¸ì›Œí¬ë§Œ ë°”ê¿”ì£¼ê³  ì˜¤ë¥¸ìª½ì˜ synthesis networkëŠ” ê³ ì •í•œë‹¤.

```python
class ConstantInput(nn.Module):
    def __init__(self, channel, size=4):
        super().__init__()

        self.input = nn.Parameter(torch.randn(1, channel, size, size))

    def forward(self, input):
        batch = input.shape[0]
        out = self.input.repeat(batch, 1, 1, 1)

        return out
```

---
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan-8.PNG?raw=1' width = '800' ></p>

> **B-block** (Stochastic Variation) : ë§ˆì§€ë§‰ìœ¼ë¡œëŠ” stochastic detailì„ ìƒì„±í•˜ê¸° ìœ„í•´ ê° convolution layerí›„ì— per-pixel noise inputì„ ë„£ì–´ì¤€ë‹¤.

- Gaussianì—ì„œ noiseë¥¼ samplingí•œ í›„, channel scaling factorë¥¼ í†µí•´ dimmensionì„ ë§ì¶°ì¤€ë‹¤.
- ì´ë•Œ noiseê°’ì„ multiplicationí•˜ê²Œ ë˜ë©´ ê²°ê³¼ê°’ì´ ì´ noiseì— ë„ˆë¬´ dependentí•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì— ë‹¨ìˆœíˆ addë§Œ í•´ì¤€ë‹¤. (noiseë¥¼ ë”í•´ì£¼ëŠ” ê±´ ë¨¸ë¦¬ì¹´ë½ì˜ ì›€ì§ì„ê³¼ ê°™ì€ ë¯¸ì„¸í•œ ë³€í™”ë§Œì„ ì¡°ì •í•´ì£¼ê¸° ìœ„í•´ ë”í•´ì£¼ëŠ” ê²ƒ)

```python
class NoiseInjection(nn.Module):
    def __init__(self, channel):
        super().__init__()

        self.weight = nn.Parameter(torch.zeros(1, channel, 1, 1))

    def forward(self, image, noise):
        return image + self.weight * noise
```

- ë‹¤ìŒ ê·¸ë¦¼ì„ ë³´ë©´, noiseë¥¼ ë³€í™”ì‹œí‚¬ ë•Œë§ˆë‹¤ ë¨¸ë¦¬ì¹´ë½ê³¼ ê°™ì€ stochastic detailë“¤ì´ ì¡°ê¸ˆì”© ë³€í™”í•œë‹¤.
 
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan-12.PNG?raw=1' width = '400' ></p>

- ë˜í•œ, Figure5ë¥¼ ë³´ë©´ ê° layerë§ˆë‹¤ ë”í•´ì§€ëŠ” noiseê°€ ê°ê¸° ë‹¤ë¥¸ stochastic detailì— ê´€ì—¬í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan-13.PNG?raw=1' width = '400' ></p>


- <span style='background-color: #FFF2CC;'> ì •ë¦¬í•˜ìë©´, **noiseë“¤ì€ localí•œ ë³€í™”ì—ë§Œ ì˜í–¥ì„ ì£¼ê³  ì „ì²´ì ì¸ íŠ¹ì„±ì„ ë°”ê¾¸ì§€ëŠ” ëª»í•œë‹¤.** </span>


---

> â­ A-blockì€ ì „ì²´ì ì¸ global attributeì´ë‚˜ identityë¥¼ ê²°ì •í•˜ê³ , B-blockì€ stochastic detailì„ ê²°ì •í•œë‹¤.

- ìš°ë¦¬ê°€ global attributeë¥¼ latent spaceë¡œ modelingí•˜ë ¤ë©´ spatial correlationì„ ê³ ë ¤í•´ì•¼í•˜ì§€ë§Œ, stochastic detailì„ modelingí•  ë•Œì—ëŠ” ì „ì²´ì ì¸ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì§€ ì•Šì•„ë„ ëœë‹¤. ë”°ë¼ì„œ detailí•œ ë³€í™”ë¥¼ ì£¼ê³ ì‹¶ì„ ë•Œì—ëŠ” spatially independent noiseë¥¼ ì‚¬ìš©í•œë‹¤.

## 3. Properties of the style-based generator

> â­ Our generator architecture makes it possible **to control the image synthesis via scale-specific modifications to the styles**. 
> 
> We can view the mapping network and affine transformations as a way to draw samples for each style from a learned distribution, and the synthesis network as a way to generate a novel image based on a collection of styles

### 3.1 Style Mixing

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan-9.PNG?raw=1' width = '500' ></p>

style mixingì€ ë‹¨ì–´ ê·¸ëŒ€ë¡œ styleì„ ì„ì–´ì£¼ëŠ” ê²ƒì´ë‹¤. latent code 2ê°œë¥¼ sampling ($z_1, z_2$)í•œ í›„, mapping networkë¥¼ ê±°ì³ 2ê°œì˜ style code($w_1, w_2$)ë¥¼ ë§Œë“œëŠ” ê²ƒì´ë‹¤. ì´í›„ ì–´ë– í•œ ê¸°ì¤€ì ì„ ì •í•œ í›„, ë‘ ê°œì˜ ìŠ¤íƒ€ì¼ì„ ë„£ì–´ì£¼ë©´ styleì´ ì„ì´ê²Œ ëœë‹¤

ì´ë ‡ê²Œ *mixing regularization*ì„ í•˜ë©´ styleë“¤ì´ correlationë˜ì§€ ì•Šê²Œ ëœë‹¤. 

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan-10.PNG?raw=1' width = '700' ></p>
---
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan-11.PNG?raw=1' width = '400' ></p>

## 4. Disentanglement studies

latent spaceì˜ interpolation ì •ë„ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•œ 2ê°€ì§€ì˜ metricì„ ì†Œê°œí•˜ë‚Ÿ.

### 4.1 Perceptual path length(PPL)

**The average perceptual path length in latent space $\mathcal{Z}$**

- $\mathbf{z}_{1}, \mathbf{z}_{2} \sim P(\mathbf{z}), t \sim U(0,1)$
- $\epsilon=10^{-4}$
- $\mathcal{Z}$ ìƒì—ì„œ interpolationì„ í•  ë•Œì—ëŠ” **spherical interpolation(slerp)**ì„ ì‚¬ìš©í•œë‹¤. 
- latent spaceì—ì„œ êµ‰ì¥íˆ ê°€ê¹Œì´ì— ìˆëŠ” ì´ë¯¸ì§€ë¥¼ 2ê°œ ìƒì„±í•œ í›„ì— ì´ ì´ë¯¸ì§€ë“¤ê°„ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•œë‹¤. ì´ ê°’ì´ ì‘ê²Œ ë‚˜ì˜¬ ìˆ˜ë¡ ì´ë¯¸ì§€ë“¤ì´ ë¹„ìŠ·í•œê±°ë‹ˆê¹Œ disentangleë˜ì—ˆë‹¤ê³  ìƒê°í•œë‹¤.
- epsilonì˜ ê°’ì— ë”°ë¼ ì°¨ì´ê°€ ë‚˜ì§€ ì•Šë„ë¡ epsilonì˜ ê°’ì— ë”°ë¼ normalizeë¥¼ í•´ì£¼ë©°, 10ë§Œê°œì˜ sampleì— ëŒ€í•´ í‰ê· ê°’ì„ ê³„ì‚°í•œë‹¤. 

$$\begin{array}{r}
l_{\mathcal{Z}}=\mathbb{E}\left[\frac { 1 } { \epsilon ^ { 2 } } d \left(G\left(\operatorname{slerp}\left(\mathbf{z}_{1}, \mathbf{z}_{2} ; t\right)\right)\right.\right.,\left.\left.G\left(\operatorname{slerp}\left(\mathbf{z}_{1}, \mathbf{z}_{2} ; t+\epsilon\right)\right)\right)\right]
\end{array}$$

**The average perceptual path length in latent space $\mathcal{W}$**
$$l_{\mathcal{W}}=\mathbb{E}\left[\frac { 1 } { \epsilon ^ { 2 } } d \left(\begin{array}{l}
g\left(\operatorname{lerp}\left(f\left(\mathbf{z}_{1}\right), f\left(\mathbf{z}_{2}\right) ; t\right)\right),\left.\left.g\left(\operatorname{lerp}\left(f\left(\mathbf{z}_{1}\right), f\left(\mathbf{z}_{2}\right) ; t+\epsilon\right)\right)\right)\right]
\end{array}\right.\right.$$

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan-14.PNG?raw=1' width = '600' ></p>

 $\mathcal{Z}$ë³´ë‹¤ëŠ”  $\mathcal{W}$ë¥¼ ì‚¬ìš©í–ˆì„ ë•Œ, ì¼ë°˜ ëª¨ë¸ë³´ë‹¤ëŠ” AdaINì„ ì‚¬ìš©í•œ style-based modelì„ ì‚¬ìš©í–ˆì„ ë•Œ disentanglementí•˜ë‹¤.


### 4.2 Linear separability 

ìƒëµ

## 5. Code 

### 5.1 Generator
```python
class StyledConvBlock(nn.Module):
    def __init__(
        self,
        in_channel,
        out_channel,
        kernel_size=3,
        padding=1,
        style_dim=512,
        initial=False,
        upsample=False,
        fused=False,
    ):
        super().__init__()

        # Learned constant tensor (4x4x512)
        if initial:
            self.conv1 = ConstantInput(in_channel)

        else:
            if upsample:
                if fused:
                    self.conv1 = nn.Sequential(
                        FusedUpsample(
                            in_channel, out_channel, kernel_size, padding=padding
                        ),
                        Blur(out_channel),
                    )

                else:
                    self.conv1 = nn.Sequential(
                        nn.Upsample(scale_factor=2, mode='nearest'),
                        EqualConv2d(
                            in_channel, out_channel, kernel_size, padding=padding
                        ),
                        Blur(out_channel),
                    )

            else:
                self.conv1 = EqualConv2d(
                    in_channel, out_channel, kernel_size, padding=padding
                )

        self.noise1 = equal_lr(NoiseInjection(out_channel))
        self.adain1 = AdaptiveInstanceNorm(out_channel, style_dim)
        self.lrelu1 = nn.LeakyReLU(0.2)

        self.conv2 = EqualConv2d(out_channel, out_channel, kernel_size, padding=padding)
        self.noise2 = equal_lr(NoiseInjection(out_channel))
        self.adain2 = AdaptiveInstanceNorm(out_channel, style_dim)
        self.lrelu2 = nn.LeakyReLU(0.2)

    def forward(self, input, style, noise):
        out = self.conv1(input)
        out = self.noise1(out, noise)
        out = self.lrelu1(out)
        out = self.adain1(out, style)

        out = self.conv2(out)
        out = self.noise2(out, noise)
        out = self.lrelu2(out)
        out = self.adain2(out, style)

        return out
```


- EqualConv2d
  
```python
class EqualConv2d(nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__()

        conv = nn.Conv2d(*args, **kwargs)
        conv.weight.data.normal_()
        conv.bias.data.zero_()
        self.conv = equal_lr(conv)

    def forward(self, input):
        return self.conv(input)
```

- Generator

> StyleGANì€ [PGGAN](https://happy-jihye.github.io/gan/gan-5/) architectureë¥¼ ë”°ë¥¸ë‹¤.

```python
class Generator(nn.Module):
    def __init__(self, code_dim, fused=True):
        super().__init__()

        self.progression = nn.ModuleList(
            [
                StyledConvBlock(512, 512, 3, 1, initial=True),  # 4
                StyledConvBlock(512, 512, 3, 1, upsample=True),  # 8
                StyledConvBlock(512, 512, 3, 1, upsample=True),  # 16
                StyledConvBlock(512, 512, 3, 1, upsample=True),  # 32
                StyledConvBlock(512, 256, 3, 1, upsample=True),  # 64
                StyledConvBlock(256, 128, 3, 1, upsample=True, fused=fused),  # 128
                StyledConvBlock(128, 64, 3, 1, upsample=True, fused=fused),  # 256
                StyledConvBlock(64, 32, 3, 1, upsample=True, fused=fused),  # 512
                StyledConvBlock(32, 16, 3, 1, upsample=True, fused=fused),  # 1024
            ]
        )

        self.to_rgb = nn.ModuleList(
            [
                EqualConv2d(512, 3, 1),
                EqualConv2d(512, 3, 1),
                EqualConv2d(512, 3, 1),
                EqualConv2d(512, 3, 1),
                EqualConv2d(256, 3, 1),
                EqualConv2d(128, 3, 1),
                EqualConv2d(64, 3, 1),
                EqualConv2d(32, 3, 1),
                EqualConv2d(16, 3, 1),
            ]
        )

        # self.blur = Blur()

    def forward(self, style, noise, step=0, alpha=-1, mixing_range=(-1, -1)):
        out = noise[0]

        if len(style) < 2:
            inject_index = [len(self.progression) + 1]

        else:
            inject_index = sorted(random.sample(list(range(step)), len(style) - 1))

        crossover = 0

        for i, (conv, to_rgb) in enumerate(zip(self.progression, self.to_rgb)):
            if mixing_range == (-1, -1):
                if crossover < len(inject_index) and i > inject_index[crossover]:
                    crossover = min(crossover + 1, len(style))

                style_step = style[crossover]

            else:
                if mixing_range[0] <= i <= mixing_range[1]:
                    style_step = style[1]

                else:
                    style_step = style[0]

            if i > 0 and step > 0:
                out_prev = out
                
            out = conv(out, style_step, noise[i])

            if i == step:
                out = to_rgb(out)

                if i > 0 and 0 <= alpha < 1:
                    skip_rgb = self.to_rgb[i - 1](out_prev)
                    skip_rgb = F.interpolate(skip_rgb, scale_factor=2, mode='nearest')
                    out = (1 - alpha) * skip_rgb + alpha * out

                break

        return out
```
- Styled Generator
  
```python
class StyledGenerator(nn.Module):
    def __init__(self, code_dim=512, n_mlp=8):
        super().__init__()

        self.generator = Generator(code_dim)

        layers = [PixelNorm()]
        for i in range(n_mlp):
            layers.append(EqualLinear(code_dim, code_dim))
            layers.append(nn.LeakyReLU(0.2))

        self.style = nn.Sequential(*layers)

    def forward(
        self,
        input,
        noise=None,
        step=0,
        alpha=-1,
        mean_style=None,
        style_weight=0,
        mixing_range=(-1, -1),
    ):
        styles = []
        if type(input) not in (list, tuple):
            input = [input]

        for i in input:
            styles.append(self.style(i))

        batch = input[0].shape[0]

        if noise is None:
            noise = []

            for i in range(step + 1):
                size = 4 * 2 ** i
                noise.append(torch.randn(batch, 1, size, size, device=input[0].device))

        if mean_style is not None:
            styles_norm = []

            for style in styles:
                styles_norm.append(mean_style + style_weight * (style - mean_style))

            styles = styles_norm

        return self.generator(styles, noise, step, alpha, mixing_range=mixing_range)

    def mean_style(self, input):
        style = self.style(input).mean(0, keepdim=True)

        return style

```

### 5.2 Discriminator

```python
class Discriminator(nn.Module):
    def __init__(self, fused=True, from_rgb_activate=False):
        super().__init__()

        self.progression = nn.ModuleList(
            [
                ConvBlock(16, 32, 3, 1, downsample=True, fused=fused),  # 512
                ConvBlock(32, 64, 3, 1, downsample=True, fused=fused),  # 256
                ConvBlock(64, 128, 3, 1, downsample=True, fused=fused),  # 128
                ConvBlock(128, 256, 3, 1, downsample=True, fused=fused),  # 64
                ConvBlock(256, 512, 3, 1, downsample=True),  # 32
                ConvBlock(512, 512, 3, 1, downsample=True),  # 16
                ConvBlock(512, 512, 3, 1, downsample=True),  # 8
                ConvBlock(512, 512, 3, 1, downsample=True),  # 4
                ConvBlock(513, 512, 3, 1, 4, 0),
            ]
        )

        def make_from_rgb(out_channel):
            if from_rgb_activate:
                return nn.Sequential(EqualConv2d(3, out_channel, 1), nn.LeakyReLU(0.2))

            else:
                return EqualConv2d(3, out_channel, 1)

        self.from_rgb = nn.ModuleList(
            [
                make_from_rgb(16),
                make_from_rgb(32),
                make_from_rgb(64),
                make_from_rgb(128),
                make_from_rgb(256),
                make_from_rgb(512),
                make_from_rgb(512),
                make_from_rgb(512),
                make_from_rgb(512),
            ]
        )

        # self.blur = Blur()

        self.n_layer = len(self.progression)

        self.linear = EqualLinear(512, 1)

    def forward(self, input, step=0, alpha=-1):
        for i in range(step, -1, -1):
            index = self.n_layer - i - 1

            if i == step:
                out = self.from_rgb[index](input)

            if i == 0:
                out_std = torch.sqrt(out.var(0, unbiased=False) + 1e-8)
                mean_std = out_std.mean()
                mean_std = mean_std.expand(out.size(0), 1, 4, 4)
                out = torch.cat([out, mean_std], 1)

            out = self.progression[index](out)

            if i > 0:
                if i == step and 0 <= alpha < 1:
                    skip_rgb = F.avg_pool2d(input, 2)
                    skip_rgb = self.from_rgb[index + 1](skip_rgb)

                    out = (1 - alpha) * skip_rgb + alpha * out

        out = out.squeeze(2).squeeze(2)
        # print(input.size(), out.size(), step)
        out = self.linear(out)

        return out
```

### 5.3 Training

```python
def train(args, dataset, generator, discriminator):
    step = int(math.log2(args.init_size)) - 2
    resolution = 4 * 2 ** step
    loader = sample_data(
        dataset, args.batch.get(resolution, args.batch_default), resolution
    )
    data_loader = iter(loader)

    adjust_lr(g_optimizer, args.lr.get(resolution, 0.001))
    adjust_lr(d_optimizer, args.lr.get(resolution, 0.001))

    pbar = tqdm(range(3_000_000))

    requires_grad(generator, False)
    requires_grad(discriminator, True)

    disc_loss_val = 0
    gen_loss_val = 0
    grad_loss_val = 0

    alpha = 0
    used_sample = 0

    max_step = int(math.log2(args.max_size)) - 2
    final_progress = False

    for i in pbar:
        discriminator.zero_grad()

        alpha = min(1, 1 / args.phase * (used_sample + 1))

        if (resolution == args.init_size and args.ckpt is None) or final_progress:
            alpha = 1

        if used_sample > args.phase * 2:
            used_sample = 0
            step += 1

            if step > max_step:
                step = max_step
                final_progress = True
                ckpt_step = step + 1

            else:
                alpha = 0
                ckpt_step = step

            resolution = 4 * 2 ** step

            loader = sample_data(
                dataset, args.batch.get(resolution, args.batch_default), resolution
            )
            data_loader = iter(loader)

            torch.save(
                {
                    'generator': generator.module.state_dict(),
                    'discriminator': discriminator.module.state_dict(),
                    'g_optimizer': g_optimizer.state_dict(),
                    'd_optimizer': d_optimizer.state_dict(),
                    'g_running': g_running.state_dict(),
                },
                f'checkpoint/train_step-{ckpt_step}.model',
            )

            adjust_lr(g_optimizer, args.lr.get(resolution, 0.001))
            adjust_lr(d_optimizer, args.lr.get(resolution, 0.001))

        try:
            real_image = next(data_loader)

        except (OSError, StopIteration):
            data_loader = iter(loader)
            real_image = next(data_loader)

        used_sample += real_image.shape[0]

        b_size = real_image.size(0)
        real_image = real_image.cuda()

        if args.loss == 'wgan-gp':
            real_predict = discriminator(real_image, step=step, alpha=alpha)
            real_predict = real_predict.mean() - 0.001 * (real_predict ** 2).mean()
            (-real_predict).backward()

        elif args.loss == 'r1':
            real_image.requires_grad = True
            real_scores = discriminator(real_image, step=step, alpha=alpha)
            real_predict = F.softplus(-real_scores).mean()
            real_predict.backward(retain_graph=True)

            grad_real = grad(
                outputs=real_scores.sum(), inputs=real_image, create_graph=True
            )[0]
            grad_penalty = (
                grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2
            ).mean()
            grad_penalty = 10 / 2 * grad_penalty
            grad_penalty.backward()
            if i%10 == 0:
                grad_loss_val = grad_penalty.item()

        if args.mixing and random.random() < 0.9:
            gen_in11, gen_in12, gen_in21, gen_in22 = torch.randn(
                4, b_size, code_size, device='cuda'
            ).chunk(4, 0)
            gen_in1 = [gen_in11.squeeze(0), gen_in12.squeeze(0)]
            gen_in2 = [gen_in21.squeeze(0), gen_in22.squeeze(0)]

        else:
            gen_in1, gen_in2 = torch.randn(2, b_size, code_size, device='cuda').chunk(
                2, 0
            )
            gen_in1 = gen_in1.squeeze(0)
            gen_in2 = gen_in2.squeeze(0)

        fake_image = generator(gen_in1, step=step, alpha=alpha)
        fake_predict = discriminator(fake_image, step=step, alpha=alpha)

        if args.loss == 'wgan-gp':
            fake_predict = fake_predict.mean()
            fake_predict.backward()

            eps = torch.rand(b_size, 1, 1, 1).cuda()
            x_hat = eps * real_image.data + (1 - eps) * fake_image.data
            x_hat.requires_grad = True
            hat_predict = discriminator(x_hat, step=step, alpha=alpha)
            grad_x_hat = grad(
                outputs=hat_predict.sum(), inputs=x_hat, create_graph=True
            )[0]
            grad_penalty = (
                (grad_x_hat.view(grad_x_hat.size(0), -1).norm(2, dim=1) - 1) ** 2
            ).mean()
            grad_penalty = 10 * grad_penalty
            grad_penalty.backward()
            if i%10 == 0:
                grad_loss_val = grad_penalty.item()
                disc_loss_val = (-real_predict + fake_predict).item()

        elif args.loss == 'r1':
            fake_predict = F.softplus(fake_predict).mean()
            fake_predict.backward()
            if i%10 == 0:
                disc_loss_val = (real_predict + fake_predict).item()

        d_optimizer.step()

        if (i + 1) % n_critic == 0:
            generator.zero_grad()

            requires_grad(generator, True)
            requires_grad(discriminator, False)

            fake_image = generator(gen_in2, step=step, alpha=alpha)

            predict = discriminator(fake_image, step=step, alpha=alpha)

            if args.loss == 'wgan-gp':
                loss = -predict.mean()

            elif args.loss == 'r1':
                loss = F.softplus(-predict).mean()

            if i%10 == 0:
                gen_loss_val = loss.item()

            loss.backward()
            g_optimizer.step()
            accumulate(g_running, generator.module)

            requires_grad(generator, False)
            requires_grad(discriminator, True)

        if (i + 1) % 100 == 0:
            images = []

            gen_i, gen_j = args.gen_sample.get(resolution, (10, 5))

            with torch.no_grad():
                for _ in range(gen_i):
                    images.append(
                        g_running(
                            torch.randn(gen_j, code_size).cuda(), step=step, alpha=alpha
                        ).data.cpu()
                    )

            utils.save_image(
                torch.cat(images, 0),
                f'sample/{str(i + 1).zfill(6)}.png',
                nrow=gen_i,
                normalize=True,
                range=(-1, 1),
            )

        if (i + 1) % 10000 == 0:
            torch.save(
                g_running.state_dict(), f'checkpoint/{str(i + 1).zfill(6)}.model'
            )

        state_msg = (
            f'Size: {4 * 2 ** step}; G: {gen_loss_val:.3f}; D: {disc_loss_val:.3f};'
            f' Grad: {grad_loss_val:.3f}; Alpha: {alpha:.5f}'
        )

        pbar.set_description(state_msg)


if __name__ == '__main__':
    code_size = 512
    batch_size = 16
    n_critic = 1

    parser = argparse.ArgumentParser(description='Progressive Growing of GANs')

    parser.add_argument('path', type=str, help='path of specified dataset')
    parser.add_argument(
        '--phase',
        type=int,
        default=600_000,
        help='number of samples used for each training phases',
    )
    parser.add_argument('--lr', default=0.001, type=float, help='learning rate')
    parser.add_argument('--sched', action='store_true', help='use lr scheduling')
    parser.add_argument('--init_size', default=8, type=int, help='initial image size')
    parser.add_argument('--max_size', default=1024, type=int, help='max image size')
    parser.add_argument(
        '--ckpt', default=None, type=str, help='load from previous checkpoints'
    )
    parser.add_argument(
        '--no_from_rgb_activate',
        action='store_true',
        help='use activate in from_rgb (original implementation)',
    )
    parser.add_argument(
        '--mixing', action='store_true', help='use mixing regularization'
    )
    parser.add_argument(
        '--loss',
        type=str,
        default='wgan-gp',
        choices=['wgan-gp', 'r1'],
        help='class of gan loss',
    )

    args = parser.parse_args()

    generator = nn.DataParallel(StyledGenerator(code_size)).cuda()
    discriminator = nn.DataParallel(
        Discriminator(from_rgb_activate=not args.no_from_rgb_activate)
    ).cuda()
    g_running = StyledGenerator(code_size).cuda()
    g_running.train(False)

    g_optimizer = optim.Adam(
        generator.module.generator.parameters(), lr=args.lr, betas=(0.0, 0.99)
    )
    g_optimizer.add_param_group(
        {
            'params': generator.module.style.parameters(),
            'lr': args.lr * 0.01,
            'mult': 0.01,
        }
    )
    d_optimizer = optim.Adam(discriminator.parameters(), lr=args.lr, betas=(0.0, 0.99))

    accumulate(g_running, generator.module, 0)

    if args.ckpt is not None:
        ckpt = torch.load(args.ckpt)

        generator.module.load_state_dict(ckpt['generator'])
        discriminator.module.load_state_dict(ckpt['discriminator'])
        g_running.load_state_dict(ckpt['g_running'])
        g_optimizer.load_state_dict(ckpt['g_optimizer'])
        d_optimizer.load_state_dict(ckpt['d_optimizer'])

    transform = transforms.Compose(
        [
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True),
        ]
    )

    dataset = MultiResolutionDataset(args.path, transform)

    if args.sched:
        args.lr = {128: 0.0015, 256: 0.002, 512: 0.003, 1024: 0.003}
        args.batch = {4: 512, 8: 256, 16: 128, 32: 64, 64: 32, 128: 32, 256: 32}

    else:
        args.lr = {}
        args.batch = {}

    args.gen_sample = {512: (8, 4), 1024: (4, 2)}

    args.batch_default = 32

    train(args, dataset, generator, discriminator)
```

---

## 6. Result
### 6.1 Quality of generated images

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan-4.PNG?raw=1' width = '500' ></p>

CELEBA-HGì™€ ì´ ë…¼ë¬¸ì„ í†µí•´ Nvidiaê°€ ìƒˆë¡­ê²Œ ê³µê°œí•œ ë°ì´í„°ì…‹ [FFHQ](https://github.com/NVlabs/ffhq-dataset)ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ì˜ baselineê³¼ ë¹„êµë¥¼ í–ˆë‹¤.

baselineì¸ PGGAN ëª¨ë¸ê³¼ ë¹„êµí–ˆì„ ë•Œ styleganì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ëƒ„ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 

### 6.2 Prior art

- <span style='background-color: #E5EBF7;'> **discriminator**ë¥¼ í–¥ìƒì‹œí‚¤ë ¤ë˜ ì—°êµ¬ë“¤ </span>
  - multiple discriminators
  - multi-resolution discrimination
  - self-attention
- <span style='background-color: #E2F0D9;'> **generator**ë¥¼ í–¥ìƒì‹œí‚¤ë ¤ëŠ” ì—°êµ¬ë“¤ </span> 
  - the exact distribution in the input latent space [5] or 
  - shaping the input latent space via Gaussian mixture models
  - clustering
  - encouraging convexity

## 7. Conclusion

> StyleGANì€ high-level attributesì™€ stochastic effectsë¥¼ ì˜ ë‚˜ëˆ ì„œ í•™ìŠµì‹œí‚¤ê³ , intermediate latent spaceë¥¼ linearí•˜ê²Œ ë§Œë“¦ìœ¼ë¡œì¨ **ê³ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±**í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ë‹¤.

## 8. Opinion

> âœğŸ» ê·¸ë™ì•ˆì€ ì˜ ìƒì„±í•˜ì§€ ëª»í–ˆë˜ ê³ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•œ ë†€ë¼ìš´ ëª¨ë¸ì¸ ê²ƒ ê°™ë‹¤.(styleë“¤ì˜ ì¢…ë¥˜ë¥¼ attributeê³¼ stochastic detailë¡œ ë‚˜ëˆˆ í›„ ê°ê°ì˜ ìƒì„±ë°©ì‹ì„ ì „ê°œí•œ ê²ƒë„ í¥ë¯¸ë¡­ë‹¤.) ìƒˆë¡œìš´ datasetì„ ê³µê°œí•˜ê³ , architecture ë¿ë§Œ ì•„ë‹ˆë¼ disentanglementê¹Œì§€ ì¸¡ì •í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì œì•ˆì„ í•œ ê²ƒì„ ë³´ë©´ nvidiaì—ì„œ ì´ë¥¼ ê°ˆê³  ë‚¸ ë…¼ë¬¸ì´ë¼ê³  ìƒê°ì´ ë“ ë‹¤. 
>
> PGGANì„ ë°œì „ì‹œí‚¨ ë°©ì‹ë„ êµ‰ì¥íˆ í¥ë¯¸ë¡œì› ë‹¤. ì´ ë…¼ë¬¸ì˜ í›„ì† ì—°êµ¬ì¸ stylegan2ë„ ê¶ê¸ˆí•˜ë‹¤!

---
**Reference**
- [medium blog/syncedreview](https://medium.com/syncedreview/nvidia-open-sources-hyper-realistic-face-generator-stylegan-f346e1a73826+4lz)
- [rosinality/style-based-gan-pytorch](https://github.com/rosinality/style-based-gan-pytorch)
- Naver AI LAB ìµœìœ¤ì œ ì—°êµ¬ì›ë‹˜ ë°œí‘œìë£Œ