---
title: "[Paper Review] BicycleGAN : Toward Multimodal Image-to-Image Translation 논문 분석"
excerpt: "Multimodal Image-to-Image translation task에 대해 현실적이고 다양한 이미지를 생성하는, BicycleGAN model에 대해 알아본다."


categories:
 - GAN
tags:
  - deeplearning
  - ai
  - pytorch
  - GAN
  - vision
search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---

> ✍🏻 이번 포스팅에서는 Multimodal Image-to-Image translation task에 대해 현실적이고 다양한 이미지를 생성하는,  **BicycleGAN model**에 대해 살펴본다.


- Paper : [[BicycleGAN] Toward Multimodal Image-to-Image Translation](https://arxiv.org/abs/1711.11586) (NeurIPS 2018 / Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, Eli Shechtman)

- [GAN-Zoos! (GAN 포스팅 모음집)](https://happy-jihye.github.io/gan/)

---

기존의 <span style='background-color: #E5EBF7;'> image-to-image translation 연구들은 여러개의 latent code에서 하나의 output으로 many-to-one mapping 되는 **mode collapse** 문제 </span>가 있었다. 

본 논문에서는 이를 해결하기 위해 output에서 latent code로 가는 mapping 네트워크를 학습시켰다.(bijective consistency) 또한, 다양한 objection function과 network architecture, latent code들을 제시하여 현실적이고 다양한 이미지를 생성하였다.

> ⭐ BicycleGAN
> - image-to-image translation의 mode collapse 문제 해결
> - 다양한 style의 이미지를 생성할 수 있게 됨(multimodal)

## 1. Introduction

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/bicyclegan1.PNG?raw=1' width = '800' ></p>

> 그동안의 생성모델은 하나의 이미지를 생성했다면, 본 논문은 **하나의 input으로 부터 여러 개의 이미지를 생성하는 Multimodal Image-to-Image Translation에 관한 연구**이다. BicycleGAN은 **(1) 현실적이고 (2) 다양한 이미지 생성**을 목표로 한다.

Multimodality한 이미지를 만드려면 low-dimensional latent code의 확률분포와 input 이미지로 output 이미지를 생성할 수 있어야한다. 또한, mode-collapse가 생기지 않도록 아주 작은 수의 input을 output과 mapping 해야한다.

---

<span style='background-color: #E5EBF7;'>**Pix2Pix**</span>

본 논문은 paired dataset에 대해 고해상도의 이미지를 생성한 **pix2pix framework**를 기반으로 하였다.
- [[Paper Review] Pix2pix : Image-to-Image Translation with Conditional Adversarial Networks 논문 분석](https://happy-jihye.github.io/gan/gan-8/)

pix2pix의 loss function은 latent code와 실제 input image를 모두 입력으로 받아 학습을 한다. 

이때, 문제는 Generator가 학습 시에 latent code를 사용하지 않는다는 것이다. (latent code를 넣었을 때나 뺐을 때 모두 비슷하게 학습이 됨) 

따라서 본 논문은 training과정에서 latent code를 활용하도록 latent space와 output이 bijetion한 관계를 갖게 하였다.

---

<span style='background-color: #E5EBF7;'> **CycleGAN** </span>

또한, BicycleGAN은 CycleGAN처럼 **(1) latent code를 output으로 mapping하고 (2) output 이미지를 latent space로 다시 복구하는 encoder를 학습**하여 여러개의 latent code가 하나의 image를 생성하지 않도록 한다.(mode collapse 예방)

- [[Paper Review] CycleGAN : Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks 논문 분석](https://happy-jihye.github.io/gan/gan-10/)

---

<span style='background-color: #E5EBF7;'> **Objective Functions** </span>
- **cVAE-GAN**
  - ground truth image를 latent space로 encoding

- **cLR-GAN**
  - latent vector -> G 로 학습 : 이렇게 되면 생성된 이미지는 현실적이지만, ground truth랑 비슷하지는 않음
  - encoder는 output image에서 latent vector로 복원이 되도록 학습
  - latent regressor, infoGAN

- **BicycleGAN**
  - latent encoding과 output의 bijective consistency를 학습!

## 2. Related Work

<span style='background-color: #E5EBF7;'> **Generative modeling** </span>

다양한 생성 모델이 있지만 GAN이 효과적이다.

---

<span style='background-color: #E5EBF7;'> **Conditional Image Generation** </span>

- conditional VAEs
- autoregressive models
- image-to-image conditional GANs
  - (좋은 성능) 고해상도의 이미지를 생성할 수 있다
  - 다만, Generator가 random noise를 무시하고 conditional information만을 이용하여 이미지를 생성하기 때문에 다양성이 떨어진다. (noise를 무시하면 학습이 더 안정적으로 됨)

---

<span style='background-color: #E5EBF7;'> **Explicitly-encoded multimodality** </span>

latent와 image space가 tight하게 연결되도록 이 둘을 encoding한 후, generator에 넣어 학습을 시킨다.

## 3. Multimodal Image-to-Image Translation

> ⭐Goal : 본 논문은 간단한 신발의 edge로부터 여러 형태의 신발을 만들어내는 것처럼 **두 image domain간의 multi-modal mapping을 학습**하는 것이 목표이다.


- input domain : $\mathcal{A} \subset \mathbb{R}^{H \times W \times 3}$
- output domain : $\mathcal{B} \subset \mathbb{R}^{H \times W \times 3}$

input instance $A$에 대해 다양한 paired instance $B$를 만들어야하지만, training을 할 때에는 (input, output)의 하나의 data pair를 이용하여 훈련을 한다. 

따라서 본 모델은 test time 때 $p(\mathbf{B} \mid \mathbf{A})$로 부터 다양한 output $\mathcal{\hat{B}}$ 을 sampling하도록 학습을 한다.

$$G:(\mathbf{A}, \mathbf{z}) \rightarrow \mathbf{B}$$

---

### 3.1 BaseLine : Pix2pix + noise ($\mathbf{z} \rightarrow \widehat{\mathbf{B}}$)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/bicyclegan2.PNG?raw=1' width = '800' ></p>

baseline으로는 pix2pix의 모델을 사용한다. 다만, pix2pix의 식을 그대로 사용한다면 학습시에 noise를 무시하는 문제가 생기므로 noise를 고려하도록 모델을 수정한다. (3.2, 3.3)

$$\mathcal{L}_{\mathrm{GAN}}(G, D)=\mathbb{E}_{\mathbf{A}, \mathbf{B} \sim p(\mathbf{A}, \mathbf{B})}[\log (D(\mathbf{A}, \mathbf{B}))]+\mathbb{E}_{\mathbf{A} \sim p(\mathbf{A}), \mathbf{z} \sim p(\mathbf{z})}[\log (1-D(\mathbf{A}, G(\mathbf{A}, \mathbf{z})))]$$ 

$$(G)=\mathbb{E}_{\mathbf{A . B} \sim p(\mathbf{A}, \mathbf{B}), \mathbf{z} \sim p(\mathbf{z})}\|\mathbf{B}-G(\mathbf{A}, \mathbf{z})\|_{1}$$

$$G^{*}=\arg \min _{G} \max _{D} \quad \mathcal{L}_{\mathrm{GAN}}(G, D)+\lambda \mathcal{L}_{1}^{\mathrm{image}}(G)$$

### 3.2 Conditional Variational Autoencoder GAN : cVAE-GAN ($\mathbf{B} \rightarrow \mathbf{z} \rightarrow \widehat{\mathbf{B}}$)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/bicyclegan3.PNG?raw=1' width = '500' ></p>

latent vector를 일반 Gaussian 분포에서 뽑으면, noise가 학습 시에 무시될 가능성이 높다. 따라서 latent vector와 ground truth가 연관이 될 수 있도록 <span style='background-color: #E5EBF7;'> **latent vector를 B와 관련된 gaussian 확률분포에서 sampling** </span>하도록 모델을 수정한다.

- $Q(\mathbf{z} \mid \mathbf{B})$가 B의 확률 분포에서 latent vector를 sampling하는 것처럼 되도록 Encoder를 사용한다. 

  $$\mathbf{z} \sim E(\mathbf{B}), Q(\mathbf{z} \mid \mathbf{B}) \triangleq E(\mathbf{B})$$
- 또한, $E(\mathbf{B})$에 의해 encoding된 latent distribution이 random Gaussian 분포와 비슷하도록 아래의 loss를 학습시킨다.

  $$\mathcal{L}_{\mathrm{KL}}(E)=\mathbb{E}_{\mathbf{B} \sim p(\mathbf{B})}\left[\mathcal{D}_{\mathrm{KL}}(E(\mathbf{B}) \| \mathcal{N}(0, I))\right]$$


<span style='background-color: #E5EBF7;'> **수정된 Objection Function** </span>

$$\mathcal{L}_{\mathrm{GAN}}^{\mathrm{VAE}}=\mathbb{E}_{\mathbf{A}, \mathbf{B} \sim p(\mathbf{A}, \mathbf{B})}[\log (D(\mathbf{A}, \mathbf{B}))]+\mathbb{E}_{\mathbf{A}, \mathbf{B} \sim p(\mathbf{A}, \mathbf{B}), \mathbf{z} \sim E(\mathbf{B})}[\log (1-D(\mathbf{A}, G(\mathbf{A}, \mathbf{z})))]$$

$$\mathcal{L}_{1}^{\mathrm{VAE}}(G)=\mathbb{E}_{\mathbf{A}, \mathbf{B} \sim p(\mathbf{A}, \mathbf{B}), \mathbf{z} \sim E(\mathbf{B})}\|\mathbf{B}-G(\mathbf{A}, \mathbf{z})\|_{1}$$

$$G^{*}, E^{*}=\arg \min _{G, E} \max _{D} \quad \mathcal{L}_{\mathrm{GAN}}^{\mathrm{VAE}}(G, D, E)+\lambda \mathcal{L}_{1}^{\mathrm{VAE}}(G, E)+\lambda_{\mathrm{KL}} \mathcal{L}_{\mathrm{KL}}(E)$$

---

### 3.3 Conditional Latent Regressor GAN: cLR-GAN $(\mathbf{z} \rightarrow \widehat{\mathbf{B}} \rightarrow \widehat{\mathbf{z}})$

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/bicyclegan4.PNG?raw=1' width = '500' ></p>

latent vector를 잘 사용하기 위한 방법으로는 **latent regressor** model과 유사한 objection function을 사용하는 방식이 있다.

이때는 latent code `z`를 random으로 sampling한 후, Encoder를 활용하여 생성된 이미지를 다시 $\hat{z}$로 복원시키도록 학습을 한다. $\widehat{\mathbf{z}}=E(G(\mathbf{A}, \mathbf{z}))$

$$\mathcal{L}_{1}^{\text {latent }}(G, E)=\mathbb{E}_{\mathbf{A} \sim p(\mathbf{A}), \mathbf{z} \sim p(\mathbf{z})}\|\mathbf{z}-E(G(\mathbf{A}, \mathbf{z}))\|_{1}$$

최종 object function이다. 이 경우에는 L1 loss를 사용하지 않아도 된다.

$$G^{*}, E^{*}=\arg \min _{G, E} \max _{D} \quad \mathcal{L}_{\mathrm{GAN}}(G, D)+\lambda_{\text {latent }} \mathcal{L}_{1}^{\text {latent }}(G, E)$$

---

### 3.4 BicycleGAN : Hybrid Model!



> **bicycleGAN은 3.2와 3.3의 cVAE-GAN과 cLR-GAN의 objective function을 모두 사용**한다.
> 
> cVAE-GAN을 사용하므로 생성된 이미지는 ground truth의 input-output pair와 유사하며, cLR-GAN을 사용하므로 생성된 이미지는 현실적이다. 두 cycle의 이점을 합친게 **bicycle GAN**!! $(\mathbf{B} \rightarrow \mathbf{z} \rightarrow \mathbf{B} \operatorname{and} \mathbf{z} \rightarrow \mathbf{B} \rightarrow \widehat{\mathbf{z}})$

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/bicyclegan5.PNG?raw=1' width = '800' ></p>


$$\begin{aligned}
G^{*}, E^{*}=\arg \min _{G, E} \max _{D} & \mathcal{L}_{\mathrm{GAN}}^{\mathrm{VAE}}(G, D, E)+\lambda \mathcal{L}_{1}^{\mathrm{VAE}}(G, E) \\
&+\mathcal{L}_{\mathrm{GAN}}(G, D)+\lambda_{\text {latent }} \mathcal{L}_{1}^{\text {latent }}(G, E)+\lambda_{\mathrm{KL}} \mathcal{L}_{\mathrm{KL}}(E),
\end{aligned}$$

## 4. Implementation

- Generator : U-Net, skip connection 사용
- Discriminator : 2개의 patchGAN
  - [pix2pix의 architecture](https://happy-jihye.github.io/gan/gan-8/#32-network-architectures)
- variant으로는 LSGAN을 참고

자세한 내용은 생략

## 5. Experiment
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/bicyclegan6.PNG?raw=1' width = '800' ></p>
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/bicyclegan7.PNG?raw=1' width = '800' ></p>

**pix2pix+noise**는 현실적이지만 변화가 거의 없고, **cVAE-GAN**은 다양성이 있는 이미지를 만들지만, space가 조밀하지 않아서 artifact가 생길 수 있다. **cLR-GAN**은 다양하지는 않지만 현실적인 이미지를 생성한다.(mode collapse가 생길 수 있음)

**bicycleGAN**은 다양하고 현실적인 이미지를 만든다.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/bicyclegan8.PNG?raw=1' width = '800' ></p>

## 6. Conclusion

> latent vector와 output space를 bijective mapping하는 objective function을 사용함으로써 다양하고 현실적인 이미지를 생성할 수 있게 되었다.

