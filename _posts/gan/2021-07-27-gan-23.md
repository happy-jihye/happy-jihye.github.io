---
title: "GAN Inversion / Encoder : Image2stylegan, IDInvert, pSp, e4e"
excerpt: ""


categories:
 - GAN
tags:
  - deeplearning
  - ai
  - GAN
  - vision
search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 

use_math: true
---

- [GAN-Zoos! (GAN í¬ìŠ¤íŒ… ëª¨ìŒì§‘)](https://happy-jihye.github.io/gan/)

---

## StyleGAN

ğŸ˜ StyleGAN Posting
- [`[Paper Review] StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks ë…¼ë¬¸ ë¶„ì„`](https://happy-jihye.github.io/gan/gan-6/)
- [`[Paper Review] StyleGAN2 : Analyzing and Improving the Image Quality of StyleGAN ë…¼ë¬¸ ë¶„ì„`](https://happy-jihye.github.io/gan/gan-7/)
- `StyleGAN2-ADA: Training Generative Adversarial Networks with Limited Data` [`#01`](https://happy-jihye.github.io/gan/gan-19/) [`#02`](https://happy-jihye.github.io/gan/gan-20/)

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan-2.PNG?raw=1" width = "600" ></p>

> **StyleGAN**ì€ standard Gaussian latent space $Z$ê°€ ì•„ë‹ˆë¼ learnt intermediate latent space $W$ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìƒì„±. $W$ëŠ” $Z$ì— ë¹„í•´ disentanglementí•˜ê¸° ë•Œë¬¸ì— ì´ë¥¼ í™œìš©í•˜ë©´ ì´ë¯¸ì§€ ì¡°ì‘ì´ ì‰¬ì›€.
> 
> ìµœê·¼ì—ëŠ” StyleGANì˜ ì´ëŸ¬í•œ ì„±ì§ˆì„ ì´ìš©í•˜ê¸° ìœ„í•´ ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ **$W$ë¡œ inversion**í•œ í›„, **latent manipulationì„ í†µí•´ ì´ë¯¸ì§€ë¥¼ ì¡°ì‘**í•˜ë ¤ëŠ” ì‹œë„ê°€ ë§ì•„ì§ (Image Editing)

1. GAN Inversion 
2. Latent space manipulation

---

## GAN Inversion

> **Inversion**: Image â†’ style code $w \in W$
> â­ï¸ (1) reconstruction (2) editability (latent manipulation â†’ meaningful image editing)

### Method

1. **Latent Optimization**
   - [Image2stylegan: How to embed images into the stylegan latent space?](https://arxiv.org/abs/1904.03189)
   - Image2stylegan++: How to edit the embedded images?
2. **Encoder**
   - ALAE: Adversarial latent autoencoders ([github](https://github.com/podgorskiy/ALAE))
   - pSp: Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation
3. **Hybrid approach**
   - [stylegan-encoder](https://github.com/pbaylies/stylegan-encoder)
   - IdInvert : In-Domain GAN Inversion for Real Image Editing


---

## 1. Image2StyleGAN

- Paper : [Image2stylegan: How to embed images into the stylegan latent space?](https://arxiv.org/abs/1904.03189) (ICCV 2019 / Rameen Abdal, Yipeng Qin, Peter Wonka)

> **Image2StyleGAN** : extended latent space $W+$ë¥¼ ì œì•ˆ
> - conventional StyleGAN: `512-dim` $z \in Z=\mathcal{N}\left(\mu, \sigma^{2}\right)$ â†’ `8 MLP` â†’ `512-dim` $w \in \mathcal{W} \subsetneq \mathbb{R}^{512}$
> 
> - ê¸°ì¡´ì˜ StyleGANì€ í•˜ë‚˜ì˜ style vector $w$ë¥¼ ì—¬ëŸ¬ AdaIN blockì˜ inputìœ¼ë¡œ ì‚¬ìš©í•˜ì˜€ë‹¤ë©´, Image2StyleGANì—ì„œëŠ” extended latent space $W+$ë¥¼ ì œì•ˆí•˜ì—¬ **`18 different 512-dim w vector`** ë¥¼ ì‚¬ìš© : $w \in \mathcal{W}^k \subsetneq \mathbb{R}^{k \times 512}$
> - $\mathcal{W+}$ì—ì„œ (1) early layersëŠ” layoutì„ control, (2) middle layersëŠ” objectë¥¼ control, (3) late layersì€ final renderingì„ controlí•¨.


> **Latent Optimization** : select a random initial latent code â†’ optimize it using gradient descent 


## 2. IDInvert

- Paper : [In-Domain GAN Inversion for Real Image Editing](https://arxiv.org/abs/2004.00049) (ECCV 2020 / Jiapeng Zhu, Yujun Shen, Deli Zhao, Bolei Zhou)

- [Project Page](https://genforce.github.io/idinvert/)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/idinvert.png?raw=1' width = '700' ></p>

- ê¸°ì¡´ ëª¨ë¸ì˜ ë¬¸ì œì  : ê¸°ì¡´ì˜ Inversion modelë“¤ì€ ì´ë¯¸ì§€ë¥¼ ë‹¨ìˆœíˆ reconstructioní•˜ëŠ” ê²ƒì´ ëª©ì ì´ì–´ì„œ inverted codeê°€ original latent spaceì˜ semantic domain ë‚´ì— ìˆì§€ ì•Šì•˜ìŒ.

> **In-domain GAN inversion** : imageì˜ reconstructionì€ ë‹¹ì—°íˆ ì˜ë˜ë©° inverted codeê°€ riginal latent spaceì˜ semantic domain ë‚´ì— ìˆê¸° ë•Œë¬¸ì— ì´ë¥¼ ì¡°ì‘í•¨ìœ¼ë¡œì¨ image editingì´ ê°€ëŠ¥í•´ì§ 
>
> **(1) *domain-guided* encoder** : imageë¥¼ in-domain latent spaceë¡œ inversion
> **(2) *domain-regularized* optimization** : inverted codeë¥¼ optimizeí•´ì„œ target imageë¡œì˜ reconì´ ë” ì˜ë˜ë„ë¡ í•¨

> real image â†’ Encoder â†’ extended latent space $W+$ (`18ê°œì˜ 512-dim w vector`â†’ image optimization


## 3. pSp : pixel2Style2pixel

- Paper : [Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation](https://arxiv.org/abs/2008.00951) (CVPR 2021 /Jiapeng Zhu, Yujun Shen, Deli Zhao, Bolei Zhou)
- [Github](https://github.com/eladrich/pixel2style2pixel)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/pSp.jpeg?raw=1' width = '700' ></p>

> **(1) StyleGAN encoder**
> - real imageë¥¼ $W+$ latent domainìœ¼ë¡œ encoding. (ì¶”ê°€ì ìœ¼ë¡œ optimization ê³¼ì •ì´ í•„ìš” ì—†ìŒ) 
> - EncoderëŠ” Feature Pyramid Network êµ¬ì¡°ë¥¼ ë”°ë¦„.
>
> **(2) Image-to-Image translation** 
> - ê¸°ì¡´ì˜ sota modelë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ëƒ„. 
> - simplification of the training process : pretrained StyleGAN Generatorë¥¼ ì´ìš©í•˜ë¯€ë¡œ adversarial í•˜ê²Œ í•™ìŠµí•  í•„ìš” ì—†ìŒ(í•™ìŠµê³¼ì •ì—ì„œ `D` í•„ìš” X)
> - ë‹¤ì–‘í•œ multi-modal I2I translation ê°€ëŠ¥

## 4. e4e : Encoder for Editing
- Paper : [Designing an Encoder for StyleGAN Image Manipulation](https://arxiv.org/abs/2102.02766) (arxiv 2021 /Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, Daniel Cohen-Or)
- [Github](https://github.com/omertov/encoder4editing)

- **GAN Inversion** : reconstruction, editability â­ï¸
- **Reconstruction** : 2ê°€ì§€ë¡œ í‰ê°€ë˜ì–´ì•¼í•¨
  1. distortion : $\mathbb{E}_{x \sim p_{X}}[\Delta(x, G(w)]$
  2. perceptual quality : $\Delta(x, G(w)$
- **Editablity**
  - latent spaceì˜ disentanglement
  - image editing í›„ì—ë„ perceptual qualityê°€ ë†’ê²Œ ìœ ì§€ë˜ëŠ” ê²ƒì´ ì¤‘ìš”

| notation | latent space |
| --- | --- |
| <img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/e4e-2.png?raw=1' width = '700' > | <img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/e4e-1.png?raw=1' width = '700' > |

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/e4e-3.png?raw=1' width = '700' ></p> 

> E4E : ecoder-based method â†’ (1) inference time ë¹ ë¦„ (2) encoderê°€ CNNìœ¼ë¡œ ë˜ì–´ìˆê¸° ë•Œë¬¸ì— image editingì— ì¢‹ìŒ
>
> **approach for getting closer to $\mathcal{W}$**
> 1. **Minimize Variation** 
>   - encoderëŠ” single $\mathcal{W_*}$ì„ ì¶”ë¡ í•˜ë„ë¡ í›ˆë ¨. ì´í›„ networkì—ì„œ $\Delta_i$ë¥¼ í•™ìŠµí•˜ì—¬ $\mathcal{W_*}$ì—ì„œ $\mathcal{W_*^k}$ë¡œ í™•ì¥
>   - $L_2$ delta-regularization loss
> 2. **Minimize Deviation From $\mathcal{W^k}$** : encoderëŠ” $\mathcal{W^k}$ê³¼ closeí•œ $\mathcal{W_*^k}$ì„ ì¶”ë¡ í•˜ë„ë¡ í›ˆë ¨.