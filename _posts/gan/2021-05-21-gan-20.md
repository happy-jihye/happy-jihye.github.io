---
title: "[Paper Review] StyleGAN2-ADA #02: Training Generative Adversarial Networks with Limited Data 코드 리뷰"
excerpt: " "


categories:
 - GAN
tags:
  - deeplearning
  - ai
  - GAN
  - vision
search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---


<p align='right'> 
 <a href='https://github.com/NVlabs/stylegan2-ada-pytorch' role='button' target='_blank'> <img class='notebook-badge-image' src='/assets/badges/github.svg' alt='View On GitHub'> </a> 
</p>


> ✍🏻 최근에는 이미 있는 모델(`pretrained model`)을 잘 `fine tuning`하여 의미있는 결과를 내는 연구가 대세이다. (FreezeD, GANSpace, StyleCLIP 등등) 
> 
> **StyleGAN2-ADA**도 이러한 흐름에서 나온 연구로, loss function이나 network의 architecture를 건들이지 않고 이미 학습이 된 GAN을 `finetuning`하거나 training과정에서 `scratch`를 내는 식으로 학습을 한다. 또한, 적은 데이터로 학습을 해도 discriminator가 overfitting 되지 않도록  `Adaptive Discriminator Augmentation Mechanism`을 제안하였다.
> 
> ⭐ 이번 포스팅에서는 StyleGAN2-ADA의 [Official Code](https://github.com/NVlabs/stylegan2-ada-pytorch)를 살펴본다.

- Paper : [Training Generative Adversarial Networks with Limited Data](https://arxiv.org/abs/2006.06676) (NeurlPS 2020 /Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, Timo Aila)

- 😎 StyleGAN Posting
    - [`[Paper Review] StyleGAN2-ADA #01: Training Generative Adversarial Networks with Limited Data 논문 분석`](https://happy-jihye.github.io/gan/gan-19/)
    - [`[Paper Review] StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks 논문 분석`](https://happy-jihye.github.io/gan/gan-6/)
    - [`[Paper Review] StyleGAN2 : Analyzing and Improving the Image Quality of StyleGAN 논문 분석`](https://happy-jihye.github.io/gan/gan-7/)

- [GAN-Zoos! (GAN 포스팅 모음집)](https://happy-jihye.github.io/gan/)

---



## Generation Images

### generator.py

- <span style='background-color: #E5EBF7;'> Generate curated MetFaces images without truncation (Fig.10 left) </span>
  
    ```bash
    $ python generate.py --outdir=out --trunc=1 --seeds=85,265,297,849 \
        --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl
    ```

    truncation 없이 이미지를 생성하였다. truncation metric은 immediate latent space $W$에서 중요한 부분만을 catch하는 metric이다. 이 metric을 사용하면 다양성은 떨어지지만, quality가 높은 이미지를 생성할 수 있다.

    | seed 85  | seed 297  |  seed 849  |
    | ---- |  ---- | ---- |
    | <img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan2-ada-ge1.png?raw=1' width = '700' >     |  <img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan2-ada-ge3.png?raw=1' width = '700' >     | <img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan2-ada-ge4.png?raw=1' width = '700' >     |



- <span style='background-color: #E5EBF7;'> Generate uncurated MetFaces images with truncation (Fig.12 upper left) </span>
    
    ```bash
    $ python generate.py --outdir=out --trunc=0.7 --seeds=600-605 \
        --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl
    ```

- <span style='background-color: #E5EBF7;'> Generate class conditional CIFAR-10 images (Fig.17 left, Car) </span>

  - stylegan2-ada에서는 conditional generation도 가능하다. (stylegan2는 x)
  - class label을 또다른 mapping network에 넣어 embedding 한 후에 그 embedding을 $w$와 concate하는 방식으로 진행된다.

  ```bash
  $ python generate.py --outdir=out --seeds=0-35 --class=1 \
      --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/cifar10.pkl
  ```

---


- `Generator.py` Code
  - pickle file은 세가지의 network, `G`, `D`, `G_ema`를 포함하고 있다. 
    - `G_ema`는 Generator weight의 평균을 exponential moving한 것으로, EMA(Exponential Moving Average)의 방식을 활용하면 GAN을 안정적으로 학습할 수 있다.

<script src="https://gist.github.com/happy-jihye/b9849c7432375bb2595b436fa375c760.js"></script>

### style_mixing.py

- <span style='background-color: #E5EBF7;'> Style mixing example </span>

    ```bash
    $ python style_mixing.py --outdir=out --rows=85,100,75,458,1500 --cols=55,821,1789,293 \
        --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl
    ```

- Result

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan2-ada-style_mixing.png?raw=1' width = '700' ></p>

- `style_mixing.py` Code


<script src="https://gist.github.com/happy-jihye/0d6da98c7e79aadbe5dc681109903879.js"></script>


---
## Projecting images to latent space

### Projector.py 

`projector.py` 함수를 사용하여 원하는 이미지의 latent vector를 구할 수 있다.

```bash
$ python projector.py --outdir=out --target=~/mytargetimg.png \
    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl
```

- `projector.py` Code

<script src="https://gist.github.com/happy-jihye/1b62d46fededfd0cf8df305f3093faa2.js"></script>

위에서 생성한 latent vector를 바탕으로 이미지를 생성할 수도 있다.

```bash
$ python generate.py --outdir=out --projected-w=out/projected_w.npz \
    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl
```

|                         target image                         |                        Projection image                        |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan2-ada-target.png?raw=1' width = '700' > | <img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan2-ada-proj.png?raw=1' width = '700' > |

## Training new networks

```bash
# Train with custom dataset using 1 GPU.
# dry-run : 중간에 error가 없는지 확인하기 위한 용도
$ python train.py --outdir=~/training-runs --data=~/mydataset.zip --gpus=1 --dry-run
$ python train.py --outdir=~/training-runs --data=~/mydataset.zip --gpus=1

# Train class-conditional CIFAR-10 using 2 GPUs.
$ python train.py --outdir=~/training-runs --data=~/datasets/cifar10.zip \\
    --gpus=2 --cfg=cifar --cond=1

# Reproduce original StyleGAN2 config F.
$ python train.py --outdir=~/training-runs --data=~/datasets/ffhq.zip \\
    --gpus=8 --cfg=stylegan2 --mirror=1 --aug=noaug

# Transfer learn MetFaces from FFHQ using 4 GPUs.
$ python train.py --outdir=~/training-runs --data=~/datasets/metfaces.zip \\
    --gpus=4 --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10
```

```
Base configs (--cfg):
    auto       Automatically select reasonable defaults based on resolution
                and GPU count. Good starting point for new datasets.
    stylegan2  Reproduce results for StyleGAN2 config F at 1024x1024.
    paper256   Reproduce results for FFHQ and LSUN Cat at 256x256.
    paper512   Reproduce results for BreCaHAD and AFHQ at 512x512.
    paper1024  Reproduce results for MetFaces at 1024x1024.
    cifar      Reproduce results for CIFAR-10 at 32x32.


Transfer learning source networks (--resume):
    ffhq256        FFHQ trained at 256x256 resolution.
    ffhq512        FFHQ trained at 512x512 resolution.
    ffhq1024       FFHQ trained at 1024x1024 resolution.
    celebahq256    CelebA-HQ trained at 256x256 resolution.
    lsundog256     LSUN Dog trained at 256x256 resolution.
    <PATH or URL>  Custom network pickle.
```

### train.py

- `train.py`의 `main` function

<script src="https://gist.github.com/happy-jihye/8725dbd531d3812d58963150a9fbfd58.js"></script>

- `setup_training_loop_kwargs` : `train.py`에서 여러 parameter들을 정의하는 파트

<script src="https://gist.github.com/happy-jihye/51e1d6069b5f35890599d766e006df03.js"></script>

main 함수에서는 `subprocess_fn`함수를 호출한다. `subprocess_fn` 함수에서 gpu의 개수에 맞게 세팅을 조정한 후, `training_loop`함수를 호출하여 학습을 본격적으로 시작 !

<script src="https://gist.github.com/happy-jihye/fdff76e0c080b52de3a002ec4d3a2eb9.js"></script>

### training_loop.py

<script src="https://gist.github.com/happy-jihye/d2e5db5b821143bb1ddb870be8102d1e.js"></script>

### network.py
<script src="https://gist.github.com/happy-jihye/1105130adae43ad198848a4342349c0f.js"></script>