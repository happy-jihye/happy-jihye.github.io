---
title: "[Paper Review] GauGAN : Semantic Image Synthesis with Spatially-Adaptive Normalization (SPADE) 논문 분석"
excerpt: "Semantic Image를 현실적인 image로 변환해주는 Spatially-adaptive normlization(SPADE) model에 대해 알아본다"

categories:
 - GAN
tags:
  - deeplearning
  - ai
  - pytorch
  - GAN
  - vision
search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---

> ✍🏻 이번 포스팅에서는 Semantic Image를 현실적인 image로 변환해주는 **Spatially-adaptive normlization(SPADE) model**에 대해 살펴본다.


- Paper : [Semantic Image Synthesis with Spatially-Adaptive Normalization](https://arxiv.org/abs/1903.07291) (CVPR 2019 / Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu)

- [GAN-Zoos! (GAN 포스팅 모음집)](https://happy-jihye.github.io/gan/)

---

|                                                              |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| <img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade3.gif?raw=1'>| <img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade2.gif?raw=1' >|




## 1. Introduction
> We propose **spatially-adaptive normalization**, **a conditional normalization layer** that modulates the activations using input semantic layouts through a spatially adaptive, learned transformation and can effectively propagate the semantic information throughout the network.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade1.PNG?raw=1' width = '800' ></p>


이전까지는 Semantic Image to photorealistic image task에서 **convolution, normalization, nonlinearity layer로 구성된 deep nework**를 사용했다. 이러한 방식을 사용하면 normalization과정에서 semantic 정보가 사라진다는 문제가 있었다.

본 논문에서는 이 문제를 해결하기 위해 <span style='background-color: #E5EBF7;'> **spatially-adaptive normalization** </span>을 제안한다. 

## 2. Related Work

### 2.1 Conditional image synthesis

이미지를 생성할 때 conditional information을 주면 더 빠르고 현실적인 이미지가 생성된다.

<span style='background-color: #FFF2CC;'> Conditional image synthesis task들과 관련 논문 </span>
- **Class-conditional models**
  - [[Paper Review] CGAN : Conditional Generative Adversarial Nets 논문 분석](https://happy-jihye.github.io/gan/gan-3/)
  
- **Generating images based on text**
  - [[Paper Review] Generative Adversarial Text to Image Synthesis 논문 분석](https://happy-jihye.github.io/gan/gan-4/)

- **Image to Image translation**
  - [[Paper Review] Pix2pix : Image-to-Image Translation with Conditional Adversarial Networks 논문 분석](https://happy-jihye.github.io/gan/gan-8/)
  - <span style='background-color: #E5EBF7;'> **Segmentation masks to images : SPADE !** </span>

    *Segmentation masks to images*는 specific한 *Image to Image translation*으로, input으로는 semantic label map을 받고 output으로는 photo를 출력한다.

---

### 2.2 Unconditional normalization layers


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan-3.PNG?raw=1' width = '600' ></p>

위의 unconditional normalization layer들은 conditional normalization layer과 다르게 외부 데이터에 의존하지 않는다. 따라서 이를 사용하려면 labeling을 해줘야한다.

### 2.3 Conditional normalization layer

Conditional normalization layer는 외부 데이터에 의존한다.(style transfer)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade4.png?raw=1' width = '700' ></p>

보통 **Conditional normalization layer**은 Figure3 과 같이 외부 데이터에서 얻어진 정보로 normalize가 되곤 한다. 이는 아래와 같은 방식으로 진행된다.

1. layer activation을 zero mean, unit deviation으로 정규화(normalization)을 한 후,
2. 외부 데이터로부터 추론된 `learned affine transformation`를 이용하여 (1에서) 정규화된 activation을 denormalize

보통 style-transfer 와 같은 task는 이미지의 전반적인 style을 바꿔야하기 때문에 affine parameter가 output image의 global한 style을 조절하고, 공간좌표에 따라 affine transformation의 parameter가 일정(uniform)하다.

> 반면, 본 논문에서는 semantic mask들마다 각기 다른 style의 이미지를 생성하기 원한다. 따라서  
> 본 논문에서는 **normalization layer에 spatially-varing affine transformation을 적용하여 semantic mask로부터 이미지를 합성**한다.

## 3. Semantic Image Synthesis

- 각 pixel의 semantic segmentation mask : $\mathbf{m} \in \mathbb{L}^{H \times W}$
- ⭐ Goal: input segmentation mask $m$ 을 photorealistic image로 바꿔주는 mapping network 학습
  
### 3.1 SPADE: SPatially-Adaptive DEnormalization

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade5.PNG?raw=1' width = '400' ></p>

1. mask를 embedding space로 projection한 후, 
2. 이를 convolution 해서 modulation parameters $\gamma, \beta$ 를 만듦
  - 이때 modulation parmaeter $\gamma, \beta$는 vector가 아니라 tensor  
  - $\gamma_{c, y, x}^{i}(\mathbf{m}), \beta_{c, y, x}^{i}(\mathbf{m})$ 는 normalization layer의 modulation parameter
      - 이 variable은 학습이 되며, Batch-Norm과 다르게 input segmentation mask와 location $(y, x)$ 에 영향을 받는다.
3. $\gamma, \beta$ 는 normalized activation에 각각 multiplied & added

$$\gamma_{c, y, x}^{i}(\mathbf{m}) \frac{h_{n, c, y, x}^{i}-\mu_{c}^{i}}{\sigma_{c}^{i}}+\beta_{c, y, x}^{i}(\mathbf{m})$$

- normalization으로는 instance norm이 아니라 Batch Norm을 사용했다.
  
$$\begin{aligned}
\mu_{c}^{i} &=\frac{1}{N H^{i} W^{i}} \sum_{n, y, x} h_{n, c, y, x}^{i} \\
\sigma_{c}^{i} &=\sqrt{\frac{1}{N H^{i} W^{i}} \sum_{n, y, x}\left(\left(h_{n, c, y, x}^{i}\right)^{2}-\left(\mu_{c}^{i}\right)^{2}\right)}
\end{aligned}$$

🤔 사실 SPADE(SPatially-Adaptive DEnormalization)는 존재하는 normalization layer들의 일반화된 표현이다.

만약 (1) segmentation mask $m$ 을 image class label로 바꾸고 (2) modulation param $\gamma, \beta$ 를 공간좌표에 따라 변하지 않도록(spatially invariant)하게 바꾼다면, 이는 Conditional BatchNorm과 유사해질 것 이다.

- 본 논문은 modulation param $\gamma, \beta$ 을 input seg mask에 따라 adaptive하도록 했기 때문에 semantic image synthesis를 더 잘하는 것 !!

  
---

### 3.2 SPADE generator


- SPADE에서는 learned modulation parameter를 사용하기 때문에 generator의 첫번째 layer로 segmentation map을 사용할 필요가 없다. 또한, normalization 과정에서 semantic mask 정보를 입력해주기 때문에 Encoder 부분(downsampling)도 필요 없다. (이미 이 modulation param이 label layout에 대한 정보들을 충분히 잘 encoding하니까) 
  - 따라서 본 논문은 encoder를 제거함으로써 모델을 경량화했다. 

- 또한, generator의 input으로는 random noise vector를 사용하여 모델을 단순화하고, multi-modal synthesis가 가능하도록 했다.
  - input으로 random noise를 사용하기 때문에 latent vector를 조절하면서 style을 변화하는 것도 가능해짐

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade6.PNG?raw=1' width = '800' ></p>

- Upsampling layer로는 ResNet Block을 사용한다.
  - 모든 normalization layer의 modulation param는 SPADE로 학습
  - 각 residual block은 각기 다른 scale에서 작동이 되기 때문에 저자들은 semantic mask를 각 spatial resolution과 match되도록 downsampling 했다.

- 저자들은 pix2pixHD에서 사용한 multi-scale discriminator와 loss function을 그대로 사용했다고 한다.
  - 이때, least squared loss를 hinge loss로 대체

### 3.3 Why dose the SPADE work better?

> A) 일반적인 normalization layers보다 semantic한 정보를 더 잘 보존하기 때문에!

Instance Norm과 같은 normalization layer는 많은 conditional image 합성에서 SoTA를 달성했지만, uniform/flat segmentation mask에 적용하면 semantic information이 사라진다는 단점이 있다.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade7.PNG?raw=1' width = '450' ></p>

segmentation mask에서는 대부분의 pixel이 같은 label을 갖고 있다. 따라서 Instance Norm을 사용하면 0으로 normalized되어 semantic information이 사라질 것이다. (`Fig3`의 pix2pixHD 결과 참고)

> 반면, SPADE는 다른 normalization layer에 비해 **semantic information**이 잘 보존된다.

--- 

### 3.4 Multi-model synthesis

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade8.PNG?raw=1' width = '450' ></p>

> Generator의 input으로 random vector를 사용함으로써 multi-modal synthesis가 가능해졌다.

- 다음과 같이 VAE 처럼 Generator를 구성할 수도 있다. (Encoder와 generator쪽의 architecture가 VAE와 비슷)
- 이 경우에는 **`Encoder`에서는 이미지의 style을 capture하고, `Generator`에서는 SPADE에서의 segmentation mask information과 encoded style을 합친다.**
- KL-Divergence loss로 학습

### 3.5 Block and network architectures

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade14.PNG?raw=1' width = '800' ></p>
---
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade15.PNG?raw=1' width = '800' ></p>

- SPADE Generator with VAEs는 VAE처럼 학습이 된다. latent vector를 gaussian에서 뽑는게 아니라 reference 이미지로부터 inference한 다음에 generator에 넣어준다.

## 4. Experiments

**Implementation details.**

We apply the **Spectral Norm** [38] to all the layers in both generator and discriminator. The **learning rates** for the generator and discriminator are 0.0001 and 0.0004, respectively [17]. We use the **ADAM** solver [27] with β1 = 0 and β2 = 0.999. All the experiments are conducted on an NVIDIA DGX1 with 8 32GB V100 GPUs. We use synchronized BatchNorm, i.e., these statistics are collected from all the GPUs.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade9.PNG?raw=1' width = '800' ></p>
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade10.PNG?raw=1' width = '800' ></p>

COCO-stuff, ADE20K, Cityscapes dataset으로 학습을 시켰을 때, 다른 모델들과 비교해봐도 SPADE가 좋다.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade11.PNG?raw=1' width = '600' ></p>

Amazon Mechanical Turk(AMT, 사람이 직접 test)로 test를 해봐도 SPADE가 좋다.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade12.PNG?raw=1' width = '600' ></p>

pix2pix와 같은 다른 architecture에 SPADE를 사용해봐도 SPADE를 쓴게 좋다.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade13.PNG?raw=1' width = '800' ></p>

random noise를 바꿔보니 이미지 style이 변했다. multi-model synthesis가 잘된다. 

## 5. Conclusions

> ⭐ 본 논문에서는 **Sp**atially-**A**daptive **(DE)**normalizatoin을 제안했다. 이를 통해 normalization layer에서 affine 변환을 할 때 semantic 정보를 잘 활용하도록 할 수 있었고, semantic image로부터 현실적인 이미지 합성이 가능해졌다.
> 
> 또한, multi-model synthesis 뿐만 아니라 guided image synthesis가 가능해졌다.


---
**Reference**
- Naver AI LAB 최윤제 연구원님 발표자료
- [https://paperswithcode.com/method/conditional-instance-normalization#](https://paperswithcode.com/method/conditional-instance-normalization#) 