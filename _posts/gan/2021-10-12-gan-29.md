---
title: "About Talking Head Models: Definition, Methods, SoTA Models..."
excerpt: ""


categories:
 - GAN
tags:
  - deeplearning
  - ai
  - GAN
  - vision
search: true

# 목차
#toc: true  
#toc_sticky: true 

use_math: true
---



## Neural Talking Head Model 이란?

Talking Head Model는 적은 수의 이미지를 가지고 특정 video의 표정이나 움직임을 따라하게 해주는 모델이다.

이때, reference가 되는 비디오를 `driving video`라고 부르며, target video(talking head model로 만들고 싶은 결과 비디오)의 identity를 가지고 있는 이미지를 `source image`라고 부른다.


Talking Head Model은 크게 세가지의 방식을 따른다.
1. **Warping based model**
   - pixel을 움직여 이미지를 조작하는 방식
   - Warping based의 talking head model은 적은 수의 이미지(few-shot images)로 talking head video를 생성할 수 있으나, 다양한 동작이나 움직임 등을 합성하긴 어려움

2. **Direct synthesis (warping-free)**
    - Deep Conv Network를 adversarial하게 학습하는 모델
    - large dataset으로 large network를 훈련시켜야하기 때문에 학습이 오래 걸리며 많은 GPU가 필요

3. **Meta Learning**
   - large talking-head datasets(ex. `VoxCeleb`)으로 deep ConvNet을 학습 시킨 후, few-shot의 이미지를 가지고 meta-learning을 하여 원하는 새로운 사람(target person)에 대해 talking head video를 만드는 방식
   - [Meta Learning 이란?](https://honeyjamtech.tistory.com/57)
     - Meta-Learning은 "learn to learn"이라고도 불리는데, 한마디로 학습하는 법 자체를 배운다는 뜻이다. meta-learning에서는 모델이 새로운 task/data를 빠르고 잘 학습하는 것을 목표로 한다.
     - meta-learning은 meta-training과 meta-testing 단계로 학습된다. meta-training에서 large data에 대해 학습을 한 후, meta-testing에서 새로운 data나 task에 잘 적용이 되도록 짧게 학습을 한다.
     - meta-testing에서 좋은 성능을 얻으려면 meta-training 과정에서 다음의 조건을 만족하도록 학습을 해야한다.
       1. meta-testing에서 조금만 update를 해도 되도록 좋은 parameter를 찾는 것
       2. 새로운 data와 task에 대해서도 학습이 잘 되도록 generalization을 잘 해놓는 것

---

Talking Head Talk는 사람의 표정뿐만 아니라 주위의 배경, head rotation 등을 반영한 자연스러운 비디오를 생성해야하기 때문에 매우 어렵다. Talking Head model은 target이 되는 사람의 움직임을 조절하기 위해 audio, landmark등을 이용한다.

1. **Graphics-based talking head generation**
   - subject-dependent하게 video를 editing하는 방식. input으로 특정 사람에 대한 full-original video가 필요하다.
   - `Synthesizing Obama`: Learning Lip Sync from Audio (SIGGRAPH 2017) : [Paper](https://grail.cs.washington.edu/projects/AudioToObama/siggraph17_obama.pdf)
     - audio signal을 input으로 받아서 lip region을 합성. target person에 대한 large video corpus가 필요
   - `TETH`: Text-based Editing of Talking-head Video (SIGGRAPH 2019) : [Paper](https://arxiv.org/abs/1906.01524)

2. **Audio-driven face generation (fixed head pose)**
   - head가 고정된 상태에서 identity-independent하게 표정을 변화시키는 face generation 방식

3. **Landmark-driven talking head generation**
   - facial landmark로 target person의 facial expression과 head rotation을 조절하는 방식
   - [`FSTH`: Few-Shot Adversarial Learning of Realistic Neural Talking Head Models](https://happy-jihye.github.io/gan/gan-22/) (arxiv 2019) : [Paper](https://arxiv.org/abs/1905.08233)
   - `LPD`: Neural Head Reenactment with Latent Pose Descriptors (IEEE 2020) : [Paper](https://arxiv.org/abs/2004.12000) 


---

## Various Talking Head Model

> 본 포스팅에서는 Talking Head Model들 중에서 유명한 몇몇 모델들에 대해 살펴볼 예정이다. 
> 
> Talking Head model Paper 모음집은 [이 링크](https://www.notion.so/3508fe269c914f21b37098bd7a230b26?v=b9bbf4a42ade4803a8ffb209b4f05957)에 

- [`FSTH`: Few-Shot Adversarial Learning of Realistic Neural Talking Head Models](https://happy-jihye.github.io/gan/gan-22/) (ICCV 2019) : [Paper](https://arxiv.org/abs/1905.08233)
- `LPD`: Neural Head Reenactment with Latent Pose Descriptors (CVPR 2020) : [Paper](https://arxiv.org/abs/2004.12000) 
- `X2Face`: A network for controlling face generation by using images, audio, and pose codes (ECCV 2018) : [Paper](https://arxiv.org/abs/1807.10550), [project](https://www.robots.ox.ac.uk/~vgg/research/unsup_learn_watch_faces/x2face.html)
- `Monkey-Net`: Animating Arbitrary Objects via Deep Motion Transfer (CVPR 2019) : [Paper](https://arxiv.org/abs/1812.08861), [project](http://www.stulyakov.com/papers/monkey-net.html), [code](https://github.com/AliaksandrSiarohin/monkey-net)
- `FOMM`: First Order Motion Model for Image Animation (NeurIPS 2019) : [arxiv](https://arxiv.org/abs/2003.00196), [code](https://github.com/AliaksandrSiarohin/first-order-model)
- Motion Representations for Articulated Animation (CVPR 2021) : [arxiv](https://arxiv.org/abs/2104.11280), [code](https://github.com/snap-research/articulated-animation), [project](https://snap-research.github.io/articulated-animation/)

- [`MocoGAN-HD`: A Good Image Generator Is What You Need for High-Resolution Video Synthesis](https://happy-jihye.github.io/gan/gan-27/) (ICLR 2021) : [arxiv](https://arxiv.org/abs/2104.15069), [code](https://github.com/snap-research/MoCoGAN-HD), [project](https://bluer555.github.io/MoCoGAN-HD/)


---
### X2Face: A network for controlling face generation by using images, audio, and pose codes (ECCV 2018)

> ⭐️ **Keyword**: Warping based model, self-supervised training


- `X2Face`: A network for controlling face generation by using images, audio, and pose codes (ECCV 2018) : [Paper](https://arxiv.org/abs/1807.10550), [project](https://www.robots.ox.ac.uk/~vgg/research/unsup_learn_watch_faces/x2face.html)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/x2face-1.jpeg?raw=1' width = '700' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> First training-stage: fully self-supervised </p></i></font>

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/x2face-2.png?raw=1' width = '700' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> Second training-stage </p></i></font>


- **Embedding network**: source frame으로부터 identity를 뽑아내는 network
  - U-Net 과 pix2pix 의 architecture를 따름
  - 이 네트워크는 source frame의 정면화된 얼굴을 추출하라고 강요하지는 않지만, pose/표정과 무관한 source frame만의 고유한 얼굴을 생성하려고 하다보니 embedded face가 정면을 바라보는 얼굴로 생성됨
- **Driving network**
  - encoder-decoder architecture
  - driving frame(input)으로 부터 driving vector를 embedding한 후, embedded face를 pixel단위로 transform하여 target image 생성
- **Training the network**: 총 2 stages로 training 함
  1. First training-stage: **fully self-supervised**
      - ex) 같은 비디오에서 4개의 프레임을 추출했을 때, 3개는 source image로 사용하고, 나머지 하나의 프레임은 driving frame으로 사용. source frame으로부터 얻은 embedded face와 driving frame에서 얻은 driving vector를 이용하여 이미지를 생성한 후, 생성된 이미지와 driving frame간에 `L1 loss`를 통해 network를 업데이트
      - 이렇게만 학습을 하면 생성된 이미지가 embedded face에서 identity에 대한 정보를 얻지 않고, driving vector에서도 identity에 대한 정보를 얻을 수 있음. second training-stage에서 identity loss function을 도입하여 identity bleeding이 생기지 않도록 함
  2. Second training-stage
      - Generated frame이 source frame의 identity를 따르도록 강제하는 identity loss function을 둠
      - $L_{identity}$는 사전에 훈련된 11-layer VGG network를 사용
      - 두가지의 loss term
        - $L_{identity}(d_A, g_{d_A})$: $g_{d_A}$와 $d_A$는 pose, 표정, identity 등 모든게 같음(사실상 recon loss). photometric *L1* loss와 *L1* content loss를 사용
        - $L_{identity}(s_A, g_{d_R})$: *L1* content loss만을 사용

- **Comparison**
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/x2face-3.png?raw=1' width = '700' ></p>

---
### Monkey-Net:Animation Arbitrary Objects via Deep Motion Transfer (CVPR 2021)

> ⭐️ **Keyword**: Adversarial Training, Model Agnostic Meta-Learning(MAML), Landmark

---
### FSTH: Few-Shot Adversarial Learning of Realistic Neural Talking Head Models (ICCV 2019)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/fsth-1.PNG?raw=1' width = '700' ></p>

> ⭐️ **Keyword**: Adversarial Training, Model Agnostic Meta-Learning(MAML), Landmark

- [`FSTH`: Few-Shot Adversarial Learning of Realistic Neural Talking Head Models](https://happy-jihye.github.io/gan/gan-22/) (ICCV 2019) : [Paper](https://arxiv.org/abs/1905.08233), [review](https://happy-jihye.github.io/gan/gan-22/)


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/FSTH-10.jpeg?raw=1' width = '700' ></p>

- **Adversarial Meta-Learning**: meta-learning 과정에 adversarial training을 도입하여 meta-learning stage에서 보지 못했던 새로운 이미지들을 생성할 수 있도록 함.
  - 합성된 이미지가 landmark를 잘 반영하고 있는지를 판별하는 `Discriminator`를 둠
- Meta-Learning Architecture는 **Embedder와 Generator**로 구성
  - `Embedder`
    - video frame과 그 landmark를 input로 받아 N차원의 Embedding vector $\hat{e_i}$ 로 mapping
    - $\hat{e_i}$ : 사람의 identity에 대한 정보를 담고 있음 (pose등에 무관)
  - `Generator`
    - landmark를 input으로 받은 후, AdaIN operation을 통해 embedding vector로 부터 target person에 대한 style을 입혀줌. 
- Few-shot의 이미지로 pre-trained model을 **fine-tuning**
  - target person의 image들을 meta-learned Embedder에 넣어 새로운 embedding vector $\hat{\mathbf{e}}_{\mathrm{NEW}}$를 계산
  - 새로 계산된 embedding vector와 landmark를 이용하여 Generator를 Fine-tuning
  - model에 대한 fine-tuning이 완료되면 원하는 표정에 해당하는 landmark를 이용하여 새로운 talking head video 생성

---

### LPD: Neural Head Reenactment with Latent Pose Descriptors (CVPR 2020)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/lpd-1.png?raw=1' width = '700' ></p>

> ⭐️ **Keyword**: Adversarial Training, Meta-Learning, Landmark, Segmentation(Masking)

- `LPD`: Neural Head Reenactment with Latent Pose Descriptors (CVPR 2020) : [Paper](https://arxiv.org/abs/2004.12000), [project](https://saic-violet.github.io/latent-pose-reenactment/), [code](https://github.com/shrubb/latent-pose-reenactment)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/lpd-3.jpeg?raw=1' width = '700' ></p>


- FSTH 보다 늦게 나온 논문. FSTH의 아이디어를 다수 차용
- **Adversarial Meta-Learning**: meta-learning 과정에 adversarial training을 도입하여 meta-learning stage에서 보지 못했던 새로운 이미지들을 생성할 수 있도록 함.
  - adversarial training을 identity와 pose가 disentaglement되도록 도와줌
- **Embedder**
  1. `Identity encoder`
     - ResNeXt-50 (32 x 4d)
     - high-capacity convolutional net $F$
     - FSTH model처럼 input으로 이미지와 landmark를 함께 받지 않고, input으로는 k개의 이미지만을 받음
  2. `Pose encoder`
     - MobileNetV2
     - input으로 random pose-augmentation transform 변환을 한 이미지 $A(I_{K+1})$ 를 사용 (이 transformation을 사용하면 pose와 identity가 더 disentanglement 되는 효과가 있음)
- **Generator**
  - `pose & identity embedding vector` concat 하여 MLP를 거치게 한 후, 이를 AdaIN operation을 통해 Generator에 넣어 $K+1$ 번째 이미지를 reconstruct
  - `Generator`는 FSTH의 Generator와 `StyleGAN v1`의 구조를 따름  - Learned constant tensor (`512 x 4 x 4`)에서 시작하여 여러개의 convolutional block으로 구성됨. AdaIN block은 각각의 Conv layer에 삽입
- **Loss Function**: Content Loss, Adversarial Loss, discriminator feature matching loss
- Few-shot의 이미지로 pre-trained model을 **Fine-Tuning**
  - 모델이 meta-learned 되면, meta-learning 과정에서 보지 못한 새로운 identity에 대해서도 talking-head video를 생성할 수 있도록 Fine-Tuning ! (FSTH의 방식을 차용)
  1. 새로운 사람의 이미지들을 identity encoder에 넣은 후, 각 vector들을 평균 내어 new identity vector $\bar{x}$ 를 얻음
  2. `meta-learned model`을 fine-tuning
      - 이 과정에서 identity vector $\bar{x}$와 pose embedding network는 fix 

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/lpd-2.png?raw=1' width = '700' ></p>


> - one-shot의 이미지로도 talking-head task가 잘되는 모델. 실험을 해보면 이미지가 몇장 있냐보다는 target person의 identity가 더 중요함 (ex. 10장의 엘사보다는 1장의 강동원이 더 잘 됨)
> - 데포로메가 심하지 않다면, cross-domain의 이미지에 대해서도 좋은 결과를 냄
> - public으로 공개되지 않은 VoxCeleb2 dataset을 사용 (저자들이 youtube 링크에서 영상들을 다운받은 후 직접 data-preprocessing을 한 것으로 추측됨. 이 데이터셋은 공개하지 않음)
> - LPD Align: 저자들이 직접 align 규격을 만들어 model을 학습. FFHQ Align와 달리 rotation을 하지 않으며, meta-learning 과정에서도 rotation을 안한 데이터셋을 사용해도 결과가 잘 나옴

