---
title: "About Talking Head Models #01: Definition, Methods, SoTA Models, Warping based model..."
excerpt: "Talking Head taskì˜ ì •ì˜ì™€, Warping based talking head model(X2Face, Monkey-Net, First-Order model(FOMM), Face Vid2Vid)ì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤."


categories:
 - GAN
tags:
  - deeplearning
  - ai
  - GAN
  - vision
search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 

use_math: true
---


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/talking-head-ex.gif?raw=1' width = '700' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> Talking Head Task Example [ì¶œì²˜](https://gfycat.com/ko/horribledampislandcanary-generative-adversarial-networks) </p></i></font>


## Neural Talking Head Model ì´ë€?

Talking Head ModelëŠ” ì ì€ ìˆ˜ì˜ ì´ë¯¸ì§€ë¥¼ ê°€ì§€ê³  íŠ¹ì • videoì˜ í‘œì •ì´ë‚˜ ì›€ì§ì„ì„ ë”°ë¼í•˜ê²Œ í•´ì£¼ëŠ” ëª¨ë¸ì´ë‹¤.

ì´ë•Œ, referenceê°€ ë˜ëŠ” ë¹„ë””ì˜¤ë¥¼ `driving video`ë¼ê³  ë¶€ë¥´ë©°, target video(talking head modelë¡œ ë§Œë“¤ê³  ì‹¶ì€ ê²°ê³¼ ë¹„ë””ì˜¤)ì˜ identityë¥¼ ê°€ì§€ê³  ìˆëŠ” ì´ë¯¸ì§€ë¥¼ `source image`ë¼ê³  ë¶€ë¥¸ë‹¤.


Talking Head Modelì€ í¬ê²Œ ì„¸ê°€ì§€ì˜ ë°©ì‹ì„ ë”°ë¥¸ë‹¤.
1. **Warping based model**
   - pixelì„ ì›€ì§ì—¬ ì´ë¯¸ì§€ë¥¼ ì¡°ì‘í•˜ëŠ” ë°©ì‹
   - Warping basedì˜ talking head modelì€ ì ì€ ìˆ˜ì˜ ì´ë¯¸ì§€(few-shot images)ë¡œ talking head videoë¥¼ ìƒì„±í•  ìˆ˜ ìˆìœ¼ë‚˜, ë‹¤ì–‘í•œ ë™ì‘ì´ë‚˜ ì›€ì§ì„ ë“±ì„ í•©ì„±í•˜ê¸´ ì–´ë ¤ì›€

2. **Direct synthesis (warping-free)**
    - Deep Conv Networkë¥¼ adversarialí•˜ê²Œ í•™ìŠµí•˜ëŠ” ëª¨ë¸
    - large datasetìœ¼ë¡œ large networkë¥¼ í›ˆë ¨ì‹œì¼œì•¼í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµì´ ì˜¤ë˜ ê±¸ë¦¬ë©° ë§ì€ GPUê°€ í•„ìš”

3. **Meta Learning**
   - large talking-head datasets(ex. `VoxCeleb`)ìœ¼ë¡œ deep ConvNetì„ í•™ìŠµ ì‹œí‚¨ í›„, few-shotì˜ ì´ë¯¸ì§€ë¥¼ ê°€ì§€ê³  meta-learningì„ í•˜ì—¬ ì›í•˜ëŠ” ìƒˆë¡œìš´ ì‚¬ëŒ(target person)ì— ëŒ€í•´ talking head videoë¥¼ ë§Œë“œëŠ” ë°©ì‹
   - [Meta Learning ì´ë€?](https://honeyjamtech.tistory.com/57)
     - Meta-Learningì€ "learn to learn"ì´ë¼ê³ ë„ ë¶ˆë¦¬ëŠ”ë°, í•œë§ˆë””ë¡œ í•™ìŠµí•˜ëŠ” ë²• ìì²´ë¥¼ ë°°ìš´ë‹¤ëŠ” ëœ»ì´ë‹¤. meta-learningì—ì„œëŠ” ëª¨ë¸ì´ ìƒˆë¡œìš´ task/dataë¥¼ ë¹ ë¥´ê³  ì˜ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.
     - meta-learningì€ meta-trainingê³¼ meta-testing ë‹¨ê³„ë¡œ í•™ìŠµëœë‹¤. meta-trainingì—ì„œ large dataì— ëŒ€í•´ í•™ìŠµì„ í•œ í›„, meta-testingì—ì„œ ìƒˆë¡œìš´ dataë‚˜ taskì— ì˜ ì ìš©ì´ ë˜ë„ë¡ ì§§ê²Œ í•™ìŠµì„ í•œë‹¤.
     - meta-testingì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ìœ¼ë ¤ë©´ meta-training ê³¼ì •ì—ì„œ ë‹¤ìŒì˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ë„ë¡ í•™ìŠµì„ í•´ì•¼í•œë‹¤.
       1. meta-testingì—ì„œ ì¡°ê¸ˆë§Œ updateë¥¼ í•´ë„ ë˜ë„ë¡ ì¢‹ì€ parameterë¥¼ ì°¾ëŠ” ê²ƒ
       2. ìƒˆë¡œìš´ dataì™€ taskì— ëŒ€í•´ì„œë„ í•™ìŠµì´ ì˜ ë˜ë„ë¡ generalizationì„ ì˜ í•´ë†“ëŠ” ê²ƒ

---

> Talking Head TalkëŠ” ì‚¬ëŒì˜ í‘œì •ë¿ë§Œ ì•„ë‹ˆë¼ ì£¼ìœ„ì˜ ë°°ê²½, head rotation ë“±ì„ ë°˜ì˜í•œ ìì—°ìŠ¤ëŸ¬ìš´ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ë§¤ìš° ì–´ë µë‹¤. Talking Head modelì€ targetì´ ë˜ëŠ” ì‚¬ëŒì˜ ì›€ì§ì„ì„ ì¡°ì ˆí•˜ê¸° ìœ„í•´ audio, landmarkë“±ì„ ì´ìš©í•œë‹¤.

1. **Graphics-based talking head generation**
   - subject-dependentí•˜ê²Œ videoë¥¼ editingí•˜ëŠ” ë°©ì‹. inputìœ¼ë¡œ íŠ¹ì • ì‚¬ëŒì— ëŒ€í•œ full-original videoê°€ í•„ìš”í•˜ë‹¤.
   - `Synthesizing Obama`: Learning Lip Sync from Audio (SIGGRAPH 2017) : [Paper](https://grail.cs.washington.edu/projects/AudioToObama/siggraph17_obama.pdf)
     - audio signalì„ inputìœ¼ë¡œ ë°›ì•„ì„œ lip regionì„ í•©ì„±. target personì— ëŒ€í•œ large video corpusê°€ í•„ìš”
   - `TETH`: Text-based Editing of Talking-head Video (SIGGRAPH 2019) : [Paper](https://arxiv.org/abs/1906.01524)

2. **Audio-driven face generation (fixed head pose)**
   - headê°€ ê³ ì •ëœ ìƒíƒœì—ì„œ identity-independentí•˜ê²Œ í‘œì •ì„ ë³€í™”ì‹œí‚¤ëŠ” face generation ë°©ì‹

3. **Landmark-driven talking head generation**
   - facial landmarkë¡œ target personì˜ facial expressionê³¼ head rotationì„ ì¡°ì ˆí•˜ëŠ” ë°©ì‹
   - [`FSTH`: Few-Shot Adversarial Learning of Realistic Neural Talking Head Models](https://happy-jihye.github.io/gan/gan-22/) (arxiv 2019) : [Paper](https://arxiv.org/abs/1905.08233)
   - `LPD`: Neural Head Reenactment with Latent Pose Descriptors (IEEE 2020) : [Paper](https://arxiv.org/abs/2004.12000) 


---

## Various Talking Head Model

> ë³¸ í¬ìŠ¤íŒ…ì—ì„œëŠ” Talking Head Modelë“¤ ì¤‘ì—ì„œ Warping based modelì— ëŒ€í•´ ì‚´í´ë³¼ ì˜ˆì •ì´ë‹¤. 
> 
> Talking Head model Paper ëª¨ìŒì§‘ì€ [ì´ ë§í¬](https://stream-chameleon-81c.notion.site/1fa3a78b6c54415784175c50eddfe3cc?v=ddc7ecf4fb0f484d8a98cb7da3fd1ea5)ì— ğŸ¤—


- `X2Face`: A network for controlling face generation by using images, audio, and pose codes (ECCV 2018) : [Paper](https://arxiv.org/abs/1807.10550), [project](https://www.robots.ox.ac.uk/~vgg/research/unsup_learn_watch_faces/x2face.html)
- `Monkey-Net`: Animating Arbitrary Objects via Deep Motion Transfer (CVPR 2019) : [Paper](https://arxiv.org/abs/1812.08861), [project](http://www.stulyakov.com/papers/monkey-net.html), [code](https://github.com/AliaksandrSiarohin/monkey-net)
- `FOMM`: First Order Motion Model for Image Animation (NeurIPS 2019) : [arxiv](https://arxiv.org/abs/2003.00196), [code](https://github.com/AliaksandrSiarohin/first-order-model)
- `face vid2vid`: One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing (CVPR 2021): [arxiv](https://arxiv.org/abs/2011.15126), [project](https://nvlabs.github.io/face-vid2vid/)
- Motion Representations for Articulated Animation (CVPR 2021) : [arxiv](https://arxiv.org/abs/2104.11280), [code](https://github.com/snap-research/articulated-animation), [project](https://snap-research.github.io/articulated-animation/)
- [`MocoGAN-HD`: A Good Image Generator Is What You Need for High-Resolution Video Synthesis](https://happy-jihye.github.io/gan/gan-27/) (ICLR 2021) : [arxiv](https://arxiv.org/abs/2104.15069), [code](https://github.com/snap-research/MoCoGAN-HD), [project](https://bluer555.github.io/MoCoGAN-HD/)


---
### X2Face: A network for controlling face generation by using images, audio, and pose codes (ECCV 2018)

> â­ï¸ **Keyword**: Warping based model, self-supervised training


- `X2Face`: A network for controlling face generation by using images, audio, and pose codes (ECCV 2018) : [arxiv](https://arxiv.org/abs/1807.10550), [project](https://www.robots.ox.ac.uk/~vgg/research/unsup_learn_watch_faces/x2face.html)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/x2face-1.jpeg?raw=1' width = '700' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> First training-stage: fully self-supervised </p></i></font>

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/x2face-2.png?raw=1' width = '700' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> Second training-stage </p></i></font>

#### Model

- <span style='background-color: #E5EBF7;'> <b>Embedding network</b> </span>: source frameìœ¼ë¡œë¶€í„° identityë¥¼ ë½‘ì•„ë‚´ëŠ” network
  - U-Net ê³¼ pix2pix ì˜ architectureë¥¼ ë”°ë¦„
  - ì´ ë„¤íŠ¸ì›Œí¬ëŠ” source frameì˜ ì •ë©´í™”ëœ ì–¼êµ´ì„ ì¶”ì¶œí•˜ë¼ê³  ê°•ìš”í•˜ì§€ëŠ” ì•Šì§€ë§Œ, pose/í‘œì •ê³¼ ë¬´ê´€í•œ source frameë§Œì˜ ê³ ìœ í•œ ì–¼êµ´ì„ ìƒì„±í•˜ë ¤ê³  í•˜ë‹¤ë³´ë‹ˆ embedded faceê°€ ì •ë©´ì„ ë°”ë¼ë³´ëŠ” ì–¼êµ´ë¡œ ìƒì„±ë¨
- <span style='background-color: #E5EBF7;'> <b>Driving network</b> </span>
  - encoder-decoder architecture
  - driving frame(input)ìœ¼ë¡œ ë¶€í„° driving vectorë¥¼ embeddingí•œ í›„(*latent embedding*), embedded faceë¥¼ pixelë‹¨ìœ„ë¡œ transformí•˜ì—¬ target image ìƒì„±

#### Training

- <span style='background-color: #E5EBF7;'> <b>Training the network</b> </span>: ì´ 2 stagesë¡œ training í•¨
  1. **First training-stage**: **fully self-supervised**
      - ex) ê°™ì€ ë¹„ë””ì˜¤ì—ì„œ 4ê°œì˜ í”„ë ˆì„ì„ ì¶”ì¶œí–ˆì„ ë•Œ, 3ê°œëŠ” source imageë¡œ ì‚¬ìš©í•˜ê³ , ë‚˜ë¨¸ì§€ í•˜ë‚˜ì˜ í”„ë ˆì„ì€ driving frameìœ¼ë¡œ ì‚¬ìš©. source frameìœ¼ë¡œë¶€í„° ì–»ì€ embedded faceì™€ driving frameì—ì„œ ì–»ì€ driving vectorë¥¼ ì´ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œ í›„, ìƒì„±ëœ ì´ë¯¸ì§€ì™€ driving frameê°„ì— `L1 loss`ë¥¼ í†µí•´ networkë¥¼ ì—…ë°ì´íŠ¸
      - ì´ë ‡ê²Œë§Œ í•™ìŠµì„ í•˜ë©´ ìƒì„±ëœ ì´ë¯¸ì§€ê°€ embedded faceì—ì„œ identityì— ëŒ€í•œ ì •ë³´ë¥¼ ì–»ì§€ ì•Šê³ , driving vectorì—ì„œë„ identityì— ëŒ€í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ. second training-stageì—ì„œ identity loss functionì„ ë„ì…í•˜ì—¬ identity bleedingì´ ìƒê¸°ì§€ ì•Šë„ë¡ í•¨
  2. **Second training-stage**
      - Generated frameì´ source frameì˜ identityë¥¼ ë”°ë¥´ë„ë¡ ê°•ì œí•˜ëŠ” identity loss functionì„ ë‘ 
      - $L_{identity}$ëŠ” ì‚¬ì „ì— í›ˆë ¨ëœ 11-layer VGG networkë¥¼ ì‚¬ìš©
      - ë‘ê°€ì§€ì˜ loss term
        - $L_{identity}(d_A, g_{d_A})$: $g_{d_A}$ì™€ $d_A$ëŠ” pose, í‘œì •, identity ë“± ëª¨ë“ ê²Œ ê°™ìŒ(ì‚¬ì‹¤ìƒ recon loss). photometric *L1* lossì™€ *L1* content lossë¥¼ ì‚¬ìš©
        - $L_{identity}(s_A, g_{d_R})$: *L1* content lossë§Œì„ ì‚¬ìš©

- **Result**
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/x2face-4.gif?raw=1' width = '700' ></p>

- **Comparison**
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/x2face-3.png?raw=1' width = '700' ></p>

---
### Monkey-Net: Animation Arbitrary Objects via Deep Motion Transfer (CVPR 2019)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/monkey-net-1.png?raw=1' width = '700' ></p>

> â­ï¸ **Keyword**: MOviNg KEYpoints, Warping based model, optical flow, self-supervised training, object-agnostic deep model

- `Monkey-Net`: Animating Arbitrary Objects via Deep Motion Transfer (CVPR 2019) : [Paper](https://arxiv.org/abs/1812.08861), [project](http://www.stulyakov.com/papers/monkey-net.html), [code](https://github.com/AliaksandrSiarohin/monkey-net)


#### Model


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/monkey-net-7.jpg?raw=1' width = '700' ></p>

<span style='background-color: #E5EBF7;'> <b>(1) Keypoint Detector $\triangle$</b> </span>

  - ë¬¼ì²´ì˜ keypointë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆë„ë¡ unsupervised í•˜ê²Œ í•™ìŠµ
  - source imageì™€ driving videoì˜ frameë“¤ë¡œë¶€í„° sparse keypointë¥¼ ì¶”ì¶œí•¨ (objectì˜ structure ë¿ë§Œ ì•„ë‹ˆë¼ motionê¹Œì§€ capture)

  <p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/monkey-net-3.jpg?raw=1' width = '900' ></p>

<span style='background-color: #E5EBF7;'> <b>(2) Dense Motion prediction network</b> </span>
  
  - sparse keypointì—ì„œ dense motion heatmapì„ ìƒì„±í•´ì„œ motion ì •ë³´ë¥¼ ë” ì˜ encoding í•  ìˆ˜ ìˆê²Œ í•¨
  <p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/monkey-net-4.jpg?raw=1' width = '900' ></p>

<span style='background-color: #E5EBF7;'> <b>(3) Motion Transfer Generator Network $G$</b> </span>
  
  - dense motion heatmapê³¼ ì™¸í˜•ì— ëŒ€í•œ ì •ë³´ë¥¼ ì´ìš©í•˜ì—¬ output frameì„ í•©ì„±
  - convolutional blockìœ¼ë¡œ ëœ encoder-decoder êµ¬ì¡°

  <p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/monkey-net-5.jpg?raw=1' width = '600' ></p>

<span style='background-color: #E5EBF7;'> <b>(4) Final</b> </span>


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/monkey-net-6.jpg?raw=1' width = '900' ></p>

#### Training

- X2faceì²˜ëŸ¼ self-supervised ë°©ì‹ìœ¼ë¡œ objectì˜ latent representationì„ í•™ìŠµ
- <span style='background-color: #E2F0D9;'> **Loss Function** </span>: GAN Loss + Feature-matching Loss

$$\mathcal{L}_{\text {tot }}=\lambda_{\text {rec }} \mathcal{L}_{\text {rec }}+\mathcal{L}_{\text {gan }}^{G}$$

- <span style='background-color: #E2F0D9;'> **GAN Loss** </span>
  - real image $\boldsymbol{x}^{\prime}$, generated image $\hat{\boldsymbol{x}^{\prime}}$
  - ***Discriminator Loss***: real image $\boldsymbol{x}^{\prime}$ ë¥¼ 1ë¡œ íŒë³„í•˜ë ¤ê³  í•˜ê³ , fake image $\hat{\boldsymbol{x}^{\prime}}$ ë¥¼ 0ìœ¼ë¡œ íŒë³„í•˜ë ¤ê³  í•¨

$$\begin{aligned}
\mathcal{L}_{\text {gan }}^{D}(D)=& \mathbb{E}_{\boldsymbol{x}^{\prime} \in \mathcal{X}}\left[\left(D\left(\boldsymbol{x}^{\prime} \oplus H^{\prime}\right)-1\right)^{2}\right] \\
&\left.+\mathbb{E}_{\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right) \in \mathcal{X}^{2}}\left[D\left(\hat{\boldsymbol{x}}^{\prime} \oplus H^{\prime}\right)\right)^{2}\right]
\end{aligned}$$

  - ***Generator Loss***: fake image $\hat{\boldsymbol{x}^{\prime}}$ ê°€ real(1) ì²˜ëŸ¼ ë³´ì´ê²Œ `Discriminator`ë¥¼ ì†ì´ë ¤ê³  í•¨
  
$$\mathcal{L}_{\mathrm{gan}}^{G}(G)=\mathbb{E}_{\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right) \in \mathcal{X}^{2}}\left[\left(D\left(\hat{\boldsymbol{x}}^{\prime} \oplus H^{\prime}\right)-1\right)^{2}\right]$$

- <span style='background-color: #E2F0D9;'> **Feature-matching Loss** </span>
  - VGG pretrained modelì´ í•„ìš”í•œ perceptual lossì™€ ë‹¤ë¥´ê²Œ ì´ lossëŠ” external pretrained networkê°€ í•„ìš” ì—†ìŒ

$$\left.\mathcal{L}_{\mathrm{rec}}=\mathbb{E}_{\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)}\left[\| D_{i}\left(\hat{\boldsymbol{x}}^{\prime} \oplus H^{\prime}\right)-D_{i}\left(\boldsymbol{x}^{\prime} \oplus H^{\prime}\right)\right) \|_{1}\right]$$

<span style='background-color: #E5EBF7;'> **Inference (Test) Time** </span>
- source imageëŠ” driving videoì—ì„œ ë½‘ì€ keypoint trajectoryì— ë”°ë¼ ì›€ì§ì´ê²Œ ë¨


- **Result**
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/monkey-net-2.gif?raw=1' width = '700' ></p>

---
### FOMM: First Order Motion Model for Image Animation (NeurIPS 2019)

<p align='center'><img src='https://github.com/AliaksandrSiarohin/first-order-model/raw/master/sup-mat/vox-teaser.gif?raw=1' width = '700' ></p>

> â­ï¸ **Keyword**: Monkey-Netì˜ í›„ì† ë…¼ë¬¸, Warping based model, optical flow, self-supervised training, local affine transformation, occlusion-masking

- `FOMM`: First Order Motion Model for Image Animation (NeurIPS 2019) : [arxiv](https://arxiv.org/abs/2003.00196), [code](https://github.com/AliaksandrSiarohin/first-order-model)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/fomm-1.jpg?raw=1' width = '700' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> Monkey-net: drivingì´ ìƒê¸´ ë¶€ë¶„ì´ ì´ìƒí•˜ê²Œ ê·¸ë ¤ì§ </p></i></font>

- **Monkey-Netì˜ í›„ì† ë…¼ë¬¸ âœ¨** (ê°™ì€ 1ì €ì!)
  - Monkey-netì€ heatmapì„ ë°”íƒ•ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ warpingí•  ë•Œ, keypoint ì£¼ë³€ì˜ object appearanceë¥¼ ì œëŒ€ë¡œ ë³€í™˜í•˜ì§€ ëª»í•¨ (ìœ„ì˜ ê·¸ë¦¼ ì°¸ê³ , ì €ìë“¤ì€ Monkey-Netì„ 0-th modelë¼ê³  ë¶€ë¦„) â¡ **First-Order Model**ì—ì„œëŠ” `local affine transformation`ê³¼ `occlusion-aware generator`ë¥¼ ë„ì…í•˜ì—¬ ì´ë¥¼ í•´ê²°
  - X2face, Monkey-Netì™€ ë§ˆì°¬ê°€ì§€ë¡œ **self-supervised ë°©ì‹**ìœ¼ë¡œ í•™ìŠµ  

#### Model 

> âœğŸ» First-Order Modelì€ Monkey-Netì˜ í›„ì† ë…¼ë¬¸ìœ¼ë¡œ, ì‚¬ì‹¤ìƒ êµ¬ì¡°ê°€ ë¹„ìŠ·í•˜ë‹¤. ì°¨ì´ì ì€ *(1) self-learned keypointë¥¼ ë½‘ì„ ë•Œ **local affine transformation**ì„ í•˜ì—¬ ëª¨ë¸ì˜ motionì„ ë³´ë‹¤ ë” ì •êµí•˜ê²Œ ë½‘ëŠ” ë‹¤ëŠ” ê²ƒ*ê³¼ *(2) **occlusion-aware generator**: ì´ë¯¸ì§€ì˜ contextë¥¼ íŒŒì•…í•˜ë„ë¡ ë„ì™€ì£¼ëŠ” occlusion maskë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ì *ì´ ìˆë‹¤.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/fomm-3.jpg?raw=1' width = '700' ></p>

<span style='background-color: #E5EBF7;'> <b>(1) Motion Estimation Module</b> </span>

- **Goal â­**: dense motion field ì˜ˆì¸¡ - $S$ ë¡œë¶€í„° feature mapì„ ê³„ì‚°í•´ì„œ $D$ ì˜ object poseë¡œ align
- `Unsupervised keypoint detector`ì™€  `Dense Motion network`ë¡œ êµ¬ì„±
- self-supervised ë°©ì‹ìœ¼ë¡œ í•™ìŠµëœ `keypoint detector`ë¡œ ë¶€í„° keypointë¥¼ ë½‘ì€ í›„, `local affine transformation`ìœ¼ë¡œ ê° keypoint ì£¼ë³€ì˜ motionë“¤ì„ modeling
  - ë‹¨ìˆœíˆ keypoint displacement ë§Œì„ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ `local affine transformation`ì„ í•˜ë©´ ë” í° ë²”ìœ„ì— ëŒ€í•´ transformationì„ í•  ìˆ˜ ìˆìŒ (affine ë³€í™˜ ìì²´ê°€ ì´ ë…¼ë¬¸ì˜ contribution âœ¨)

<span style='background-color: #E2F0D9;'> <b>(1-1) Local Affine Trnasformations for Approximate Motion Description</b> </span>

- forward optical flow ë°©ì‹ì´ ì•„ë‹Œ **backward optical flow** ë°©ì‹ì„ ì±„íƒ

$$\mathcal{T}_{\mathbf{S}\leftarrow \mathbf{D}} : \mathbb{R}^{2} \rightarrow \mathbb{R}^{2}$$

- FOMMì—ì„œëŠ” ë‹¤ì†Œ íŠ¹ì´í•œ ë°©ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œë‹¤. source imageë¥¼ driving videoì˜ poseë¡œ transform ì‹œí‚¬ ë•Œ, source imageë¥¼ ë°”ë¡œ ë³€í˜•ì‹œí‚¤ì§€ ì•Šê³  ì¤‘ê°„ì— abstract reference frame $R$ ì„ ì‚¬ìš©í•˜ì—¬ ë³€í˜•ì‹œí‚¨ë‹¤. ì´ë ‡ê²Œ í•˜ë©´ $D$ì™€ $S$ë¥¼ ë…ë¦½ì ì¸ ë°©ì‹ìœ¼ë¡œ processingí•  ìˆ˜ ìˆë‹¤.

$$\mathcal{T}_{\mathbf{D}\leftarrow \mathbf{R}}  /  \mathcal{T}_{\mathbf{S}\leftarrow \mathbf{R}}$$

- ìˆ˜ì‹ ì„¤ëª…

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/fomm-2.jpg?raw=1' width = '700' ></p>

---
<span style='background-color: #E2F0D9;'> <b>(1-2) Combining Local Motion</b> </span>

- $\hat{\mathcal{T}}_{\mathbf{S}\leftarrow \mathbf{D}}$ ëŠ” $D$ ì˜ pixel locationì— mappingë˜ì–´ ìˆìœ¼ë¯€ë¡œ $S$ ì™€ pixel-to-pixelë¡œ alignì„ í•  í•„ìš”ê°€ ìˆìŒ (edge, texture...) â¡ Source frame $S$ ë¥¼ feature warping strategyì— ë”°ë¼ transform !
- Dense motion networkë¡œ heatmap $\mathcal{H}_k$ ê³„ì‚°

$$\mathbf{H}_{k}(z)=\exp \left(\frac{\left(\mathcal{T}_{\mathbf{D} \leftarrow \mathbf{R}}\left(p_{k}\right)-z\right)^{2}}{\sigma}\right)-\exp \left(\frac{\left(\mathcal{T}_{\mathbf{S} \leftarrow \mathbf{R}}\left(p_{k}\right)-z\right)^{2}}{\sigma}\right)$$

- Monkey-net ì²˜ëŸ¼ optical flowë¥¼ masking

$$\hat{\mathcal{T}}_{\mathbf{S} \leftarrow \mathbf{D}}(z)=\mathbf{M}_{0} z+\sum_{k=1}^{K} \mathbf{M}_{k}\left(\mathcal{T}_{\mathbf{S} \leftarrow \mathbf{R}}\left(p_{k}\right)+J_{k}\left(z-\mathcal{T}_{\mathbf{D} \leftarrow \mathbf{R}}\left(p_{k}\right)\right)\right)$$


<span style='background-color: #E5EBF7;'> <b>(2) Occlusion-aware Image Generation</b> </span>

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/fomm-4.jpg?raw=1' width = '700' ></p>

#### Training

- **Reconstruction Loss**: Monkey-netê³¼ ë‹¤ë¥´ê²Œ feature matching lossê°€ ì•„ë‹Œ VGG-19 networkë¡œ pretrain ëœ `perceptual loss`ë¥¼ ì‚¬ìš© 
  - MS-SSIMê³¼ ë¹„ìŠ·í•˜ê²Œ resolution ë³„ë¡œ ì´ lossë¥¼ ì‚¬ìš©

$$L_{r e c}(\hat{\mathbf{D}}, \mathbf{D})=\sum_{i=1}^{I}\left|N_{i}(\hat{\mathbf{D}})-N_{i}(\mathbf{D})\right|$$

- **Result**
<p align='center'><img src='https://github.com/AliaksandrSiarohin/first-order-model/raw/master/sup-mat/vox-teaser.gif?raw=1' width = '700' ></p>
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/fomm-5.gif?raw=1' width = '700' ></p>
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/fomm-6.gif?raw=1' width = '700' ></p>


---
### face vid2vid: One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing (CVPR 2021)

> â­ï¸ **Keyword**: NVIDA, Warping based model, optical flow, self-supervised training

- `face vid2vid`: One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing (CVPR 2021): [arxiv](https://arxiv.org/abs/2011.15126), [project](https://nvlabs.github.io/face-vid2vid/)

<p align='center'><img src='https://nvlabs.github.io/face-vid2vid/web_gifs/teaser.gif?raw=1' width = '700' ></p>


