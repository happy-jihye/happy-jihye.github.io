---
title: "[Paper Review] MocoGAN-HD: A Good Image Generator Is What You Need for High-Resolution Video Synthesis"
excerpt: ""


categories:
 - GAN
tags:
  - deeplearning
  - ai
  - GAN
  - vision
search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---


- Paper : `MocoGAN-HD`: A Good Image Generator Is What You Need for High-Resolution Video Synthesis (ICLR 2021) ([arxiv](https://arxiv.org/abs/2104.15069), [code](https://github.com/snap-research/MoCoGAN-HD), [project](https://bluer555.github.io/MoCoGAN-HD/))
  - `MoCoGAN`: Decomposing Motion and Content for Video Generation (CVPR 2018) ([arxiv](https://arxiv.org/abs/1707.04993), [code](https://github.com/sergeytulyakov/mocogan))
- [GAN-Zoos! (GAN 포스팅 모음집)](https://happy-jihye.github.io/gan/)

---


> `MocoGAN-HD`: 기존의 SoTA generator인 StyleGAN과 BigGAN의 pre-trained model을 이용하여 latent vector를 manipulation하는 방식으로 비디오를 생성하는 모델이다. Generator는 고정시킨 상태에서 latent vector만을 조절하여 이미지를 생성하는 컨셉이라 새롭지는 않았다. 
> 
> 특이했던 점은 이 모델이 pose를 찾는 방식으로 `motion generator`라는 LSTM model을 이용했다는 점이다. LSTM 모델을 적용하여 disentangle한 latent trajectory를 찾았다는 점이 흥미로웠다. 


<b>저자들이 주장한 contribution</b>
1. <b>High-resolution videos</b>를 생성할 수 있지만, computation resource가 많이 안듦
    - 🤔: baseline model로 biggan과 stylegan을 사용해서 그런거라.. 당연한 수순이 아닌가 싶음.. 
2. <font color='#2C4D88'>content와 motion이 disentangle한 상태에서 원하는 latent trajectory를 찾을 수 있는 <b>motion generator</b> 발견</font>
3. <b>cross-domain video synthesis</b>: 다른 domain의 dataset들에 대하여 image & motion generator를 훈련하여 비디오를 합성할 수 있음 

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/mocoganhd-1.PNG?raw=1' width = '700' ></p>

## 1. Generator
### 1.1 <i>pre-trained</i> Image Generator $G_I$

- Image Generator로는 StyleGAN과 BigGAN의 pretrained model을 사용
- 훈련과정에서 fix되기 때문에 image content의 video motion이 disentangle할 수 있으며, 서로 다른 domain에서의 video 합성이 가능해짐 
  - voxceleb dataset으로 motion을 capture한 후, ffhq-stylegan의 generator를 이용하여 비디오를 합성하는 방식(cross-domain) 

### 1.2 <i>pre-trained</i> Motion Generator $G_M$

#### LSTM network
- `Motion Generator`: 두 개의 `LSTM network(LSTM encoder, LSTM decoder)`를 이용하여 latent motion <i>trajectory</i> 를 예측 
  - `latent motion trajectory`: $\mathbf{Z}=\left\{\mathbf{z}_{1}, \mathbf{z}_{2}, \cdots, \mathbf{z}_{n}\right\}$ 
  - $n$: number of frames
  - latent vector: BigGAN은 normal distribution $p_z$에서, StyleGAN은 MLP에서

$$\mathbf{h}_{1}, \mathbf{c}_{1}=\operatorname{LSTM}_{\text {enc }}\left(\mathbf{z}_{1}\right)$$

$$\mathbf{h}_{t}, \mathbf{c}_{t}=\operatorname{LSTM}_{\text {dec }}\left(\epsilon_{t},\left(\mathbf{h}_{t-1}, \mathbf{c}_{t-1}\right)\right), \quad t=2,3, \cdots, n$$

- LSTM encoder로 initial hidden state와 cell state를 얻은 후, 이후부터는 LSTM decoder를 통해 $n-1$개의 continuous hidden state를 계산
  - $\epsilon_{t}$ : noise vector

- image generator의 input으로 latent vector를 넣어 비디오의 이미지들을 생성: $\tilde{\mathbf{x}}_{t}=G_I(z_t)$

$$\tilde{\mathbf{v}}=\left\{\tilde{\mathbf{x}}_{1}, \tilde{\mathbf{x}}_{2}, \cdots, \tilde{\mathbf{x}}_{n}\right\}$$

- real video clip

$$\mathbf{v}=\left\{\mathbf{x}_{1},\mathbf{x}_{2}, \cdots, \mathbf{x}_{n}\right\}$$

#### Motion Disentanglement

- [SeFA](https://happy-jihye.github.io/gan/gan-25/#21-sefa)나 [GanSpace](https://happy-jihye.github.io/gan/gan-25/#22-ganspace) 처럼 latent space의 interpretable direction을 찾기
  - sefa: Closed-Form Factorization of Latent Semantics in GANs (CVPR 2021) : [arxiv](https://arxiv.org/abs/2007.06600), [code](https://github.com/happy-jihye/GAN/tree/main/SEFA), [review](https://happy-jihye.github.io/gan/gan-25/#21-sefa)
  - GANSpace: Discovering Interpretable GAN Controls (NeurIPS 2020) : [arxiv](https://arxiv.org/abs/2004.02546), [code](https://github.com/harskish/ganspace), [review](https://happy-jihye.github.io/gan/gan-25/#22-ganspace)
- PCA(principal component analysis)로 latent vector에 대한 기저벡터 $V$를 구한 후, 현재 프레임 $z_t$에 대한 motion direction을 추정

$$\mathbf{z}_{t}=\mathbf{z}_{t-1}+\lambda \cdot \mathbf{h}_{t} \cdot \mathbf{V}, \quad t=2,3, \cdots, n$$

#### Motion Diversity

- `motion mode collapse`: 위에 LSTM 식에서 motion을 다양하게 조절하기 위해 noise $\epsilon_{t}$ 를 추가했지만, LSTM decoder가 이를 무시하는 문제가 있었다고 함
  - $G_M$이 다양한 motion pattern을 capture하지 못하고, 특정 latent code에 대해서 비슷한 motion들만 만들어냄
- 이를 해결하기 위해 $L_m$ loss를 도입하여 hidden vector $h_t$와 noise vector $\epsilon_{t}$ 의 <i>mutual information</i>이 maximize되도록 함

$$\mathcal{L}_{\mathrm{m}}=\frac{1}{n-1} \sum_{t=2}^{n} \operatorname{sim}\left(H\left(\mathbf{h}_{t}\right), \epsilon_{t}\right)$$

---

## 2. Discriminator

### 2.1 Multi-Scale Video Discriminator $D_V$
- Discriminator로는 multi-scale video discriminator $D_V$ 를 사용. 
- [PatchGAN architecture 기반](https://happy-jihye.github.io/gan/gan-8/#322-markovian-discriminator-patchgan)
  - 2D 보다 3D conv layer가 modeling을 더 잘하므로 3D conv 사용

- Adversarial 하게 학습

$$\mathcal{L}_{D_{\mathrm{V}}}=\mathbb{E}_{\mathbf{v} \sim p_{v}}\left[\log D_{\mathrm{v}}(\mathbf{v})\right]+\mathbb{E}_{\mathbf{z}_{1} \sim p_{z}}\left[\log \left(1-D_{\mathrm{V}}\left(G_{\mathrm{I}}\left(G_{\mathrm{M}}\left(\mathbf{z}_{1}\right)\right)\right)\right)\right]$$

- 다만 `L_Dv Loss` 만을 이용하여 학습을 하면, $\tilde{\mathbf{x}}_{t}$ 의 identity(content)가 첫번째 생성 이미지 $\tilde{\mathbf{x}}_{1}$ 와 달라질 수도 있음 ➡ <b>Contrastive Image Discriminator</b>를 도입 !

### 2.2 Contrastive Image Discriminator $D_I$

> $\tilde{\mathbf{x}}_{t}$ 과 $\tilde{\mathbf{x}}_{1}$ 간의 content와 image quality가 match 되는지를 판별

<b>Quality Matching</b>

$$\mathcal{L}_{D_{\mathrm{I}}}=\mathbb{E}_{\mathbf{z}_{1} \sim p_{z}}\left[\log D_{\mathrm{I}}\left(G_{\mathrm{I}}\left(\mathbf{z}_{1}\right)\right)\right]+\mathbb{E}_{\mathbf{z}_{1} \sim p_{z}, \mathbf{z}_{t} \sim G_{\mathrm{M}}\left(\mathbf{z}_{1}\right) \mid t>1}\left[\log \left(1-D_{\mathrm{I}}\left(G_{\mathrm{I}}\left(\mathbf{z}_{t}\right)\right)\right)\right]$$

<b>Content Matching</b>

- 이미지간의 content가 같도록 constrastive loss function $L_{constr}$ 도입

$$\mathcal{L}_{\text {contr }}=-\sum_{i=1}^{N} \sum_{\alpha=a}^{b} \log \frac{\exp \left(\operatorname{sim}\left(F\left(\tilde{\mathbf{x}}_{t}^{(i a)}\right), F\left(\tilde{\mathbf{x}}_{t}^{(i b)}\right)\right) / \tau\right)}{\sum_{j=1}^{N} \sum_{\beta=a}^{b} \mathbb{1}_{[j \neq i]}\left(\exp \left(\operatorname{sim}\left(F\left(\tilde{\mathbf{x}}_{t}^{(i \alpha)}\right), F\left(\tilde{\mathbf{x}}_{t}^{(j \beta)}\right)\right) / \tau\right)\right.}$$

## 3. Full Objective

$$\min _{G_{M}}\left(\max _{D_{\mathrm{V}}} \mathcal{L}_{D_{\mathrm{V}}}+\max _{D_{\mathrm{I}}} \mathcal{L}_{D_{\mathrm{I}}}\right)+\max _{G_{\mathrm{M}}}\left(\lambda_{\mathrm{m}} \mathcal{L}_{\mathrm{m}}+\lambda_{\mathrm{f}} \mathcal{L}_{\mathrm{f}}\right)+\min _{D_{\mathrm{I}}}\left(\lambda_{\text {contr }} \mathcal{L}_{\text {contr }}\right)$$

---

## Result


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/mocoganhd-2.PNG?raw=1' width = '700' ></p>

<p align='center'>
  <iframe src="https://bluer555.github.io/MoCoGAN-HD/videos/FaceForensics.mp4"
    frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen style="width: 42.5em; height: 25em;"></iframe>
</p>

<p align='center'>
  <iframe src="https://bluer555.github.io/MoCoGAN-HD/videos/LSUN-Church.mp4"
    frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen style="width: 42.5em; height: 25em;"></iframe>
</p>

<p align='center'>
  <iframe src="https://bluer555.github.io/MoCoGAN-HD/videos/AFHQ-DOG-Interpolate_32.mp4"
    frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen style="width: 42.5em; height: 25em;"></iframe>
</p>


