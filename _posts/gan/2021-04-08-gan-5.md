---
title: "[Paper Review] PGGAN : Progressive Growing of GANs for Improved Quality, Stability, and Variation ë…¼ë¬¸ ë¶„ì„"
excerpt: "layerë¥¼ ì ì§„ì ìœ¼ë¡œ ìŒ“ì•„ ê³ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œ NVIDIAì˜ PGGAN(Progressive Growing of GANs) modelì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤."

categories:
 - GAN
tags:
  - deeplearning
  - ai
  - GAN
  - vision
search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 

use_math: true
---

> âœğŸ» ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” layerë¥¼ ì ì§„ì ìœ¼ë¡œ ìŒ“ì•„ ê³ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œ NVIDIAì˜ **PGGAN(Progressive Growing of GANs)** modelì— ëŒ€í•´ ì‚´í´ë³¸ë‹¤.


- Paper : [Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://arxiv.org/abs/1710.10196)
          (2018 / Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen)
          
- [GAN-Zoos! (GAN í¬ìŠ¤íŒ… ëª¨ìŒì§‘)](https://happy-jihye.github.io/gan/)

---


## 1. Introduction

Autoregressive models(ex. PixelCNN), VAEs, GANs ë“± ë§ì€ ìƒì„±ëª¨ë¸ë“¤ì´ ìˆë‹¤. ë³¸ ë…¼ë¬¸ì€ ì´ ì¤‘ì—ì„œë„ GANì˜ architectureë¥¼ ì‚¬ìš©í•œ ë…¼ë¬¸ì´ë‹¤.

- **Autoregressive models** : sharp images, slow to evaluate, no latent space
- **VAE** : fast to train, blurry images,
- **GANs** : sharp images, low resolutioin, limited variation, unstabble training

<span style="background-color: #FFF2CC;"> GAN </span>

GANì˜ `generator`ëŠ” latent codeë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë©°, ì´ìƒì ì¸ ê²½ìš°ì—ëŠ” ì´ ì´ë¯¸ì§€ì˜ ë¶„í¬ë¥¼ training distributionì™€ êµ¬ë³„í•˜ê¸° ì–´ë µë‹¤. `discriminator`ëŠ” ì´ë¯¸ì§€ë¥¼ ì˜ í‰ê°€í•˜ë„ë¡ í•™ìŠµí•œë‹¤.

ë³´í†µ GANì—ì„œëŠ” generatorë¥¼ ì˜ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.(discriminatorëŠ” í•™ìŠµì¤‘ì—ë§Œ ì‚¬ìš©ë˜ê³  ì´í›„ì—ëŠ” ë²„ë ¤ì§)

- GANì— ëŒ€í•œ ì„¤ëª…ì€ ì´ ê¸€ì„ ì°¸ê³  : [[Paper Review] Generative Adversarial Networks(GAN) ë…¼ë¬¸ ì„¤ëª… ë° pytorch ì½”ë“œ êµ¬í˜„](https://happy-jihye.github.io/gan/gan-1/)

---

### â­ Challenge


GANì—ëŠ” í•´ê²°í•´ì•¼í•  ë¬¸ì œì ë“¤ì´ ìˆë‹¤.

<span style="background-color: #D5E0EF;">1. generated distributionê³¼ training distributionë“¤ì´ ê²¹ì¹˜ëŠ” ë¶€ë¶„(overlap)ì´ ì ë‹¤ë©´, ì´ ë¶„í¬ë“¤ê°„ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•  ë•Œ gradientëŠ” randomí•œ ë°©í–¥ì„ ê°€ë¦¬í‚¬ ìˆ˜ ìˆë‹¤. </span>
   - original GAN(ì´ì•ˆ êµ¿ íŒ°ë¡œìš°)ì—ì„œëŠ” Jensen-Shannon Divergenceë¥¼ distance metricìœ¼ë¡œ ì‚¬ìš©í–ˆë‹¤ë©´, ìµœê·¼ì—ëŠ” **least squares**ë‚˜ **Wasserstein distance**ë“±ì˜ metricì„ ì‚¬ìš©í•´ì„œ ëª¨ë¸ì„ ì•ˆì •í™” ì‹œì¼°ë‹¤.

<span style="background-color: #D5E0EF;">2. [mode collapse](https://ratsgo.github.io/generative%20model/2017/12/20/gan) : generated distributionì´ ì‹¤ì œ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ëª¨ë‘ ì»¤ë²„í•˜ì§€ ëª»í•˜ê³  ë‹¤ì–‘ì„±ì„ ìƒì–´ë²„ë¦¬ëŠ” í˜„ìƒì„ ëœ»í•œë‹¤. GëŠ” ê·¸ì € lossë§Œì„ ì¤„ì´ë ¤ê³  í•™ìŠµì„ í•˜ê¸° ë•Œë¬¸ì— ì „ì²´ ë°ì´í„° ë¶„í¬ë¥¼ ì°¾ì§€ ëª»í•˜ê²Œ ë˜ê³ , ê²°êµ­ì—ëŠ” í•˜ë‚˜ì˜ modeì—ë§Œ ê°•í•˜ê²Œ ëª°ë¦¬ê²Œ ë˜ëŠ” ê²½ìš°ì´ë‹¤. </span>
   - ì˜ˆë¥¼ ë“¤ì–´, MNISTì—ì„œ `G`ê°€ íŠ¹ì • ìˆ«ìë§Œì„ ìƒì„±í•˜ê²Œë˜ëŠ” ê²½ìš°ê°€ ì´ì— ì†í•œë‹¤.

<span style="background-color: #D5E0EF;">3. High-resolutionì˜ imageë¥¼ ìƒì„±í•  ìˆ˜ë¡, ê°€ì§œ ì´ë¯¸ì§€ë¼ê³  íŒë³„í•˜ê¸° ì‰¬ì›Œì§„ë‹¤. </span>

<span style="background-color: #D5E0EF;">4. High-resolutionì˜ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” memory constraint ë•Œë¬¸ì— ë” ì‘ì€ minibatchë¥¼ ì‚¬ìš©í•´ì•¼í•˜ê³ , training stability ì—­ì‹œ ë–¨ì–´ì§„ë‹¤. </span>


> â­ ë”°ë¼ì„œ ì´ëŸ¬í•œ ë¬¸ì œì ë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•´ PGGANì—ì„œëŠ” **Generatorì™€ Discriminatorë¥¼ ì ì§„ì ìœ¼ë¡œ í•™ìŠµ**ì‹œí‚¨ë‹¤. ì¦‰, ë§Œë“¤ê¸° ì‰¬ìš´ low-resolutionë¶€í„° ì‹œì‘í•˜ì—¬ ìƒˆë¡œìš´ layerë¥¼ ì¡°ê¸ˆì”© ì¶”ê°€í•˜ê³  higher-resolutionì˜ detailë“¤ì„ ìƒì„±í•œë‹¤. 

## 2. Progressive Growing of GANs

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/pggan1.gif?raw=1" width = "700" ></p>

ìœ„ì˜ ê·¸ë¦¼ì²˜ëŸ¼ PGGANì€ low-resolutionì˜ imageì—ì„œ ì‹œì‘í•˜ì—¬ ì ì°¨ layerë¥¼ ì¶”ê°€í•˜ë©´ì„œ high-resolutionì„ í•™ìŠµí•˜ê²Œ ëœë‹¤. ë˜í•œ, discriminatorëŠ” generatorì™€ ëŒ€ì¹­ì˜ í˜•íƒœë¥¼ ì´ë£¨ê³  ìˆìœ¼ë©° ëª¨ë“  layerë“¤ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

> ì²˜ìŒì—ëŠ” **large scale(low frequency)**ì˜ ì •ë³´ë“¤ì„ í•™ìŠµí•˜ê³ , ì ì°¨ **fine scale(higher frequency)**ì˜ ì •ë³´ë“¤ì„ í•™ìŠµí•˜ê²Œ ëœë‹¤.

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/pggan2.png?raw=1" width = "800" ></p>

ì´ëŸ¬í•œ Progressive trainingì€ ëª‡ê°€ì§€ ì¥ì ì´ ìˆë‹¤.
1. **Stable** : low-resolutionì˜ ì´ë¯¸ì§€ë¥¼ í•™ìŠµí•˜ë©´ class informationë„ ì ê³  modeë„ ëª‡ì—†ê¸° ë•Œë¬¸ì— ì•ˆì •ì ì´ë‹¤.
    - ê´€ë ¨ ì—°êµ¬ : WGAN-GP loss, LSGAN loss
2. **Reduced training time** : PGGANì€ lower resolutionì—ì„œë¶€í„° ë¹„êµí•˜ì—¬ í•™ìŠµì„ í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµì†ë„ê°€ 2-6ë°°ë‚˜ ë¹¨ë¼ì§„ë‹¤.

---
**Fading in higher resolution layers**

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/pggan3.png?raw=1" width = "800" ></p>


Gì™€ Dì˜ resolutionì„ upsamplingí•  ë•Œ, PGGANì€ ìƒˆë¡œìš´ layerì— **fade in**í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.

(a) **stabilize** 
  - (16x16x512) ->`to RGB` -> (16x16x1) -> `from RGB` -> (16x16x512)

(b) **transition (fade in)** : í•´ìƒë„ë¥¼ ëŠ˜ë¦¬ëŠ” ê³¼ì •
  - ì´ ê·¸ë¦¼ì˜ `to RGB`ëŠ” 32x32ì— ìµœì í™”ëœ 1x1 convolution
  - ì´ë•Œ $\alpha$ê°’ì´ í•µì‹¬ì´ë‹¤. ì²˜ìŒì—ëŠ” $\alpha$ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”ì‹œì¼œì£¼ê³ (16x16ì„ 2xë¡œ upsamplingí•˜ê³  `to RGB`ë¥¼ ê±°ì³ ì˜¤ë„ë¡), ì ì  $\alpha$ë¥¼ 1ë¡œ ì¦ê°€ì‹œì¼œê°€ë©° í•™ìŠµì„ í•´ì„œ 32x32ì— ëŒ€í•´ ìƒì„±í•œ ê°’ì„ ë¶ˆëŸ¬ì˜¤ë„ë¡ í•œë‹¤.

(c) **stablilize**


resolutionì„ ë†’ì´ë©´ì„œ(32x32) í•™ìŠµì„ ì§„í–‰í•  ë•Œ, ê¸°ì¡´ì— í•™ìŠµí–ˆë˜ ì´ë¯¸ì§€ì— ëŒ€í•œ ì •ë³´(16x16)ì„ ìŠì–´ë²„ë¦´ ìˆ˜ë„ ìˆìœ¼ë‹ˆê¹Œ residual blockì„ ì´ìš©í•˜ì—¬ í•œë²ˆ ë” ë”í•´ì£¼ëŠ” ê²ƒì´ë‹¤. í•´ìƒë„ë¥¼ ì¤„ì´ëŠ” ê²½ìš°ë„ ë§ˆì°¬ê°€ì§€ì˜ ì´ìœ ë¡œ Fade inì„ í•´ì¤€ë‹¤.

> ğŸ˜‰ ì‚¬ì‹¤ ì´ techniqueì€ stylegan2ë¡œ ê°€ë©´ì„œ ê±°ì˜ ì‚¬ìš©í•˜ì§€ ì•Šì•„ì„œ.. ê·¸ëƒ¥ ì´ëŸ°ê²Œ ìˆì—ˆë‹¤ ì •ë„ë§Œ ì•Œë©´ ë  ê²ƒ ê°™ë‹¤.

---

- ì—¬ëŸ¬ê°œì˜ ìƒì„±ìë‚˜ íŒë³„ìë“¤ì„ ì‚¬ìš©í•˜ëŠ” ì—°êµ¬ë“¤ì´ ìˆì—ˆìŒ
  - [High-Resolution Image Synthesis and Semantic Manipulation with Conditional GAN](https://arxiv.org/abs/1711.11585)(2017), etc  
- PGGANì€ ì´ì— motivateë˜ì–´ <span style="background-color: #D5E0EF;">latentì—ì„œë¶€í„° ì—¬ëŸ¬ ë‹¨ê³„ì— ê±¸ì³ high-resolutionì˜ imageë¡œ mappingí•˜ëŠ” network</span>ë¥¼ ë§Œë“¤ì—ˆìŒ. ë‹¤ë§Œ ë‹¤ë¥¸ ì ì€ PGGANì€ **Single** GAN !!


## 3. Increasing Variation using Minibatch Standard Deviation

> **Goal** : Encouraging the minibatches of generated and training images to show similar statics


PGGANì—ì„œëŠ” mode collapsingì„ í•´ê²°í•˜ê¸° ìœ„í•œ í•œê°€ì§€ ë°©ë²•ì¸ <span style="background-color: #FFF2CC;">**Mini-batch discrimination**</span>ì˜ ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤. mini-batch ë³„ë¡œ ìƒì„±ì´ë¯¸ì§€ì™€ ì‹¤ì œ ì´ë¯¸ì§€ ì‚¬ì´ì˜ ê±°ë¦¬ í•©ì˜ ì°¨ì´ë¥¼ ëª©ì í•¨ìˆ˜ì— ì¶”ê°€í•˜ëŠ” ê²ƒì´ë‹¤.

- ì´ ê°’ì„ discriminatorì˜ ì–´ë””ì—ë‚˜ ì¶”ê°€í•´ë„ ë˜ì§€ë§Œ, ë³´í†µì€ ë§¨ ë’¤ì— ì¶”ê°€í•˜ê³¤ í•œë‹¤.
- ì´ ë°©ì‹ ì™¸ì—ë„ **repelling regularizer**ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤.

## 4. Normalization in Generator and Discriminator

GANì—ì„œëŠ” Gì™€ Dê°€ ê²½ìŸì„ í•  ë•Œ signal magnitudeê°€ ì»¤ì§€ê¸° ì‰½ë‹¤. ë”°ë¼ì„œ ë³´í†µì€ batch normalizationì„ í•˜ê³¤ í•œë‹¤. ê·¸ëŸ°ë° PGGANì—ì„œëŠ” signal magnitudeì„ í•  ë•Œ ì´ëŸ¬í•œ í˜„ìƒì´ ë‚˜íƒ€ë‚˜ì§€ ì•Šê¸° ë•Œë¬¸ì— parameterë¥¼ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ë°©ì‹ìœ¼ë¡œ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.

---
### 4.1 Equalized Learning Rate

batch sizeê°€ í° ì¼ë°˜ GANì˜ ê²½ìš° batch normì„ ì‚¬ìš©í•´ë„ ë¬¸ì œê°€ ì—†ì§€ë§Œ, PGGANì—ì„œëŠ” high-resolutionì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ì‘ì€ ì‚¬ì´ì¦ˆì˜ batchë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ê³  ê·¸ë ‡ê¸° ë•Œë¬¸ì— initilizationì´ êµ‰ì¥íˆ ì¤‘ìš”í•´ì§„ë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ëª¨ë“  layerì˜ learning speedê°€ ê°™ë„ë¡ **equalized learning rate**ì˜ ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤. gradientì˜ í•™ìŠµ ì†ë„ê°€ parameterì™€ ë¬´ê´€í•˜ë„ë¡ standard deviationìœ¼ë¡œ gradientë¥¼ normalizeí•˜ëŠ” ë°©ì‹ì´ë‹¤. (weightë¥¼ $N(0,1)$ì˜ ì •ê·œ ë¶„í¬ì—ì„œ initialization í•œ í›„, runtimeì‹œì— scaling í•´ì¤€ë‹¤.) 

<span style="background-color: #FFF2CC;">**(ì°¸ê³ ) Xavier initilization ì™€ He initilization**</span>

- **Xavier initilization**
  <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/pggan4.PNG?raw=1" width = "500" ></p>  
  <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/pggan5.PNG?raw=1" width = "500" ></p>  
  - ë§Œì•½ì— variationì´ 1ë³´ë‹¤ í¬ë‹¤ë©´ networkë¥¼ deepí•˜ê²Œ ìŒ“ì„ ìˆ˜ë¡ varianceê°€ ì»¤ì ¸ì„œ explodingì´ ë˜ê³ , variationì´ 1ë³´ë‹¤ ì‘ë‹¤ë©´ vanishingì´ ëœë‹¤.
  - ë”°ë¼ì„œ ì´ëŸ¬í•œ í˜„ìƒì„ ë§‰ê¸° ìœ„í•´ì„œ forward activationì´ë‚˜ gradientì—ì„œ **input activation $x_i$ì˜ í‰ê· ì´ 0, variationì´ 1ì´ë©´ output activation $Y$ì˜ í‰ê· ê³¼ ë¶„ì‚°ë„ ê°ê° 0, 1**ë¡œ ë§Œë“¤ì–´ì¤˜ì•¼ í•œë‹¤.  
  - $Var(Y)=1$ì´ ë˜ë ¤ë©´ $nVar(W)=1$ì´ ë˜ì–´ì•¼í•¨ìœ¼ë¡œ $Var(W)$ì€ $\frac{1}{n}$ì´ ë˜ì–´ì•¼í•œë‹¤.
  
  > â­ $W \sim N(0, \frac{1}{n})$ì´ ë˜ë„ë¡ í•™ìŠµì„ í•˜ë©´ networkë¥¼ deepí•˜ê²Œ ìŒ“ì•„ë„ layerë§ˆë‹¤ standard variationì´ 1ì´ ë  ê²ƒì´ë‹¤.

- Xavier initilizationì€ inputì—ì„œì˜ activationì´ linearë¼ê³  ê°€ì •ì„ í•˜ì§€ë§Œ, **He initilization**ì€ ì´ë¥¼ ReLUë‚˜ Leaky-ReLUë¡œ ê°€ì •ì„ í•œë‹¤. 
  - ReLUì™€ ê°™ì€ activation functionì„ ì‚¬ìš©í•˜ë©´ inputì˜ ì ˆë°˜ì€ 0ì´ ë˜ê¸° ë•Œë¬¸ì— Variationì´ 1ì´ ëœë‹¤ëŠ” ê°€ì •ì´ í‹€ë¦´ ìˆ˜ë„ ìˆë‹¤. ë”°ë¼ì„œ He's initilizationì—ì„œëŠ” ì´ë¥¼ ë³´ì •í•´ì£¼ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.

- **Equalized Learning Rate**
   
  <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/pggan6.PNG?raw=1" width = "850" ></p>  

  ì‚¬ì‹¤ forward ê³„ì‚°ì—ì„œëŠ” Equalized LRê³¼ He initializationëŠ” ì •í™•í•˜ê²Œ ë™ì¼í•˜ë‹¤.(ìˆœì„œë§Œ ë‹¤ë¦„) 

  ì°¨ì´ëŠ” gradientë¥¼ updateí•  ë•Œ ìƒê¸´ë‹¤. He initializationë¥¼ ì“´ë‹¤ë©´ RMSPropì´ë‚˜ Adamê³¼ ê°™ì€ adpative stochastic gradient descentì„ ì“¸ ë•Œ networkì˜ íŒŒë¼ë¯¸í„°ê°€ scaleì— independentí•˜ê²Œ updateê°€ ë˜ê²Œ ëœë‹¤.

---
### 4.2 Pixelwise Feature Vector Normalization in Generator

Dì™€ Gê°€ ê²½ìŸì„ í•˜ë©´ì„œ í¬ê¸°ê°€ controlì´ ì˜ ì•ˆë˜ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ PGGANì—ì„œëŠ” convolutional layerí›„ generatorì—ì„œ ê° pixel ë³„ë¡œ normalizationì„ í•´ì¤¬ë‹¤.

 
$$b_{x, y}=a_{x, y} / \sqrt{\frac{1}{N} \sum_{j=0}^{N-1}\left(a_{x, y}^{j}\right)^{2}+\epsilon}$$


> ğŸ˜‰ ì´ technique ì—­ì‹œ stylegan2ë¶€í„°ëŠ” ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.


## 5. Experiments

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/pggan8.PNG?raw=1" width = "850" ></p> 
<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/pggan7.PNG?raw=1" width = "850" ></p> 

ë‹¤ë¥¸ GANë“¤ê³¼ ë¹„êµí–ˆì„ ë•Œ ê³ í•´ìƒë„ì˜ ì´ë¯¸ì§€ê°€ ì˜ ì¶œë ¥ëœë‹¤.

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/pggan9.PNG?raw=1" width = "850" ></p> 

ë˜í•œ, í•™ìŠµ ì†ë„ ì—­ì‹œ ë§¤ìš° ë¹ ë¥´ë‹¤.

## 6. Discussion

ì´ì „ì˜ GANë“¤ì— ë¹„í•´ PGGANì€ high-resolutionì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµì„ í•  ìˆ˜ ìˆì—ˆë‹¤. ë‹¤ë§Œ, ì•„ì§ í˜„ì‹¤ì ì¸ ì‚¬ì§„ì„ ë§Œë“¤ê¸°ì—ëŠ” í•œê³„ê°€ ìˆë‹¤.

--
## 7. Code

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/pggan10.png?raw=1" width = "700" ></p> 
<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/pggan11.png?raw=1" width = "700" ></p> 

<script src="https://gist.github.com/happy-jihye/71d516689db2f1c3ed43dfaab7b7310c.js"></script>

{% gist happy-jihye/71d516689db2f1c3ed43dfaab7b7310c %}
## 8. Opinion

> stylegan1ì˜ baselineì´ ë˜ëŠ” PGGAN ë…¼ë¬¸ì„ ì½ì—ˆë‹¤. ëª¨ë¸ì˜ architectureìì²´ëŠ” ì–´ë µì§€ ì•Šì§€ë§Œ, ì„¸ë¶€ì ì¸ techniqueë“¤ì´ ì´í•´í•˜ê¸° ì–´ë ¤ì› ë˜ ë…¼ë¬¸ì´ì—ˆë‹¤. 

---

**Reference**
- https://hackmd.io/@_XGVS6ZYTL2p6MEHmqMvsA/HJ1BBDtP4?type=view
- https://github.com/deepsound-project/pggan-pytorch
- Naver AI LAB ìµœìœ¤ì œ ì—°êµ¬ì›ë‹˜ ë°œí‘œìë£Œ