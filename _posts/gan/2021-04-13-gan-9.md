---
title: "[Paper Review] SPADE : Semantic Image Synthesis with Spatially-Adaptive Normalization 논문 분석"
excerpt: "Semantic Image를 현실적인 image로 변환해주는 Spatially-adaptive normlization(SPADE) model에 대해 알아본다"

categories:
 - GAN
tags:
  - deeplearning
  - ai
  - pytorch
  - GAN
  - vision
search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---

> ✍🏻 이번 포스팅에서는 Semantic Image를 현실적인 image로 변환해주는 **Spatially-adaptive normlization(SPADE) model**에 대해 살펴본다.


- Paper : [Semantic Image Synthesis with Spatially-Adaptive Normalization](https://arxiv.org/abs/1903.07291) (CVPR 2019 / Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu)

- [GAN-Zoos! (GAN 포스팅 모음집)](https://happy-jihye.github.io/gan/)

---

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade2.gif?raw=1' width = '700' ></p>
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade3.gif?raw=1' width = '700' ></p>


## 1. Introduction

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade1.PNG?raw=1' width = '700' ></p>

> We propose **spatially-adaptive normalization**, **a conditional normalization layer** that modulates the activations using input semantic layouts through a spatially adaptive, learned transformation and can effectively propagate the semantic information throughout the network.

이전까지는 Semantic Image to photorealistic image task에서 convolution, normalization, nonlinearity layer로 구성된 deep nework를 사용했다. 이러한 방식을 사용하면 normalization과정에서 semantic 정보가 사라진다는 문제가 있었다.

본 논문에서는 이 문제를 해결하기 위해 spatially-adaptive normalization을 제안한다. 

## 2. Related Work

### 2.1 Conditional image synthesis

이미지를 생성할 때 conditional information을 주면 더 빠르고 현실적인 이미지가 생성된다.

Conditional image synthesis task들과 관련 논문
- **Class-conditional models**
  - [[Paper Review] CGAN : Conditional Generative Adversarial Nets 논문 분석](https://happy-jihye.github.io/gan/gan-3/)
  
- **Generating images based on text**
  - [[Paper Review] Generative Adversarial Text to Image Synthesis 논문 분석](https://happy-jihye.github.io/gan/gan-4/)

- **Image to Image translation**
  - [[Paper Review] Pix2pix : Image-to-Image Translation with Conditional Adversarial Networks 논문 분석](https://happy-jihye.github.io/gan/gan-8/)
  - <span style='background-color: #E5EBF7;'> **Segmentation masks to images : SPADE !** </span>

    *Segmentation masks to images*는 specific한 *Image to Image translation*으로, input으로는 semantic label map을 받고 output으로는 photo를 출력한다.

---

### 2.2 Unconditional normalization layers

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan-3.PNG?raw=1' width = '500' ></p>

위의 unconditional normalization layer들은 conditional normalization layer과 다르게 외부 데이터에 의존하지 않는다. 따라서 이를 사용하려면 labeling을 해줘야한다.

### 2.3 Conditional normalization layer

Conditional normalization layer는 외부 데이터에 의존한다.(style transfer)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade4.png?raw=1' width = '700' ></p>

보통 **Conditional normalization layer**은 그림과 같이
1. zero mean, unit deviation으로 normalization을 한 후,
2. 외부 데이터를 받아 denormalize를 한다.(affine transformation)

> 본 논문에서는 **normalization layer에 spatially-varing affine transformation을 적용하여 semantic mask로부터 이미지를 합성**한다.

## 3. Semantic Image Synthesis

- 각 pixel의 semantic segmentation mask : $\mathbf{m} \in \mathbb{L}^{H \times W}$
  
### 3.1 SPatially-Adaptive DEnormalization
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade5.PNG?raw=1' width = '700' ></p>

- mask를 embedding space로 projection한 후, $\gamma, \beta$로 convolution을 한다.
- 이때 modulation parmaeter $\gamma, \beta$는 vector가 아니라 tensor
  
$$\gamma_{c, y, x}^{i}(\mathbf{m}) \frac{h_{n, c, y, x}^{i}-\mu_{c}^{i}}{\sigma_{c}^{i}}+\beta_{c, y, x}^{i}(\mathbf{m})$$

- normalization으로는 instance norm이 아니라 Batch Norm을 사용했다.
  
$$\begin{aligned}
\mu_{c}^{i} &=\frac{1}{N H^{i} W^{i}} \sum_{n, y, x} h_{n, c, y, x}^{i} \\
\sigma_{c}^{i} &=\sqrt{\frac{1}{N H^{i} W^{i}} \sum_{n, y, x}\left(\left(h_{n, c, y, x}^{i}\right)^{2}-\left(\mu_{c}^{i}\right)^{2}\right)}
\end{aligned}$$

### 3.2 SPADE generator

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade6.PNG?raw=1' width = '700' ></p>


- SPADE에서는 learned modulation parameter를 사용하기 때문에 generator의 첫번째 layer로 segmentation map을 사용할 필요가 없다. 또한, normalization 과정에서 semantic mask정보를 입력해주기 때문에 Encoder 부분(downsampling)도 필요 없다. (Decoder 부분만 필요)

  따라서 generator의 input으로는 random noise vector를 사용하며, encoder를 제거함으로써 모델도 경량화되었다. 
  - input으로 random noise를 사용하기 때문에 latent vector를 조절하면서 style을 변화하는 것도 가능해졌다.

- upsampling layer로는 ResNet Block을 사용한다.
  - 모든 normalization layer의 modulation parm으로는 SPADE를 사용한다.
- multi-modal synthesis도 가능해졌다.

### 3.3 Why dose the SPADE work better?

> SPADE는 다른 normalization layer에 비해 semantic information이 잘 보존된다.

Instance Norm과 같은 normalization layer는 많은 conditional image 합성에서 SoTA를 달성했지만, uniform/flat segmentation mask에 적용하면 semantic information이 사라진다.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade7.PNG?raw=1' width = '700' ></p>

segmentation mask에서는 대부분의 pixel이 같은 label을 갖고 있다. 따라서 pix2pixHD 에서 Instance Norm을 사용하면 0으로 normalized되어 semantic information이 사라질 것이다.

### 3.4 Multi-model synthesis

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade8.PNG?raw=1' width = '700' ></p>

> Generator의 input으로 random vector를 사용함으로써 multi-modal synthesis가 가능해졌다.

- Encoder에서는 이미지의 style을 capture하고, generator에서는 SPADE에서의 segmentation mask information과 encoded style을 합친다.
  
- Encoder와 generator쪽의 architecture가 VAE와 비슷
- KL-Divergence loss로 학습

## 4. Experiments
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade9.PNG?raw=1' width = '700' ></p>
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade10.PNG?raw=1' width = '700' ></p>

COCO-stuff, ADE20K, Cityscapes dataset으로 학습을 시켰을 때, 다른 모델들과 비교해봐도 SPADE가 좋다.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade11.PNG?raw=1' width = '700' ></p>

Amazon Mechanical Turk(AMT, 사람이 직접 test)로 test를 해봐도 SPADE가 좋다.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade12.PNG?raw=1' width = '700' ></p>

pix2pix와 같은 다른 architecture에 SPADE를 사용해봐도 SPADE를 쓴게 좋다.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/spade13.PNG?raw=1' width = '700' ></p>

random noise를 바꿔보니 이미지 style이 변했다. multi-model synthesis가 잘된다. 

## 5. Conclusions

> 본 논문에서는 **Sp**atially-**A**daptive **(DE)**normalizatoin을 제안했다. 이를 통해 normalization layer에서 affine 변환을 할 때 semantic 정보를 잘 활용하도록 할 수 있었고, semantic image로부터 현실적인 이미지 합성이 가능해졌다.
> 
> 또한, multi-model synthesis 뿐만 아니라 guided image synthesis가 가능해졌다.


---
**Reference**
- Naver AI LAB 최윤제 연구원님 발표자료
- [https://paperswithcode.com/method/conditional-instance-normalization#](https://paperswithcode.com/method/conditional-instance-normalization#) 