---
title: "[Paper Review] Few-Shot Adversarial Learning of Realistic Neural Talking Head Models ë…¼ë¬¸ ì½ê¸°"
excerpt: "ì ì€ ì´ë¯¸ì§€ë¡œ í•™ìŠµê°€ëŠ¥í•œ Talking head modelì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤."


categories:
 - GAN
tags:
  - deeplearning
  - ai
  - GAN
  - vision
search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 

use_math: true
---

<p align='center'>
  <iframe src="https://www.youtube.com/embed/p1b5aiTrGzY"
    frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen style="width: 42.5em; height: 25em;"></iframe>
</p>

> âœğŸ» ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” ì ì€ ì´ë¯¸ì§€ë¡œ í•™ìŠµê°€ëŠ¥í•œ **Talking head model**ì— ëŒ€í•´ ì‚´í´ë³¸ë‹¤.


- Paper : [Few-Shot Adversarial Learning of Realistic Neural Talking Head Models](https://arxiv.org/abs/1905.08233) (arxiv 2019 /Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, Victor Lempitsky)

- [GAN-Zoos! (GAN í¬ìŠ¤íŒ… ëª¨ìŒì§‘)](https://happy-jihye.github.io/gan/)

---

## 1. Introduction

> â­ Goal : Few-shotì˜ ì´ë¯¸ì§€ë¡œ í•™ìŠµí•˜ì—¬ ì§„ì§œê°™ì€ **Talking head model** ë§Œë“¤ê¸°

**Realistic Neural Talking Head**ë¥¼ í•©ì„±í•˜ëŠ” ê±´ ì–´ë ¤ì›€ 
1. ì‚¬ëŒì˜ ì–¼êµ´ì€ ë„ˆë¬´ ë³µì¡ : ì–¼êµ´ë¿ë§Œ ì•„ë‹ˆë¼ ë¨¸ë¦¬, ì˜· ë“±ì„ ëª¨ë¸ë§í•˜ê¸° ì–´ë ¤ì›€
2. [Uncanny valley effect](https://ko.wikipedia.org/wiki/%EB%B6%88%EC%BE%8C%ED%95%9C_%EA%B3%A8%EC%A7%9C%EA%B8%B0) : ì‚¬ëŒê°™ì´ëŠ” ìƒê²¼ëŠ”ë° ì• ë§¤í•˜ê²Œ ë‹®ìœ¼ë©´ ê±°ë¶€ê°ì´ ì‹¬í•¨ â¡ ì •ë§ ì§„ì§œ ì‚¬ëŒê°™ì€ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ì•¼í•¨

---
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/fsth-1.PNG?raw=1' width = '800' ></p>

---

In this work, 
- <span style='background-color: #E5EBF7;'> <b>Few-shot learning</b> </span>ìœ¼ë¡œ talking head model ìƒì„±ì´ ê°€ëŠ¥. (one-shot learningìœ¼ë¡œë„ í•™ìŠµì´ ê°€ëŠ¥í•˜ì§€ë§Œ, few-shotì¼ ë•Œê°€ ë” ì„±ëŠ¥ì´ ì¢‹ìŒ) 
  - **meta-learning** : ì•„ì£¼ í° ë°ì´í„°ë¡œ talking head videoë¡œ í•™ìŠµì‹œí‚¨ pre-trained modelì„ fine-tunning í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— Few-shot learning ê°€ëŠ¥
- Warpingì´ ì•„ë‹ˆë¼ <span style='background-color: #E5EBF7;'> <b>Direct synthesis</b> </span>ì˜ ë°©ì‹ì„ ì°¨ìš©.
  - **Warping-based system** : ì ì€ ìˆ˜ì˜ ì´ë¯¸ì§€ë¡œë„ talking head sequencesë¥¼ ìƒì„±í•  ìˆ˜ ìˆì§€ë§Œ, ë‹¤ì–‘í•œ ë™ì‘ì´ë‚˜ ì›€ì§ì„ ë“±ì„ í•©ì„±í•˜ê¸´ ì–´ë ¤ì›€
  - **Direct synthesis (warping-free)** : Deep ConvNetì„ í›ˆë ¨ì‹œí‚¤ëŠ” ë°©ì‹. ë‹¤ë§Œ, large corpusì™€ ë§ì€ resource í•„ìš”

## 2. Methods

### 2.1 Architecture and Meta-Learning stage

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/fsth-4.jpg?raw=1' width = '800' ></p>
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/fsth-3.PNG?raw=1' width = '800' ></p>

---

<span style='background-color: #E5EBF7;'> <b>Generator</b> </span>

- **Generator**ëŠ” reference video(source)ì˜ ëª‡ëª‡ frameë“¤ì„ Embeddingí•œ ê²°ê³¼ $\hat{e_i}$ì™€ Target Imageì˜ Landmark $y_i(t)$ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ìƒˆë¡œìš´ ì´ë¯¸ì§€ $\hat{x_i}(t)$ë¥¼ í•©ì„± 
  - **Embedder** : source videoì—ì„œ randomìœ¼ë¡œ ì¶”ì¶œí•œ imageì™€ ê·¸ ì´ë¯¸ì§€ì˜ landmarkë¥¼ í™œìš©í•˜ì—¬ N-dimì˜ vectorë¡œ ì„ë² ë”©. ê° ì´ë¯¸ì§€ì˜ ì„ë² ë”© ê°’ë“¤ê°„ì˜ í‰ê· ì´ $\hat{e_i}$
- **Generator Loss** : content loss + adversarial loss + match loss

$$\mathcal{L}\left(\phi, \psi, \mathbf{P}, \theta, \mathbf{W}, \mathbf{w}_{0}, b\right)=\mathcal{L}_{\mathrm{CNT}}(\phi, \psi, \mathbf{P})+\mathcal{L}_{\mathrm{ADV}}\left(\phi, \psi, \mathbf{P}, \theta, \mathbf{W}, \mathbf{w}_{0}, b\right)+\mathcal{L}_{\mathrm{MCH}}(\phi, \mathbf{W})$$

<span style='background-color: #E5EBF7;'> <b>Discriminator</b> </span>


- **Discriminator** : í•©ì„±ëœ ì´ë¯¸ì§€ $\hat{x_i}(t)$ê°€ landmark $y_i(t)$ë¥¼ ì˜ ë°˜ì˜í•˜ê³  ìˆëŠ”ì§€ íŒë‹¨ (realism score)
- **Discriminator Loss**

$$\mathcal{L}_{\mathrm{DSC}}\left(\phi, \psi, \mathbf{P}, \theta, \mathbf{W}, \mathbf{w}_{0}, b\right)=\max \left(0,1+D\left(\hat{\mathbf{x}}_{i}(t), \mathbf{y}_{i}(t), i ; \phi, \psi, \theta, \mathbf{W}, \mathbf{w}_{0}, b\right)\right)+\max \left(0,1-D\left(\mathbf{x}_{i}(t), \mathbf{y}_{i}(t), i ; \theta, \mathbf{W}, \mathbf{w}_{0}, b\right)\right)$$


---

### 2.2 Few-shot learning by fine-tuning

(1) **Target image â¡ Landmark image** : source videoì˜ $T$ ê°œì˜ frameì— ëŒ€í•˜ì—¬ training imageì™€ landmark imageë¥¼ ëª¨ë‘ êµ¬í•´ì•¼í•¨
   
$$x(1), x(2), ..., x(T)   /   y(1), y(2), ..., y(T)$$
  
(2) meta-learned Embedderë¥¼ í†µí•´ embedding $\hat{\mathbf{e}}_{\mathrm{NEW}}$ ê³„ì‚°
   
$$\hat{\mathbf{e}}_{\mathrm{NEW}}=\frac{1}{T} \sum_{t=1}^{T} E(\mathbf{x}(t), \mathbf{y}(t) ; \phi)$$

ìœ„ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ pretrained modelì„ í™œìš©í•˜ì—¬ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ í•©ì„±. ë‹¤ë§Œ, Fine-tuning ê³¼ì •ì´ í•„ìš”

<span style='background-color: #E5EBF7;'> <b>Generator</b> </span>

- $G\left(\mathbf{y}(t), \hat{\mathbf{e}}_{\mathrm{NEW}} ; \psi, \mathbf{P}\right)$ â¡ $G^{\prime}\left(\mathbf{y}(t) ; \psi, \psi^{\prime}\right)$
- $\psi^{\prime}=\mathbf{P} \hat{\mathbf{e}}_{\mathrm{NEW}}$ 
  
  ìƒˆë¡œ ê³„ì‚°ëœ embedding $\hat{\mathbf{e}}_{\mathrm{NEW}}$ì„ í™œìš©í•˜ì—¬ $\psi^{\prime}$ ë¡œ ì´ˆê¸°í™”

<span style='background-color: #E5EBF7;'> <b>Discriminator</b> </span>

- Fine-Tuning ê³¼ì •ì—ì„œë„ meta-learning stageì™€ ë¹„ìŠ·í•œ ë°©ì‹ìœ¼ë¡œ DëŠ” realism scoreë¥¼ ê³„ì‚°
  
  $$D^{\prime}\left(\hat{\mathbf{x}}(t), \mathbf{y}(t) ; \theta, \mathbf{w}^{\prime}, b\right)= V(\hat{\mathbf{x}}(t), \mathbf{y}(t) ; \theta)^{T} \mathbf{w}^{\prime}+b$$

<span style='background-color: #E5EBF7;'> <b>Loss Function</b> </span>

- **Generator Loss**

$$\mathcal{L}^{\prime}\left(\psi, \psi^{\prime}, \theta, \mathbf{w}^{\prime}, b\right)=\mathcal{L}_{\mathrm{CNT}}^{\prime}\left(\psi, \psi^{\prime}\right)+\mathcal{L}_{\mathrm{ADV}}^{\prime}\left(\psi, \psi^{\prime}, \theta, \mathbf{w}^{\prime}, b\right)$$

- **Discriminator Loss**

$$\mathcal{L}_{\mathrm{DSC}}^{\prime}\left(\psi, \psi^{\prime}, \theta, \mathbf{w}^{\prime}, b\right)=
\max \left(0,1+D\left(\hat{\mathbf{x}}(t), \mathbf{y}(t) ; \psi, \psi^{\prime}, \theta, \mathbf{w}^{\prime}, b\right)\right)+
\max \left(0,1-D\left(\mathbf{x}(t), \mathbf{y}(t) ; \theta, \mathbf{w}^{\prime}, b\right)\right)$$

## 3. Experiments

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/fsth-5.PNG?raw=1' width = '500' ></p>

- Dataset : VoxCeleb1, VoxCeleb2
- Metrics : FID, SSIM. CSIM, USER
- Method : FF(no fine-tune), FT

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/fsth-6.PNG?raw=1' width = '700' ></p>

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/fsth-7.PNG?raw=1' width = '700' ></p>


> ë‹¤ë¥¸ ëª¨ë¸ë“¤ë³´ë‹¤ scoreê°€ ë‚®ê¸°ë„ í•˜ì§€ë§Œ, target ì´ë¯¸ì§€ë¥¼ ê°€ì¥ ì˜ ë°˜ì˜ + userí‰ê°€ë„ ë†’ìŒ

## 4. Conclusions

> âœğŸ» ë³¸ ë…¼ë¬¸ì€ **Deep Generator networkë¡œ ì§„ì§œê°™ì€ talking head ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” ëª¨ë¸**ì„ ì œì•ˆí–ˆë‹¤. í•œì¥ì˜ ì´ë¯¸ì§€ë§Œìœ¼ë¡œë„ ê´œì°®ì€ ì´ë¯¸ì§€ê°€ í•©ì„±ë˜ë©°, few-shot imageë¡œ í•™ìŠµí•˜ë©´ ì§„ì§œê°™ì€ ì´ë¯¸ì§€ê°€ ìƒì„±ëœë‹¤.
> 
> ë‹¤ë§Œ, ì•„ì§ ì‹œì„ ì²˜ë¦¬ì™€ ê°™ì´ mimic representationì€ ì˜ ì•ˆë˜ë©°, landmark adaptationì´ í•„ìš”í•˜ë‹¤ëŠ” í•œê³„ê°€ ìˆë‹¤.

