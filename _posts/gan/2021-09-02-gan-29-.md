---
title: "[Paper Review] Face Vid2Vid: One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing 논문 리뷰"
excerpt: ""


categories:
 - GAN
tags:
  - deeplearning
  - ai
  - GAN
  - vision
  - talking-head
search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---


- Paper : One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing (CVPR 2021) ( [arxiv](https://arxiv.org/abs/2011.15126), [project](https://nvlabs.github.io/face-vid2vid/))
- [GAN-Zoos! (GAN 포스팅 모음집)](https://happy-jihye.github.io/gan/)

---

## Related Work

### 3D model-based talking-head synthesis
**Face reenactment**: 사람간의 변환을 얼마나 잘하는가 (ex. 사람`A`를 사람 `B`로), 보통 두가지의 방식으로 진행됨

1. <b><i>subject-dependent</i> model </b>
  - traditional 3D-based methods 들이 이 방식. 오직 하나의 subject만을 합성할 수 있음
  - head movement없이 표정 변환에만 집중
  - 관련 논문
    - `Face2Face`: Real-time Face Capture and Reenactment of RGB Videos (CVPR 2016): [project](http://niessnerlab.org/projects/thies2016face.html)
    - `Synthesizing Obama`: Learning Lip Sync from Audio (SIGGRAPH 2017): [project](https://grail.cs.washington.edu/projects/AudioToObama/)

2. <b><i>subject-agnostic</i> face synthesis </b>
  - inner face region 쪽은 합성을 잘하지만, realistic hair, teeth, accessories 등의 영역에 대해서는 합성을 잘 못함(아래 관련 논문의 결과들 참고)
  - 관련 논문
    - Text-based Editing of Talking-head Video: [arxiv](https://arxiv.org/abs/1906.01524), [project](https://www.ohadf.com/projects/text-based-editing/)
    - `paGAN`: Real-time Avatars Using Dynamic Textures (SIGGRAPH 2018): [project](https://vgl.ict.usc.edu/Research/pagan/)

### 2D-based talking-head synthesis
1. <b><i>subject-dependent</i> model </b>
  - 특정 target person으로만 바뀌도록 훈련됨
  - 관련 논문
    - `Recycle-GAN`: Unsupervised Video Retargeting (ECCV 2018): [arxiv](https://arxiv.org/abs/1808.05174), [project](https://www.cs.cmu.edu/~aayushb/Recycle-GAN/), [code](https://github.com/aayushbansal/Recycle-GAN)
    - `ReenactGAN`: Learning to Reenact Faces via Boundary Transfer (ECCV 2018): [arxiv](https://arxiv.org/abs/1807.11079), [code](https://github.com/wywu/ReenactGAN)

2. <b><i>subject-agnostic</i> face synthesis </b>
  - training 과정에서 보지 못한 target person을 합성할 수 있음
  - Few-Shot Adversarial Learning of Realistic Neural Talking Head Models (arxiv 2019) : [arxiv](https://arxiv.org/abs/1905.08233), [review](https://happy-jihye.github.io/gan/gan-22/)
  - `FOMM`: First Order Motion Model for Image Animation (NeurIPS 2019) : [arxiv](https://arxiv.org/abs/2003.00196), [code](https://github.com/AliaksandrSiarohin/first-order-model)
  - `Few-shot Vid2Vid`: Few-shot Video-to-Video Synthesis (NeurIPS 2019) : [arxiv](https://arxiv.org/abs/1910.12713), [code](https://github.com/NVlabs/few-shot-vid2vid), [project](https://nvlabs.github.io/few-shot-vid2vid/)
  - `Bi-layer model`: Fast Bi-layer Neural Synthesis of One-Shot Realistic Head Avatars (ECCV 2020): [arxiv](https://arxiv.org/abs/2008.10174), [code](https://github.com/saic-violet/bilayer-model), [project](https://saic-violet.github.io/bilayer-model/)