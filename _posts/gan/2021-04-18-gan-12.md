---
title: "[Paper Review] StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation ë…¼ë¬¸ ë¶„ì„"
excerpt: "í•˜ë‚˜ì˜ Generatorì™€ discriminatorë¡œ ë‹¤ì–‘í•œ datasetì— ëŒ€í•´ image-to-image translationì„ í•˜ëŠ” StarGAN modelì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤."


categories:
 - GAN
tags:
  - deeplearning
  - ai
  - GAN
  - vision
search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 

use_math: true
---

> âœğŸ» ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” í•˜ë‚˜ì˜ Generatorì™€ discriminatorë¡œ ë‹¤ì–‘í•œ datasetì— ëŒ€í•´ image-to-image translationì„ í•˜ëŠ” **starGAN model**ì— ëŒ€í•´ ì‚´í´ë³¸ë‹¤.


- Paper : [StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation](https://arxiv.org/abs/1711.09020) (CVPR 2018) / Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo)

- [GAN-Zoos! (GAN í¬ìŠ¤íŒ… ëª¨ìŒì§‘)](https://happy-jihye.github.io/gan/)

---

> 2ê°œ ì´ìƒì˜ domainì„ ë‹¤ë£¨ëŠ” image-to-image translation ì—°êµ¬ë“¤ì€ **scalabilityì™€ robustness**ì— í•œê³„ê°€ ìˆì—ˆë‹¤. StarGANì€ ì´ë¥¼ ê°œì„ í•˜ì—¬ single networkë¡œ Multi-Domainì— ëŒ€í•´ ì´ë¯¸ì§€ ë³€í™˜ì„ ê°€ëŠ¥í•˜ê²Œ í–ˆë‹¤. StarGANì€ flexibleí•˜ê³  scalableí•˜ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.

## 1. Introduction

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stargan1.PNG?raw=1' width = '800' ></p>

ë³¸ ë…¼ë¬¸ì˜ ëŒ€í‘œì ì¸ taskëŠ” <span style='background-color: #E5EBF7;'> **CelebAì™€ RaFD datasetì„ ì´ìš©í•˜ì—¬ ì–¼êµ´ì˜ íŠ¹ì§•ê³¼ í‘œì •ì„ ë³€í™”** </span>ì‹œí‚¤ëŠ” ê²ƒì´ë‹¤.

**Using Dataset**
- CelebA : 40ê°œì˜ label (ë¨¸ë¦¬ìƒ‰, ì„±ë³„, ë‚˜ì´ ë“±ì˜ facial attributeì™€ ê´€ë ¨ëœ ì •ë³´)
- RaFD : 8ê°œì˜ label(happy, sad, angry ë“±ì˜ facial expressionì™€ ê´€ë ¨ëœ ì •ë³´)

---



**Exsiting Model : Cross-domain model**

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stargan2.PNG?raw=1' width = '450' ></p>
<font color="gray"><i><p align='center' style="font-size:11px"> StarGANì—ì„œëŠ” single generatorë¥¼ ì‚¬ìš©í•´ì„œ ë‹¤ì–‘í•œ domainì„ mappingí•œë‹¤. : â­Star ëª¨ì–‘â­ </p></i></font>

ê¸°ì¡´ì˜ ì—°êµ¬ë“¤ì€ ë‹¤ì–‘í•œ domainì— ëŒ€í•´ image translation í•˜ëŠ” ê²ƒì´ ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ì—ˆë‹¤.

Figure2ì˜ (a)ì²˜ëŸ¼ multi-domainì„ í•™ìŠµí•  ë•Œ, ê°ê°ì˜ generatorë“¤ì€ ì „ì²´ì˜ dataë¥¼ í™œìš©í•˜ì§€ ëª»í•˜ê³  2ê°œì˜ domainë§Œì„ ì‚¬ìš©í•  ìˆ˜ ìˆì—ˆë‹¤. ë˜í•œ, ë‹¤ì–‘í•œ datasetì˜ domainë“¤ì„ ê²°í•©í•˜ì—¬ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒì´ ì–´ë ¤ì› ë‹¤. (`k`ê°œì˜ domainì„ í•™ìŠµì‹œí‚¤ë ¤ë©´ `k(k-1)`ì˜ generatorê°€ í•„ìš”) 

---

StarGANì€ ê¸°ì¡´ ëª¨ë¸ì˜ ë¬¸ì œì (Fixed translation)ì„ ê°œì„ í•˜ì˜€ë‹¤. 

> - StarGANì€ í•˜ë‚˜ì˜ generatorë§Œìœ¼ë¡œë„ ë‹¤ì–‘í•œ domainë“¤ì„ mappping í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ íš¨ìœ¨ì ì¸ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤.
> - ëª¨ë“  domainì˜ ì •ë³´(label)ë“¤ì„ controlí•  ìˆ˜ ìˆë„ë¡ mask vectorë¥¼ ì‚¬ìš©í•œë‹¤.

## 2. Related Work

**Generated Adversarial Networks**

- [[GAN] Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) (2014) : [Review](https://happy-jihye.github.io/gan/gan-1/) 

**Conditionals GANs**

- [[CGAN] Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784) (2014) : [Review](https://happy-jihye.github.io/gan/gan-3/)
- [Generative Adversarial Text to Image Synthesis](https://arxiv.org/abs/1605.05396) (2016) : [Review](https://happy-jihye.github.io/gan/gan-4/)

**Image-to-Image Translation**

- [[Pix2Pix] Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004) (CVPR 2017) : [Review](https://happy-jihye.github.io/gan/gan-8/)
- [[SPADE] Semantic Image Synthesis with Spatially Adaptive Normalization](https://arxiv.org/abs/1903.07291) (CVPR 2019) : [Review](https://happy-jihye.github.io/gan/gan-9/)
- [[CycleGAN] Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593)(ICCV 2017) : [Review](https://happy-jihye.github.io/gan/gan-10/)
- CoGAN
- DiscoGAN

ê¸°ì¡´ì˜ ì—°êµ¬ëŠ” 2ê°€ì§€ì˜ domainì— ëŒ€í•œ ê´€ê³„ë¥¼ ì°¾ì•˜ë‹¤ë©´, starganì—ì„œëŠ” ì´ë¥¼ í™•ì¥í•´ multi-domainì— ëŒ€í•´ì„œë„ ì´ë¯¸ì§€ ë³€í™˜ taskë¥¼ í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.

## 3. Star Generative Adversarial Networks

### 3.1 Multi-Domain Image-to-Image Translation

> â­Our goal is to **train a single generator G that learns mappings among multiple domains.**

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stargan3.PNG?raw=1' width = '800' ></p>

- **Generator**ëŠ” target domain label $c$ì™€ input $x$ì„ ì´ìš©í•´ì„œ output imageë¥¼ ìƒì„±í•œë‹¤.
  $$c, G(x, c) \rightarrow y$$
  - input imageì—ì„œ ë‹¤ë¥¸ imageë¥¼ flexibleí•˜ê²Œ ìƒì„±í•  ìˆ˜ ìˆë„ë¡, target domain label $c$ëŠ” randomìœ¼ë¡œ ìƒì„±í•œë‹¤.



  ```python
  class Generator(nn.Module):
    """Generator network."""
    def __init__(self, conv_dim=64, c_dim=5, repeat_num=6):
        super(Generator, self).__init__()

        layers = []
        layers.append(nn.Conv2d(3+c_dim, conv_dim, kernel_size=7, stride=1, padding=3, bias=False))
        layers.append(nn.InstanceNorm2d(conv_dim, affine=True, track_running_stats=True))
        layers.append(nn.ReLU(inplace=True))

        # Down-sampling layers.
        curr_dim = conv_dim
        for i in range(2):
            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1, bias=False))
            layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True, track_running_stats=True))
            layers.append(nn.ReLU(inplace=True))
            curr_dim = curr_dim * 2

        # Bottleneck layers.
        for i in range(repeat_num):
            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))

        # Up-sampling layers.
        for i in range(2):
            layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=4, stride=2, padding=1, bias=False))
            layers.append(nn.InstanceNorm2d(curr_dim//2, affine=True, track_running_stats=True))
            layers.append(nn.ReLU(inplace=True))
            curr_dim = curr_dim // 2

        layers.append(nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False))
        layers.append(nn.Tanh())
        self.main = nn.Sequential(*layers)

    def forward(self, x, c):
        # Replicate spatially and concatenate domain information.
        # Note that this type of label conditioning does not work at all if we use reflection padding in Conv2d.
        # This is because instance normalization ignores the shifting (or bias) effect.
        c = c.view(c.size(0), c.size(1), 1, 1)
        c = c.repeat(1, 1, x.size(2), x.size(3))
        x = torch.cat([x, c], dim=1)
        return self.main(x)
  ```

- **Discriminator**ëŠ” [ACGANê³¼ ìœ ì‚¬í•˜ê²Œ Auxiliary Classifier](https://happy-jihye.github.io/gan/gan-13/#2-ac-gans-auxiliary-classifier-gan)ì„ ì‚¬ìš©í•œë‹¤.
  - Auxiliary classifierëŠ” í•˜ë‚˜ì˜ discriminatorë¡œ ì—¬ëŸ¬ domainì„ controlí•œë‹¤.
  - DiscriminatorëŠ” `source`ì™€ `domain label`ì— ëŒ€í•œ í™•ë¥ ë¶„í¬ë¥¼ ìƒì„±í•œë‹¤.

    
  ```python
  class Discriminator(nn.Module):
      """Discriminator network with PatchGAN."""
      def __init__(self, image_size=128, conv_dim=64, c_dim=5, repeat_num=6):
          super(Discriminator, self).__init__()
          layers = []
          layers.append(nn.Conv2d(3, conv_dim, kernel_size=4, stride=2, padding=1))
          layers.append(nn.LeakyReLU(0.01))

          curr_dim = conv_dim
          for i in range(1, repeat_num):
              layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1))
              layers.append(nn.LeakyReLU(0.01))
              curr_dim = curr_dim * 2

          kernel_size = int(image_size / np.power(2, repeat_num))
          self.main = nn.Sequential(*layers)
          self.conv1 = nn.Conv2d(curr_dim, 1, kernel_size=3, stride=1, padding=1, bias=False)
          self.conv2 = nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size, bias=False)
          
      def forward(self, x):
          h = self.main(x)
          out_src = self.conv1(h)
          out_cls = self.conv2(h)
          return out_src, out_cls.view(out_cls.size(0), out_cls.size(1))
  ```

---

#### Adversarial Loss

- GëŠ” ì•„ë˜ì˜ object functionì„ minimizeí•˜ëŠ” ë°©í–¥ìœ¼ë¡œ, DëŠ” maximizeí•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•œë‹¤. (1)
  
$$\begin{aligned}
\mathcal{L}_{a d v}=& \mathbb{E}_{x}\left[\log D_{s r c}(x)\right]+\mathbb{E}_{x, c}\left[\log \left(1-D_{s r c}(G(x, c))\right)\right]
\end{aligned}$$

---
#### Domain Classification Loss

StarGANì˜ ëª©í‘œëŠ” target domain label $c$ì— ë”°ë¼ $x$ì—ì„œ $y$ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ìœ„í•´ Dì˜ ìµœìƒë‹¨ì— auxiliary classifierë¥¼ ì¶”ê°€í•˜ì—¬ domain classification lossì— ëŒ€í•´ì„œë„ í•™ìŠµí•˜ë„ë¡ í•˜ì˜€ë‹¤. 
- [[Paper Review] ACGAN: Conditional Image Synthesis with Auxiliary Classifier GANs ê°„ë‹¨í•œ ë…¼ë¬¸ ë¦¬ë·°](https://happy-jihye.github.io/gan/gan-13/)


<span style='background-color: #E5EBF7;'> (1) Domain Classification Loss of **Real image** </span>

- original domain $c'$ì— ë”°ë¼ real image $x$ë¥¼ ë¶„ë¥˜í•˜ë„ë¡ í›ˆë ¨í•œë‹¤. (2)

$$\mathcal{L}_{c l s}^{r}=\mathbb{E}_{x, c^{\prime}}\left[-\log D_{c l s}\left(c^{\prime} \mid x\right)\right]$$


<span style='background-color: #E5EBF7;'> (2) Domain Classification Loss of **Fake image** </span>

- GëŠ” target domain $c$ì— ë”°ë¼ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë„ë¡ ì´ loss functionì„ minimizeí•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í›ˆë ¨í•œë‹¤. (3)

$$\mathcal{L}_{c l s}^{f}=\mathbb{E}_{x, c}\left[-\log D_{c l s}(c \mid G(x, c))\right]$$

(3) Code

```python
  # Original-to-target domain.
  x_fake = self.G(x_real, c_trg)
  out_src, out_cls = self.D(x_fake)
  g_loss_fake = - torch.mean(out_src)
  g_loss_cls = self.classification_loss(out_cls, label_trg, self.dataset)
```

#### Reconstruction Loss

(1), (3)ì˜ ë‘ lossë¥¼ ì‚¬ìš©í•˜ë©´ ê·¸ëŸ´ì‹¸í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ëŠ” ìˆê² ì§€ë§Œ, domainì„ ë³€í™”ì‹œí‚¤ë©´ì„œ attributeë¥¼ ë³€í™”ì‹œí‚¬ ë•Œ input imageì˜ contentê°€ í›¼ì†ë  ìˆ˜ë„ ìˆë‹¤. íŠ¹ì„±ì„ ë³€í™”ì‹œí‚¬ ë•Œ ì›ë³¸ ì´ë¯¸ì§€ì˜ íŠ¹ì„±ì´ ë‚¨ì•„ìˆë„ë¡ [CycleGANì—ì„œ ì‚¬ìš©í•œ cycle-consistency loss](https://happy-jihye.github.io/gan/gan-10/#32-cycle-consistency-loss)ë¥¼ ì‚¬ìš©í•œë‹¤.

<span style='background-color: #E5EBF7;'> **Cycle-Consistency Loss** </span>

$$\mathcal{L}_{r e c}=\mathbb{E}_{x, c, c^{\prime}}\left[\left\|x-G\left(G(x, c), c^{\prime}\right)\right\|_{1}\right]$$

- where G takes in the translated image $G(x, c)$ and the original domain label $c'$ as input and tries to reconstruct the original image $x$
- L1 norm 

```python
x_reconst = self.G(x_fake, c_org)
g_loss_rec = torch.mean(torch.abs(x_real - x_reconst))
```
---

#### Full Objective

$$\begin{array}{c}
\mathcal{L}_{D}=-\mathcal{L}_{a d v}+\lambda_{c l s} \mathcal{L}_{c l s}^{r}, \\
\mathcal{L}_{G}=\mathcal{L}_{a d v}+\lambda_{c l s} \mathcal{L}_{c l s}^{f}+\lambda_{r e c} \mathcal{L}_{r e c}
\end{array}$$

- $\lambda_{c l s}$ì™€ $\lambda_{r e c}$ëŠ” hyper-parameterë¡œ, domain classificationê³¼ reconstruction lossì˜ ìƒëŒ€ì ì¸ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
- ë³¸ ë…¼ë¬¸ì˜ ì‹¤í—˜ì—ì„œëŠ” $\lambda_{c l s} = 1$, $\lambda_{r e c} = 10$ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.
- [ì´ ë¶€ë¶„ ì—­ì‹œ CycleGANê³¼ ìœ ì‚¬](https://happy-jihye.github.io/gan/gan-10/#33-full-object)

```python
# Backward and optimize.
g_loss = g_loss_fake + self.lambda_rec * g_loss_rec + self.lambda_cls * g_loss_cls
```

---

### 3.2 Training with Multiple Datasets

starGANì€ ì„œë¡œ ë‹¤ë¥¸ domainì„ ê°€ì§„ datasetì„ í†µí•©í•  ìˆ˜ ìˆë‹¤.
- ex) CelebAì˜ ë¨¸ë¦¬ìƒ‰ labelì„ RaFD datasetì— ì ìš©í•  ìˆ˜ ìˆìŒ

ê·¸ëŸ¬ë‚˜ ë‹¤ìˆ˜ì˜ datasetì„ í•™ìŠµì‹œí‚¬ ë•Œ, ì›í•˜ëŠ” labelì— ëŒ€í•œ ì •ë³´ëŠ” ì¼ë¶€ datasetì—ë§Œ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— ì‚¬ì§„ì„ ë³µì›í•˜ëŠ” ê³¼ì •ì—ì„œ ë¬¸ì œê°€ ìƒê¸´ë‹¤. 
$$\mathcal{L}_{r e c}=\mathbb{E}_{x, c, c^{\prime}}\left[\left\|x-G\left(G(x, c), c^{\prime}\right)\right\|_{1}\right]$$
ì˜ ì‹ì—ì„œ $G(x, c)$ë¡œë¶€í„° input image $x$ë¥¼ ë³µì›í•˜ë ¤ë©´ $c'$ì˜ label vectorê°€ í•„ìš”í•œë° ì´ labelì´ ì—†ëŠ” ê²ƒì´ë‹¤.

(CelebAì˜ ì–¼êµ´ì„ ì›ƒëŠ” í‘œì •ì„ ë³€í™”ì‹œí‚¨ í›„ ë‹¤ì‹œ ì´ë¥¼ ìŠ¬í”ˆ í‘œì •ìœ¼ë¡œ ë³µì›ì‹œí‚¤ë ¤ê³  í•  ë•Œ, ê¸°ì¡´ì˜ CelebA datasetì€ ë¨¸ë¦¬ìƒ‰, ì£¼ê·¼ê¹¨ ë“±ì˜ labelë§Œ ìˆìœ¼ë¯€ë¡œ ìŠ¬í”ˆì–¼êµ´ $c'$ì— ëŒ€í•´ ë³µì›í•˜ê¸°ê°€ ì–´ë ¤ì›€)

---

#### Mask Vector
ë”°ë¼ì„œ ì´ë¥¼ ìœ„í•´ Mask vector $m$ì„ ë„ì…í•˜ì—¬ ì˜ëª¨ë¥´ëŠ” labelì— ëŒ€í•´ì„œëŠ” ë¬´ì‹œí•˜ë„ë¡ í•˜ì˜€ë‹¤. (one-hot vectorì—ì„œ 0ìœ¼ë¡œ) <span style='background-color: #E5EBF7;'> mask vectorë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµì„ í•˜ë©´ íŠ¹ì • datasetì˜ ì˜ ì•Œë ¤ì§„ labelì— ëŒ€í•´ì„œë§Œ í•™ìŠµì„ í•  ìˆ˜ ìˆë‹¤. </span>

$$\tilde{c}=\left[c_{1}, \ldots, c_{n}, m\right]$$

Ex) CelebAì˜ ì´ë¯¸ì§€ë¥¼ trainingí•  ë•Œ, discriminatorëŠ” celebAì™€ ê´€ë ¨ëœ íŠ¹ì„±ë“¤(ë¨¸ë¦¬ìƒ‰, ì£¼ê·¼ê¹¨ ë“±ë“±)ì— ëŒ€í•œ classification errorë§Œì„ ìµœì†Œí™”í•˜ë„ë¡ í•™ìŠµì„ í•œë‹¤. (RaFDì˜ íŠ¹ì„±-í‘œì •ì— ê´€í•´ì„œëŠ” í•™ìŠµì„ ì•ˆí•¨)

> â­ **Training Strategy**
> 
> DiscriminatorëŠ” CelebAì™€ RaFDë¥¼ ë²ˆê°ˆì•„ê°€ë©° í•™ìŠµì„ í•´ì„œ ë‘ datasetì˜ featureë“¤ì„ ê³¨ê³ ë£¨ í•™ìŠµí•˜ë„ë¡ í•œë‹¤. ë°˜ë©´, GeneratorëŠ” ëª¨ë“  datasetì— ëŒ€í•œ labelì„ ì œì–´í•˜ë„ë¡ í•™ìŠµí•œë‹¤.

## 4. Implementation

### Improved GAN Training

í•™ìŠµì„ ì•ˆì •í™”í•˜ê³ , ë” ì¢‹ì€ qualityì˜ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ gradient penalty($\lambda_{g p}=10$)ì™€ **Wasserstein GANì˜ objective function**ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.

$$\begin{aligned}
\mathcal{L}_{a d v}=& \mathbb{E}_{x}\left[D_{s r c}(x)\right]-\mathbb{E}_{x, c}\left[D_{s r c}(G(x, c))\right] \\
&-\lambda_{g p} \mathbb{E}_{\hat{x}}\left[\left(\left\|\nabla_{\hat{x}} D_{s r c}(\hat{x})\right\|_{2}-1\right)^{2}\right]
\end{aligned}$$

### Network Architecture

CycleGANì˜ architectureë¥¼ baselineìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.
- [[Paper Review] CycleGAN : Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks ë…¼ë¬¸ ë¶„ì„](https://happy-jihye.github.io/gan/gan-10/#1-introduction)

- 2ê°œì˜ convolutional layersë¡œ êµ¬ì„±ëœ generator network 
  - stride size of 2 for downsampling
  - 6 residual blocks 
  - 2 transposed convolutional layers with the stride size of 2 for upsampling. 

- Gë§Œ instance normalization (DëŠ” X)

## 5. Experiment Results

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stargan4.PNG?raw=1' width = '800' ></p>
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stargan6.PNG?raw=1' width = '800' ></p>

ê¸°ì¡´ì˜ **cross-domain model**ë“¤ì€ fixed translationì„ í•˜ê¸° ë•Œë¬¸ì— overfittingì´ ë˜ê¸° ì‰½ë‹¤. ë°˜ë©´, starGANì€ ìœ ì—°í•˜ê²Œ ë³€í™˜ì„ í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë³´ë‹¤ í™”ì§ˆë„ ë” ì¢‹ê³  íŠ¹ì„±ë“¤ì˜ ì ìš©ì´ ì˜ëœë‹¤.


---

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stargan5.PNG?raw=1' width = '450' ></p>

Amazon Mechanical Turk (AMT)ë¥¼ í†µí•´ ì‹¤ì œ userë“¤ì—ê²Œ í‰ê°€ë¥¼ ë°›ì•„ë´¤ëŠ”ë° starGANì´ ì œì¼ ì¢‹ì€ ê²°ê³¼ë¥¼ ë°›ì•˜ë‹¤.

---

### CelebA + RaFD
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stargan7.PNG?raw=1' width = '800' ></p>
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stargan9.PNG?raw=1' width = '700' ></p>

í•˜ë‚˜ì˜ datasetë§Œì„ ì‚¬ìš©í•œ *StarGAN-SNG*ì€ íšŒìƒ‰ ë°°ê²½ê³¼ bluryí•œ ì´ë¯¸ì§€ê°€ ë‚˜íƒ€ë‚˜ì§€ë§Œ, datasetì„ ì„ì€ *StarGAN-JNT*ëŠ” high visual qualityì˜ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì—ˆë‹¤.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stargan8.PNG?raw=1' width = '600' ></p>

mask vectorë¥¼ ì‚¬ìš©í•´ì•¼ ì˜ ì•Œê³ ìˆëŠ” attributeì— ëŒ€í•´ì„œë§Œ í•™ìŠµì´ ë˜ë¯€ë¡œ mask vectorë¥¼ ì‚¬ìš©í•œ ì‚¬ì§„ì´ í€„ë¦¬í‹°ê°€ ê´œì°®ë‹¤.

## 6. Conclusion

> âœğŸ» StarGANì€ í•˜ë‚˜ì˜ Generatorì™€ discriminatorë§Œìœ¼ë¡œë„ ë‹¤ì–‘í•œ datasetì— ëŒ€í•´ image-to-image translationì„ ê°€ëŠ¥í•˜ê²Œ í•´ì£¼ëŠ” íš¨ê³¼ì ì¸ ëª¨ë¸ì´ë‹¤. Scalablityí•˜ë‹¤ëŠ” ì¥ì ì´ ìˆìœ¼ë©°, ê¸°ì¡´ì˜ ëª¨ë¸ì— ë¹„í•´ high visual qualityì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œë‹¤.

---

## 7. Opinions

> ê¸°ì¡´ì— ê³µë¶€í–ˆë˜ ë…¼ë¬¸ë“¤ì˜ ì§‘ì•½ì²´? ê°™ì•˜ë˜ ë…¼ë¬¸ì´ë‹¤. ë‹¤ì–‘í•œ ë„ë©”ì¸ì— ëŒ€í•´ ì–´ë–¤ ì‹ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆì–´ì„œ ì¢‹ì•˜ë‹¤. ì‹¤ì œë¡œ ì½”ë”©ì„ í†µí•´ ì‹¤í—˜ì„ í•´ë´ì•¼ê² ë‹¤ ğŸ˜š

--- 
(2021.04.30 ì¶”ê°€)
- starGAN v1ì˜ ê³µì‹ repoì˜ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµì„ ì‹œì¼œë³´ì•˜ë‹¤.(celebA single dataset)

|                                                              |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stargan10.jpg?raw=1) | ![](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stargan11.jpg?raw=1) |

í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ image-to-image translationì´ ì˜ë¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

