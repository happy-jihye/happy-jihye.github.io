---
title: "[Paper Review] SS-GAN: Self-Supervised GANs via Auxiliary Rotation Loss 간단한 논문 리뷰"
excerpt: " "


categories:
 - GAN
tags:
  - deeplearning
  - ai
  - GAN
  - vision
search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---

- Paper : [SS-GAN: Self-Supervised GANs via Auxiliary Rotation Loss](https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Self-Supervised_GANs_via_Auxiliary_Rotation_Loss_CVPR_2019_paper.pdf) (CVPR 2019)

- [GAN-Zoos! (GAN 포스팅 모음집)](https://happy-jihye.github.io/gan/)

---


- Conditional GAN은 안정적이고 학습이 쉽지만, label이 꼭 필요 

> ⭐ <font color='#2C4D88'><b>Unsupervised Generative Model</b> that combines <b>adversarial training</b> with <b>self-supervised learning</b></font> 
> 
> - SS-GAN : GAN에 self-supervised learning을 거의 처음으로 적용한 논문
> - SS-GAN은 labeled data가 없어도 conditional GAN의 이점을 가짐
> - `D`에 auxiliary, self-supervised loss를 추가하여 학습이 stable + useful 하도록 함.
> - natural image synthesis에서 self-supervised GAN은 label이 없어도 label이 있는 것과 비슷하게 학습이 됨

---

## The Self-Supervised GAN

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ssgan-1.PNG?raw=1' width = '800' ></p>

- The main idea behind **self-supervision** is to train a model on a pretext task like **predicting rotation angle or relativelocation of an image patch, and then extracting representations from the resulting networks**
- 본 논문은 ***SOTA self-supervision method*** 중 하나인 <font color='#2C4D88'><b><i>Image Rotation</i></b></font>를 GAN에 적용
- rotation-based loss로 `D`를 augment !
  
<span style='background-color: #E5EBF7;'> **Loss Function** </span>

- Original GAN Loss
  
$$\begin{equation}
\begin{aligned}
V(G, D)=& \mathbb{E}_{\boldsymbol{x} \sim P_{\text {data }}(\boldsymbol{x})}\left[\log P_{D}(S=1 \mid \boldsymbol{x})\right] \\
&+\mathbb{E}_{\boldsymbol{x} \sim P_{G}(\boldsymbol{x})}\left[\log \left(1-P_{D}(S=0 \mid \boldsymbol{x})\right)\right]
\end{aligned}
\end{equation}$$

- Original GAN Loss + Rotation-based Loss

$$L_{G}=-V(G, D)-\alpha \mathbb{E}_{\boldsymbol{x} \sim P_{G}} \mathbb{E}_{r \sim \mathcal{R}}\left[\log Q_{D}\left(R=r \mid \boldsymbol{x}^{r}\right)\right] \\
L_{D}=V(G, D)-\beta \mathbb{E}_{\boldsymbol{x} \sim P_{\text {data }}} \mathbb{E}_{r \sim \mathcal{R}}\left[\log Q_{D}\left(R=r \mid \boldsymbol{x}^{r}\right)\right]$$

## Experimental Results

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/ssgan-2.PNG?raw=1' width = '800' ></p>

- 학습은 생각보다 잘됨
- Unconditional-GAN보다는 훨씬 결과가 좋고, Conditional-GAN과는 비슷한 결과를 가짐

