---
title: "[Generative Model] Variational Auto-Encoder" 
excerpt: ""

categories:
 - VAE
tags:
  - deeplearning
  - ai
  - pytorch
  - vision
search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 

use_math: true
---

> VAE ìˆ˜ì‹ ì •ë¦¬ ë° ê°œë… ì •ë¦¬ 
> - ëŒ€ì¶© ì´í•´í•œ ë‚´ìš©ë“¤ì„ ì ì–´ë†“ì€ ê¸€ì´ë¼.. ê°€ë…ì„±ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤ ğŸ˜­

- **Scalable** Generative model
    - **Amortized** variational inference
        - ì „í†µì ì¸ variational inference: ê¸°ì¡´ì— í•™ìŠµí–ˆë˜ ë°ì´í„°ë“¤ì˜ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆëŠ”ê²Œ ì•„ë‹ˆë¼, loss function ë‚´ì—ì„œ gradientë¥¼ ì–´ë–¤ ë°©í–¥ìœ¼ë¡œë§Œ ì—…ë°ì´íŠ¸í•˜ë©´ ëê¸° ë•Œë¬¸ì— ë°ì´í„°ê°€ ì–´ë–¤ ê²ƒì´ë“  ìƒê´€ì´ ì—†ì—ˆìŒ
        - stochastic gradientì— ê¸°ë°˜í•œ backpropagationì„ ì´ìš©í•œ ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— sequential í•˜ê²Œ í•™ìŠµì´ ê°€ëŠ¥í•˜ë©°, ê¸°ì¡´ì— í•™ìŠµëœ ëª¨ë¸ì„ ì¶”ê°€ ë°ì´í„°ì…‹ìœ¼ë¡œ weight updateê°€ ê°€ëŠ¥
    - Stochastic gradient descent (Back-Propagation)
        - í° ë°ì´í„°ë„ í•™ìŠµ ê°€ëŠ¥
- **Inference** based on probability graphical model
    - Continuous latent space
    - data reduction / data imputationì— ê°•í•¨
        - ê¸°ì¡´ì˜ auto-encoderëŠ” ë‹¨ìˆœíˆ raw dataì˜ dimensionì„ ê·¸ëŒ€ë¡œ ë„˜ê²¨ì£¼ëŠ” ë°©ì‹ì´ì–´ì„œ ê¸°ì¡´ì˜ ë°ì´í„°ì™€ ì•½ê°„ ë‹¤ë¥¸ í˜•ì‹ì˜ ë°ì´í„°ë¥¼ inputìœ¼ë¡œ ì£¼ë©´ ë¬¸ì œê°€ ìƒê¸°ë‚˜
        - VAEëŠ” í™•ë¥  ìì²´ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ê¸° ë•Œë¬¸ì— noiseê°€ ìˆëŠ” ë°ì´í„°ê°€ ë“¤ì–´ì™€ë„ inferenceê°€ ì˜ë¨

---

# `AE` VS `VAE`

## Auto-Encoder

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/vae/Untitled.png?raw=1' width = '600' ></p>

- **input $x$ ë¥¼ recon**í•˜ë„ë¡ í•˜ëŠ” **neural netì„ í›ˆë ¨**í•˜ëŠ” auto-encoder êµ¬ì¡°
    - bottleneck hidden-layer: latent space `z`, êµ‰ì¥íˆ ì¤‘ìš”í•œ ì •ë³´ë“¤ì„ ê°–ê³  ìˆëŠ” layer
- **architecture**
    
    <p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/vae/Untitled%201.png?raw=1' width = '600' ></p>

    - ì¶œì²˜: ì„œìš¸ëŒ€ ë”¥ëŸ¬ë‹ ê°•ì¢Œ (ìœ¤ì„±ë¡œ êµìˆ˜ë‹˜)
    - stochastic encoder / decoder
        - encoder: x ë¼ëŠ” inputì´ conditioningë˜ì–´ìˆì„ ë•Œ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ latent codeë¥¼ ì¶œë ¥
    - ë³´í†µ dimensionì´ í° input imageì—ì„œ ì¤‘ìš”í•œ featureë“¤ì„ ë½‘ì•„ì„œ latent code zì— ì €ì¥í•˜ê¸° ë•Œë¬¸ì— `undercomplete`ë¬¸ì œë¼ê³  ë³¼ ìˆ˜ ìˆìŒ
    - ë§Œì•½ encoder layer (f)ì™€ decoder layer (g)ê°€ linearí•˜ê³ , lossê°€ mse lossë¼ë©´, hì˜ latent spaceëŠ” PCAì˜ spaceë¼ê³  ë³¼ ìˆ˜ ìˆìŒ
- **training**
    - recon loss
        - mean square loss, cross entropy loss, ...
        
        $$
        L_{A E}=\|x-y\|^{2}
        $$
        
- **applications**
    - dimensionality reduction
    - feature learning
    - generative modelì˜ forefront
- **regularization**
    - AEì—ì„œëŠ” encoder, decoderì˜ capacityê°€ ë„ˆë¬´ë‚˜ë„ í¬ë‹¤ë©´, ì…ë ¥ ì´ë¯¸ì§€ë¥¼ copyí•˜ë„ë¡ ëª¨ë¸ì´ í•™ìŠµë  ìˆ˜ ìˆìŒ â†’ ìœ ì˜ë¯¸í•œ data distributionì„ í•™ìŠµí•˜ì§„ ëª»í•˜ê²Œ ë¨
    - ë”°ë¼ì„œ AEì— ëŒ€í•œ ì •ê·œí™”ëŠ” í•„ìˆ˜ì 
        - ëª¨ë¸ì˜ capacityë¥¼ ì œí•œí•˜ê±°ë‚˜
        - ì›í•˜ëŠ” propertyë¥¼ ë³´ë‹¤ ì˜ í•™ìŠµì‹œí‚¤ë„ë¡ lossë¥¼ ì¤„ ìˆ˜ ìˆìŒ
    - ì •ê·œí™”ë¥¼ ì˜í•˜ë©´..
        - noiseëœ Inputì´ë‚˜ í•™ìŠµ ë•Œ ë³´ì§€ ëª»í•œ inputì„ ì£¼ì–´ë„ robustí•˜ê²Œ ê°’ì„ ë½‘ì•„ë‚¼ ìˆ˜ ìˆê²Œ ë¨
        - sparse representation
        - representationì˜ ë¯¸ë¶„ê°’ì„ ì‘ê²Œ í•  ìˆ˜ ìˆìŒ

### Various AE

**Denoising AE(DAE)**

- inputì— noiseë¥¼ ì„ì–´ì¤€ AE â†’ DecoderëŠ” denoiserì˜ ì—­í• ì„ í•˜ê²Œ ë¨
- noiseëŠ” `gaussian noise`ë¥¼ ì¶”ê°€í•´ì¤˜ë„ ë˜ê³ , ì…ë ¥ì˜ ì¼ë¶€ë¥¼ `dropout`ìœ¼ë¡œ ë‚ ë ¤ì¤˜ë„ ë¨

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/vae/Untitled%202.png?raw=1' width = '600' ></p>

**Sparse AE**

- reconstruction error
- sparsity penalty
    
    $$
    J(\phi, \boldsymbol{\theta})=\underbrace{L(\boldsymbol{x}, \tilde{\boldsymbol{x}})}_{\begin{array}{c}\text { reconstruction } \\\text { loss }\end{array}}+\underbrace{\Omega(\boldsymbol{h})}_{\begin{array}{c}\text { sparsity } \\\text { loss }\end{array}}
    $$
    
    - hidden layerì˜ activation functionì˜ ê²°ê³¼ê°’ì´ ëŒ€ë¶€ë¶„ 0ì´ ë‚˜ì˜¤ë„ë¡ penaltyë¥¼ ê±¸ì–´ì¤Œ
    - hidden layerì—ì„œ 5% ì •ë„ë§Œ í™œì„±í™”ë˜ë„ë¡ ì œì•½ì„ ê±¸ì–´ì¤˜ì„œ ì˜¤í† ì¸ì½”ë”ê°€ 5%ì˜ ë‰´ëŸ°ì˜ ì¡°í•©ì„ ì‚¬ìš©í•´ì„œ inputì„ ì¬êµ¬ì„±í•  ìˆ˜ ìˆê²Œ í•¨
    - ì´ë ‡ê²Œ ë˜ë©´ AEê°€ ë‹¨ìˆœíˆ ì´ë¯¸ì§€ë¥¼ copy & pasteí•˜ëŠ”ê²Œ ì•„ë‹ˆë¼, ê°ê°ì˜ ë‰´ëŸ´ë„·ì´ ì˜ë¯¸ìˆëŠ” ì •ë³´ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë¨
    - [UFLDL Tutorial â€“ 1. ì˜¤í† ì¸ì½”ë”(Sparse Autoencoder) 1 â€“ AutoEncoders & Sparsity](http://solarisailab.com/archives/113)
    

## Variational Auto-Encoder

AEëŠ” latent spaceê°€ discreteí–ˆë‹¤ë©´, VAEëŠ” continuousí•œ í™•ë¥ ë¶„í¬

|||
|--|--|
|<img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/vae/Untitled%203.png?raw=1' width = '600' >|<img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/vae/Untitled%204.png?raw=1' width = '600' >|


> **Auto-Encoder**ê°€ ë‹¨ìˆœíˆ ì´ë¯¸ì§€ì˜ ì°¨ì›ì„ ì¶•ì†Œí•˜ê¸° ìœ„í•´ ë§Œë“  ê²ƒì´ë¼ë©´,(Encoderë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ Decoderë¥¼ í™œìš©)
> 
> **Varational AE**ëŠ” Decoderë¥¼ ìœ„í•´ ê°œë°œëœ ê²ƒìœ¼ë¡œ, input ì´ë¯¸ì§€ê°€ ì£¼ì–´ì§€ë©´ ì´ë¥¼ í™•ë¥  ë¶„í¬ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ìˆë„ë¡ encodingì„ í•œë‹¤. (ë‹¤ì–‘í•œ ì´ë¯¸ì§€ì˜ samplingì´ ê°€ëŠ¥)


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/vae/Untitled%205.png?raw=1' width = '600' ></p>

- ì¶œì²˜: [https://taeu.github.io/paper/deeplearning-paper-vae/](https://taeu.github.io/paper/deeplearning-paper-vae/)

**PipeLine**

- input image $x_i$ ë¥¼ `encoder`ì— ë„£ì–´ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ êµ¬í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ normal distributionì„ ë§Œë“¤ì–´ zë¥¼ sampling
- ì´í›„ ì´ zë¥¼ `decoder`ì— ë„£ì–´ outputì„ ë³µì›
- `Reparameterization Trick`: back-prop ê³¼ì •ì—ì„œ ë¯¸ë¶„ì´ ê°€ëŠ¥í•˜ë„ë¡ trickì„ ì ìš©

**Loss Function**

- **Reconstruction Error**
    - Cross Entropyë¥¼ Loss functionìœ¼ë¡œ ì‚¬ìš© (zë¥¼ decoderì— ë„£ì–´ ì–»ì€ outputì´ bernoulli distributionì„ ë”°ë¥¸ë‹¤ê³  ê°€ì •)
- **Regularization**
    - encoderë¥¼ í†µê³¼í•˜ì—¬ ì–»ì€ zì˜ í™•ë¥ ë¶„í¬ $q(z \mid x)$  ê°€ original distribution $p(z \mid x)$  ì˜ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ë„ë¡ KL-Divergenceë¥¼ ì‚¬ìš©


---

VAEëŠ” ìš°ë¦¬ê°€ ì°¾ê³ ìí•˜ëŠ” dataì˜ distributionì„ ì°¾ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤. ì¦‰,  $p_{model}(x;\theta)$ë¥¼ ìµœëŒ€í™”í•˜ì—¬ ìš°ë¦¬ê°€ ì›í•˜ëŠ” distributionì„ ì°¾ì•„ì•¼í•˜ëŠ”ë°, ì‹¤ì œ ìƒí™©ì—ì„œ ì´ í™•ë¥ ë¶„í¬ëŠ” êµ¬í•˜ê¸° ë§¤ìš° ì–´ë µë‹¤.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/vae/Untitled.jpeg?raw=1' width = '800' ></p>

ë”°ë¼ì„œ ì´ë¥¼ ì§ì ‘ì ìœ¼ë¡œ êµ¬í•˜ì§€ ì•Šê³ , variational approximationì„ í†µí•´ ê°„ì ‘ì ìœ¼ë¡œ êµ¬í•˜ê²Œ ëœë‹¤. (variational boundë¥¼ maximizeí•˜ë„ë¡)

* ìì„¸í•œ ë‚´ìš©ì€ ìœ„ ê·¸ë¦¼ì„ ì°¸ê³ 

**ì°¸ê³ í• ë§Œí•œ ê°œë…**

- **Bayesâ€™ theorem**
    
    <p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/vae/Untitled%206.png?raw=1' width = '350' ></p>
    
    <p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/vae/Untitled%201.jpeg?raw=1' width = '800' ></p>
    
- **Variational Inference**
    
    ë§Œì•½ì— $p(x)$ ë¼ëŠ” target distributionì„ ê·¼ì‚¬í•˜ê³  ì‹¶ì€ë° ì´ í™•ë¥ ë¶„í¬ê°€ complexityê°€ ë†’ê³  intractableí•˜ì—¬ êµ¬í•˜ê¸°ê°€ ì–´ë µë‹¤ë©´, ëŒ€ì‹  ì´ì™€ ë¹„ìŠ·í•˜ì§€ë§Œ simpleí•œ tractable distribution $q(x)$ë¥¼ ì°¾ëŠ” ê²ƒì´ VIì´ë‹¤.
    
    - ì´ë•Œ, ë‘ distributionì´ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ëŠ” KL-Divergenceë¥¼ í†µí•´ ê³„ì‚°í•˜ë©° ì´ ê°’ì„ minimizeí•¨ìœ¼ë¡œì¨ ì›í•˜ëŠ” distribution $q(x)$ë¥¼ ì°¾ëŠ”ë‹¤.
        
        $$
        q^{*}=\operatorname{argmin}_{q \in Q} K L(q \| p)
        $$
        
    - qëŠ” ìš°ë¦¬ê°€ ì„ì˜ë¡œ ë°”ê¿€ ìˆ˜ ìˆëŠ” tractableí•œ í™•ë¥ ë¶„í¬ì´ê¸° ë•Œë¬¸ì—, ì§ì ‘ì ìœ¼ë¡œ pë¥¼ êµ¬í•˜ëŠ” ê²ƒë³´ë‹¤ qì™€ pì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ ì‹ì„ ì „ê°œí•¨ìœ¼ë¡œì¨ pì™€ ë¹„ìŠ·í•œ që¥¼ ì°¾ëŠ”ê²Œ í¸í•˜ë‹¤.
    
    <p align='center'><img src='https://i.imgur.com/05LNq0o.png?raw=1' width = '300' ></p>
    
- ì½ì–´ë³´ë©´ ì¢‹ì€ ê¸€
    - [Entropy & Information Theory](https://hyeongminlee.github.io/post/prob001_information_theory/)
    - [Kullback-Leibler Divergence & Jensen-Shannon Divergence](https://hyeongminlee.github.io/post/prob002_kld_jsd/)
    - [Variational Inference](https://hyeongminlee.github.io/post/bnn003_vi/)
    

### Optimization

**Reparameterization Trick**

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/vae/Untitled%207.png?raw=1' width = '600' ></p>

- back-propagationì´ ê°€ëŠ¥í•˜ë„ë¡ í‘œì¤€ ì •ê·œë¶„í¬ì—ì„œ ë¨¼ì € samplingì„ í•œ í›„, ì‹ì„ ì•½ê°„ ë°”ê¿”ì¤Œ

**Regularization**

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/vae/Untitled%208.png?raw=1' width = '600' ></p>

**Reconstruction Error**

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/vae/Untitled%209.png?raw=1' width = '600' ></p>

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/vae/Untitled%2010.png?raw=1' width = '600' ></p>

---

**Reference**
- [VAEë§ê³  Auto-encoding variational bayesë¥¼ ì•Œì•„ë³´ì](https://www.youtube.com/watch?v=SAfJz_uzaa8&t=1324s)
- [ë”¥ëŸ¬ë‹ Ch3.3 VAE](https://www.youtube.com/watch?v=GbCAwVVKaHY)