---
title: "[Paper Review] MUNIT : Multi-Modal Unsupervised Image-to-Image Translation ê°„ë‹¨í•œ ë…¼ë¬¸ ë¶„ì„"
excerpt: "Unpaired datasetì— ëŒ€í•´ Multimodal Image-to-Image translationë¥¼ í•˜ëŠ” MUNIT modelì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤."


categories:
 - GAN
tags:
  - deeplearning
  - ai
  - pytorch
  - GAN
  - vision
search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 

use_math: true
---

> âœğŸ» ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” Unpaired datasetì— ëŒ€í•´ Multimodal Image-to-Image translationë¥¼ í•˜ëŠ” **MUNIT**ì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤.


- Paper : [[MUNIT] Multi-Modal Unsupervised Image-to-Image Translation](https://arxiv.org/abs/1804.04732) (ECCV 2018 / Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz)

- [GAN-Zoos! (GAN í¬ìŠ¤íŒ… ëª¨ìŒì§‘)](https://happy-jihye.github.io/gan/)

---

## 1. Introduction

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/munit1.PNG?raw=1' width = '750' ></p>

> ê¸°ì¡´ì˜ Multimodal Image-to-Image translationì€ paired datasetì— ëŒ€í•´ í•™ìŠµì„ í–ˆë‹¤ë©´(ex, [BicycleGAN](https://happy-jihye.github.io/gan/gan-11/)), **MUNITì€ Unpaired datasetì— ëŒ€í•´ unsupervised Image-to-Image translation**ì„ ìˆ˜í–‰í•œë‹¤.


ê¸°ì¡´ì˜ ëª¨ë¸ë“¤ì€ Source domainì˜ ì´ë¯¸ì§€ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì´ì— ìƒì‘í•˜ëŠ” target domainì˜ Conditional Distributionì„ deterministic one-to-one mappingë¡œ êµ¬í–ˆê¸° ë•Œë¬¸ì— ë‹¤ì–‘í•œ output ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆë‹¤. <span style='background-color: #E5EBF7;'> ë³¸ ë…¼ë¬¸ì˜ Multimodal Image-to-Image translationëŠ” **Source domainì— ìƒì‘í•˜ëŠ” target domainì˜ Multimodal Conditional Distributionì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ ëª©í‘œ**ì´ë‹¤. </span>

MUNITì˜ í° íë¦„ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
(1) ì´ë¯¸ì§€ë¥¼ ìš°ì„  **content codeì™€ style code**ë¡œ decomposeí•œë‹¤.
  <p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/munit2.PNG?raw=1' width = '500' ></p>

  - <span style='background-color: #E5EBF7;'> **content code** </span>ëŠ” domain-invariant(domain-shared)í•œ ì´ë¯¸ì§€ì˜ attributeì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” codeë‹¤. ëª¨ë“  domainì˜ ì´ë¯¸ì§€ë“¤ì€ content space $\mathcal{C}$ ë¥¼ ê³µìœ í•œë‹¤.
  - <span style='background-color: #E5EBF7;'> **style code** </span>ëŠ” domain-specific í•œ ì´ë¯¸ì§€ì˜ styleì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” codeë¡œ, ê° domain $\mathcal{X_i}$ ë“¤ì˜ style space $\mathcal{S_i}$ ëŠ” ê°ê° ë‹¤ë¥´ë‹¤.

(2) ì´í›„ ì´ë¯¸ì§€ì˜ target domainì—ì„œ samplingëœ random style codeì™€ source domainì˜ content codeë¥¼ ê²°í•©í•˜ì—¬ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œë‹¤.
  <p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/munit3.PNG?raw=1' width = '600' ></p>


> ğŸ˜‰ ì´ ì—°êµ¬ë¥¼ ë°œì „ì‹œí‚¨ ì—°êµ¬ê°€ styleGAN !

## 2. Model
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/munit4.PNG?raw=1' width = '700' ></p>

MUNITì˜ Image-to-Image TranslationëŠ” encoder-decoderë¥¼ í†µí•´ ì´ë£¨ì–´ì§„ë‹¤.
  
ì˜ˆë¥¼ ë“¤ì–´ $x_{1} \in \mathcal{X}_{1}$ ì˜ ì´ë¯¸ì§€ë¥¼ $\mathcal{X}_{2}$ domainìœ¼ë¡œ ë³€í™˜í•˜ê³ ì í•  ë•Œ,
- auto-encoderë¥¼ í†µí•´ ê° ì´ë¯¸ì§€ë¥¼ content code $c_i$ì™€ style code $s_i$ë¡œ ë³€í™˜í•œ í›„
  $$\left(c_{i}, s_{i}\right)=\left(E_{i}^{c}\left(x_{i}\right), E_{i}^{s}\left(x_{i}\right)\right)=E_{i}\left(x_{i}\right)$$

- $G_2$ë¥¼ ì´ìš©í•˜ì—¬ output imageë¥¼ ìƒì„±í•œë‹¤. 
  $$x_{1 \rightarrow 2}=G_{2}\left(c_{1}, s_{2}\right)$$

## 2.1 Loss function 
### 2.1.1 Bidirectional Reconstruction Loss

**(1) Image Reconstruction** : image -> latent -> image

- Figure2ì˜ (a)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/munit6.PNG?raw=1' width = '550' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> (ì¶œì²˜) ë„¤ì´ë²„ AI ë©, ìµœìœ¤ì œ ì—°êµ¬ì›ë‹˜ ë°œí‘œìë£Œ </p></i></font>

$$\mathcal{L}_{\mathrm{recon}}^{x_{1}}=\mathbb{E}_{x_{1} \sim p\left(x_{1}\right)}\left[\left\|G_{1}\left(E_{1}^{c}\left(x_{1}\right), E_{1}^{s}\left(x_{1}\right)\right)-x_{1}\right\|_{1}\right]$$

**(2) Latent Reconstruction** : latent -> image -> latent

- Figure2ì˜ (b)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/munit7.PNG?raw=1' width = '700' ></p>
---
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/munit8.PNG?raw=1' width = '700' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> (ì¶œì²˜) ë„¤ì´ë²„ AI ë©, ìµœìœ¤ì œ ì—°êµ¬ì›ë‹˜ ë°œí‘œìë£Œ </p></i></font>

- **Content Reconstructive Loss**
  - source imageì˜ attributeì„ í•™ìŠµí•˜ê¸° ìœ„í•œ Loss

$$\mathcal{L}_{\text {recon }}^{c_{1}}=\mathbb{E}_{c_{1} \sim p\left(c_{1}\right), s_{2} \sim q\left(s_{2}\right)}\left[\left\|E_{2}^{c}\left(G_{2}\left(c_{1}, s_{2}\right)\right)-c_{1}\right\|_{1}\right]$$

- **Style Reconstructive Loss**
  - imageì˜ styleì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ Loss

$$\mathcal{L}_{\text {recon }}^{s_{2}}=\mathbb{E}_{c_{1} \sim p\left(c_{1}\right), s_{2} \sim q\left(s_{2}\right)}\left[\left\|E_{2}^{s}\left(G_{2}\left(c_{1}, s_{2}\right)\right)-s_{2}\right\|_{1}\right]$$

$$\text { where } q\left(s_{2}\right) \text { is the prior } \mathcal{N}(0, \mathbf{I}), p\left(c_{1}\right) \text { is given by } c_{1}=E_{1}^{c}\left(x_{1}\right) \text { and } x_{1} \sim p\left(x_{1}\right) \text { . }$$

---

### 2.1.2 Adversarial Loss

$$\mathcal{L}_{\mathrm{GAN}}^{x_{2}}=\mathbb{E}_{c_{1} \sim p\left(c_{1}\right), s_{2} \sim q\left(s_{2}\right)}\left[\log \left(1-D_{2}\left(G_{2}\left(c_{1}, s_{2}\right)\right)\right)\right]+\mathbb{E}_{x_{2} \sim p\left(x_{2}\right)}\left[\log D_{2}\left(x_{2}\right)\right]$$

---

### 2.1.3 Total Loss
$$\begin{array}{l}
\min _{E_{1}, E_{2}, G_{1}, G_{2}} \max _{D_{1}, D_{2}} \mathcal{L}\left(E_{1}, E_{2}, G_{1}, G_{2}, D_{1}, D_{2}\right)= \\
\mathcal{L}_{\mathrm{GAN}}^{x_{1}}+\mathcal{L}_{\mathrm{GAN}}^{x_{2}}+\lambda_{x}\left(\mathcal{L}_{\mathrm{recon}}^{x_{1}}+\mathcal{L}_{\mathrm{recon}}^{x_{2}}\right)+ \\
\lambda_{c}\left(\mathcal{L}_{\mathrm{recon}}^{c_{1}}+\mathcal{L}_{\mathrm{recon}}^{c_{2}}\right)+\lambda_{s}\left(\mathcal{L}_{\mathrm{recon}}^{s_{1}}+\mathcal{L}_{\mathrm{recon}}^{s_{2}}\right)
\end{array}$$

## 2.2 Encoder-Decoder
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/munit5.PNG?raw=1' width = '700' ></p>

MUNITì€ auto-encoderì˜ êµ¬ì¡°ë¥¼ ë”°ë¥´ë©°, **content encoder, style encoder, join decoder**, ì„¸ê°€ì§€ì˜ subnetworksë¡œ êµ¬ì„±ë˜ì–´ìˆë‹¤.

### (1) Content Encoder
- ì—¬ëŸ¬ê°œì˜ strided convolution layersë¥¼ í†µí•´ inputì„ downsampling
- residual block
- ëª¨ë“  conv layerì—ëŠ” Instance Normalization(IN)ì„ ì ìš©

### (2) Style Encoder
- ì—¬ëŸ¬ê°œì˜ strided convolution layersë¥¼ í†µí•´ inputì„ downsampling
  - global averge pooling layerë‚˜ FC layerë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ì‚¬ìš©í•´ì„œ spatialí•œ ì •ë³´ë“¤ì„ ìµœëŒ€í•œ ì—†ì• ê³ ì í•¨
- (1)ê³¼ ë‹¬ë¦¬ INì„ ì‚¬ìš©í•˜ë©´ style informationì´ ì‚¬ë¼ì§€ë¯€ë¡œ style encoderì—ì„œëŠ” INì„ ì ìš© X

### (3) Decoder
- AdaIN(Adaptive Instance Normalization) ì‚¬ìš©
$$\operatorname{AdaIN}(z, \gamma, \beta)=\gamma\left(\frac{z-\mu(z)}{\sigma(z)}\right)+\beta$$
- ì—¬ëŸ¬ê°œì˜ upsampling + conv layerë¥¼ ì´ìš©í•˜ì—¬ reconstruction 

## 3. Experiments
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/munit9.PNG?raw=1' width = '700' ></p>
---
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/munit10.PNG?raw=1' width = '700' ></p>
---
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/munit11.PNG?raw=1' width = '700' ></p>

## 4. Opinion
> ğŸ˜Š MUNITì€ Multimodal Image-to-Image Translationì˜ baselineì´ ë˜ëŠ” ë…¼ë¬¸ì´ë‹¤. styleganì„ ë¹„ë¡¯í•œ ë‹¤ì–‘í•œ image-to-image translation ë…¼ë¬¸ë“¤ê³¼ ë¹„ìŠ·í•œ logicì´ ë§ì•„ ì½ê¸° ì‰¬ì› ë˜ ê²ƒ ê°™ë‹¤.