---
title: "[Paper Review] StyleGAN3: Alias-Free Generative Adversarial Networks 논문 리뷰"
excerpt: ""


categories:
 - GAN
tags:
  - deeplearning
  - ai
  - GAN
  - vision
search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---


- Paper : Alias-Free Generative Adversarial Networks (2021) ([arxiv](https://arxiv.org/abs/2106.12423), [code](https://github.com/NVlabs/alias-free-gan), [project](https://nvlabs.github.io/alias-free-gan/))

- 😎 StyleGAN Series
    - [`[Paper Review] StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks 논문 분석`](https://happy-jihye.github.io/gan/gan-6/)
    - [`[Paper Review] StyleGAN2 : Analyzing and Improving the Image Quality of StyleGAN 논문 분석`](https://happy-jihye.github.io/gan/gan-7/)
    - [`[Paper Review] StyleGAN2-ADA #01: Training Generative Adversarial Networks with Limited Data 논문 분석`](https://happy-jihye.github.io/gan/gan-19/)
    - [`[Paper Review] StyleGAN2-ADA #02: Training Generative Adversarial Networks with Limited Data 코드 리뷰`](https://happy-jihye.github.io/gan/gan-20/)

- [GAN-Zoos! (GAN 포스팅 모음집)](https://happy-jihye.github.io/gan/)

---

<!-- <p align='center'>
  <iframe src="https://nvlabs-fi-cdn.nvidia.com/_web/alias-free-gan/videos/video_0_ffhq_cinemagraphs.mp4#t=0.001"
    frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen style="width: 42.5em; height: 25em;"></iframe>
</p> -->

> - **Probloem**: 기존의 `StyleGAN2`은 **texture sticking** 이라는 문제를 가지고 있다. 이미지는 구조적으로 학습되어야 하는데(ex. 턱에 해당하는 위치에 수염이 있어야함), stylegan의 generator는 이미지의 각 특징들을 hierarchical 방식으로 학습하지 않고 고정된 픽셀 단위로 학습을 한다.
>   - interpolation 영상을 보면, StyleGAN2의 결과는 턱 수염이 인물을 따라가지 않고, 픽셀 단위로 고정되어 있는 것을 확인할 수 있음.
> - **Cause**: careless signal processing that causes aliasing in the generator network
> - **Problem Solving**: hiearchical 하게 이미지를 합성할 수 있도록 alias-free 한 network 제안

# 1. Introduction

- **기존의 StyleGAN generator**: coarse, low-resolution feature에서 시작하여 upsampling하고, convolution으로 local하게 mixing하고, non-linear function을 거쳐 detail들을 찾아가는 방식으로 학습
  - coarse feature들이 finer feature의 여부에 대해서는 조절을 하지만, 정확한 위치까지 control하지는 못함
  - 결과적으로, fine detail이 hiearchical하게 학습되는 것이 아니라 pixel coordinate에 고착화된 상태로 학습이 됨
- 이러한 **texture sticking** 문제는 fig1에서 확인할 수 있음
  - latent interpolation을 통해 자연스러운 transformation을 만들었을 때, 이 transformation이 hierarchy하게 조절되는 것이 아니라 각 feature들이 특정 pixel에 고착화되어 있음
- ⭐️ Our goal is an architecture that exhibits **a more natural transformation hierarchy**, where the exact sub-pixel position of each feature is exclusively inherited from the underlying coarse features.


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-1.png?raw=1' width = '700' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> figure1 </p></i></font>

## The cause of the problem

현재 network가 이상적인 hierachical construction을 가지지 못하는 이유는 여러 가지가 있다.
- **image borders**: stylegan은 image border가 spatial information을 줘서 generator가 texture sticking이 되도록 학습이 되고 있음 → (sol) image 를 좀 더 크게 잡은 다음에 나중에 crop하는 식으로 학습 ([`3.4.1`](https://happy-jihye.github.io//gan/gan-33/#341-boundaries-and-upsampling-config-e))
- **per-pixel noise inputs**: stylegan에서는 각 pixel마다 independant한 gaussian noise가 들어감. 우리는 pixel에 들어가는 transform에 따라 이미지가 다르게 생성되기를 원하기 때문에 translation과 independant한 noise를 넣어주면 안됨 → (sol) alias-free gan에서는 이를 제거 ! ([`3.3`](https://happy-jihye.github.io//gan/gan-33/#33-baseline-simplification)) 
- **positional encoding** ([`3.2`](https://happy-jihye.github.io//gan/gan-33/#32-fourier-features)) 
- **aliasing** ⭐️

저자들은 이 중에서 <u>aliasing이 가장 critical한 issue</u>라고 주장한다. network는 aliasing이 조금만 존재해도 이를 증폭하는 경향이 있어서 학습이 진행되면서 scale이 커질 수록 픽셀에 특정 texture가 고착되곤 한다.

> 🧐 Aliasing은 왜 발생하나?
> 1. **non-ideal upsampling filters** (ex. nearest, bilinear, strided conv) 때문에 pixel grid에 희미한 after-image가 생김
> 2. **non-linear한 함수** (ex. ReLU)


또, 저자들은 이러한 aliasing에서 비롯되는 문제가 stylegan 뿐만 아니라 deep learning에서 전반적으로 발생한다고 보고하고 있다.

그렇다면, aliasing은 어떻게 해결할 수 있을까?

이론적으로 aliasing은 [Nyquist-Shannon sampling theorem](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem) 로 해결된다. 저자들은 StyleGAN2의 Generator를 신호론적으로 분석하여 upsampling filter랑 pointwise nonlinearties에서 생기는 aliasing을 해결하고자 하였다.


---

## About translation equivariant in CNN

최근 image classification 연구에서는 CNN이 translation equivariant 하지 못했을 때의 문제점에 대해 많은 연구가 진행되고 있다. (CNN 자체는 equivariance해야지 Gloval average pooling 을 통과했을 때, final representation이 translation invariant하게 됨)

- 참고할만한 글
  - [`CNN과 이미지가 찰떡궁합인 이유`](https://seoilgun.medium.com/cnn%EC%9D%98-stationarity%EC%99%80-locality-610166700979)
  - [`translation invariance 설명 및 정리`](https://ganghee-lee.tistory.com/43)

저자들은 CNN representation이 translation equivariant 하려면, CNN에서 나오는 feature map이 Nyquist frequency 를 넘어서는 빠른 패턴들을 가지면 안된다고 보고한다. 즉, aliasing이 발생하면 안된다.


---

# 2. Equivariance via continuous signal interpretation


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-3.jpg?raw=1' width = '800' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> figure2 </p></i></font>

> 본 논문에서는 discrete domain와 continuous domain 사이를 자유롭게 넘나들 수 있도록 도와주는 operation에 대해 소개한다. (`figure2` 참고)
> - **sampling**: continuous → discrete
> - **interpolation**: discrete → continuous


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-2.jpeg?raw=1' width = '800' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> figure3 </p></i></font>

- **figure3**: discrete → continuous domain 으로의 interpolation 변환 과정에서 aliasing이 안생긴 채 신호를 복원할 수 있도록 도와주는 이론

- **Nyquist-Shannon sampling Theorem** ([`#01`](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=guburi&logNo=221369911121), [`#02`](https://linecard.tistory.com/20))
  - 만약 신호가 대역제한(bandlimited)신호이고, 표본화 주파수가 신호의 대역의 두 배 이상이라면 표본으로부터 연속 시간 기저 대역 신호를 완전히 재구성할 수 있다.
  - 입력 신호의 최고 주파수 $f_{max}$ 의 2배 이상으로 모든 신호들을 균일하게 sampling 한다면, 원래 신호를 완벽하게 복원할 수 있다.
  - `aliasing 현상`: 아날로그 신호를 디지털 신호에 적용할 때, sampling 속도가 $2f_{max}$ 보다 작을 경우 아날로그 입력 신호에서 일부 최고 주파수 성분이 디지털 출력에 올바르게 출력되지 않는다. 따라서 이 디지털 신호를 다시금 아날로그 신호로 변환하고자 할 때, 원래 주파수에 없던 잘못된 주파수 성분이 나타난다. 

- **Whittaker-Shannon interpolation Theorem**
  - sinc interpolation
  - ideal band-limited inteporlation

> 🤔 그렇다면 왜 CNN에서 aliasing이 일어날까?
> - **non-ideal upsampling filter**: generator에서 upsampling을 하는 과정에서 low-path filtering을 하지 않았음. 즉 ideal하지 않은 upsampling filter때문에 원치 않은 high-frequency들이 계속 더해져서 aliasing이 일어나는 것
> - **pointwise application nonlinearities such as ReLU**: 예를 들어 음수일때 0으로 만들어주는 relu가 있으면 갑자기 값이 확 튀게 됨 


**Discrete and continuous representations of network layers**

또한, continuous domain과 discrete domain간의 변환이 자유로우려면 각각의 domain에서 행해지는 operation간의 변환도 자유로워야한다. ( 단, 이때 frequency가 bandlimit을 넘어서면 안됨)

$$\mathbf{f}(z)=\phi_{s^{\prime}} * \mathbf{F}\left(\mathrm{W}_{s} \odot z\right), \quad \mathbf{F}(Z)=\mathrm{W}_{s^{\prime}} \odot \mathbf{f}\left(\phi_{s} * Z\right)$$

- discrete domain
  - practical neural network는 discretely sampled feature map에서 동작
  - discrete feature map에서 convolution, nonlinearity와 같은 operation $F$ 은 다음과 같이 표현됨

$$ Z' = F(Z)$$

- continuous domain

$$ z' = f(z)$$


## 2.1 Equivariant network layers

우리는 2D plane상에서 어떤 operation $f$ 가 특정 transformation $t$ 에 대해 교환 법칙이 성립되면 **equivariant** 하다고 말한다.

$$\mathbf{t} \circ \mathbf{f}=\mathbf{f} \circ \mathbf{t}$$

> ⭐ 본 논문에서는 **2가지 tranformation(translation, rotation)** 과 전형적인 generator network의 4가지 **operations(convolution, upsampling, downsampling, nonlinearity)** 에 대해서 equivariant한지 확인한다. 
> 
> 또한, **aliasing이 없으려면** nyquist sampling을 했을 때 이상한 high frequency가 없어야한다. 즉 low-path filtering이 output까지 유지되고 있는지를 확인해줘야한다.

### 2.1.1 Convolution

<span style='background-color: #E5EBF7;'> <b>discrete domain</b> </span>

우선, discrete domain에서부터 살펴보자. discrete kernel $K$ 에서의 standard convolution은 다음과 같이 표현된다.

$$\mathbf{F}_{\text {conv }}(Z)=K * Z$$

<span style='background-color: #E5EBF7;'> <b>continuous domain</b> </span>

discrete domain에서의 convolution 식을 continuous domain에서의 식으로 변환하면 다음과 같다.

$$\mathbf{f}_{\mathrm{conv}}(z)=\phi_{s} *\left(K *\left(\text { Ш }_{s} \odot z\right)\right)$$

(1) convolution은 commutativity 하므로 $\phi_{s} * K = K *\phi_{s}$ 

$$\phi_{s} *\left(K *\left(\text { Ш }_{s} \odot z\right)\right)=K *\left(\phi_{s} *\left(\text { Ш }_{s} \odot z\right)\right)$$

(2) ideal low path filter를 사용한다면, $z$ 를 dirac comb를 통해 sampling 한 후 ideal interpolation filter로 interpolation 하면 다시 $z$ 가 됨

$$\phi_{s} *(\text { Ш }_{s} \odot z) = z$$

$$\mathbf{f}_{\mathrm{conv}}(z) = K *\left(\phi_{s} *\left(\text { Ш }_{s} \odot z\right)\right)=K * z$$

즉, convolution의 commutativity한 성질 (1) 때문에 **translation equivariance 는 만족**하며, convolution과정에서 새로운 frequency가 추가되거나 하지 않으므로 (2) domain간의 변환에서 추가적으로 **aliasing 역시 생기지 않는다**.


---

### 2.1.2 Upsampling & Downsampling

<span style='background-color: #E5EBF7;'> <b>Continuous domain에서의 Upsampling & Downsampling</b> </span>

사실 continuous domain에서의 upsampling은 아무런 의미가 없다. (이미 infinite domain이니까) 

$$f_{up}(z) = z$$

즉, translation or rotation을 하고 upsampling 한 것과 upsampling 하고 translation or rotation 하는 것이 동일하다 (**equivariance**)

<span style='background-color: #E5EBF7;'> <b>Discrete domain에서의 Upsampling & Downsampling</b> </span>

그러나 discrete한 domain에서의 upsampling filter는 ideal 하지 않기 때문에 upsampling을 하는 과정에서 aliasing이 생기고, 이 때문에 등변성이 사라진다. 따라서 본 논문은 upsampling 과정에서 low path filtering을 하여 upsampling과 downsampling이 이상적으로 동작하는 것처럼 되게 한다. (→ equivariance !)


---

### 2.1.3 Nonlinearity

**continuous domain**에서는 ReLU가 pointwise operation이기 때문에 equivariance가 당연히 성립된다. 그러나 bandlimit constraint는 만족되지 않을 수도 있다.

즉, continuous domain에서 ReLU operation을 하고 나면 output에서 의도치 않은 high-frequncy가 생길 수 있다는 것이다. (aliasing)


논문에서는 이러한 aliasing을 제거하기 위해 non-linearity의 결과값에 low-path filtering을 해준다.

$$\mathbf{f}_{\sigma}(z)=\phi_{s} * \sigma(z) \quad \mathbf{F}_{\sigma}(Z)=\text { Ш }_{s} \odot\left(\phi_{s} * \sigma\left(\phi_{s} * Z\right)\right)$$

> **정리**
> - Generator가 low-resolution에서 시작하여 upsampling을 하면서 detail한 부분(high-frequency)을 만들어나가는데, 
> - 이때 생성되는 high-frequency 영역들을 low-path filter를 통해 cut-off 하면서 
> - high-frequency를 적절하게 학습하도록 함


---

# 3. Practical application to generator network

> **2절**에서는 주요 operation에서 어떤 문제가 생기는지와 그것을 어떻게 해결하는지에 대해 소개하였다. **3절**에서는 실질적인 문제들을 해결하기 위해 어떤 식으로 network를 바꿨는지에 대해 하나씩 설명한다. 

## 3.1 StyleGAN2 

**Discriminator**
  - alias-free-gan에서는 stylegan2의 discriminator 구조를 유지😊


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/stylegan2-5.PNG?raw=1' width = '500' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> figure4: stylegan2 architecture </p></i></font>

**Generator**

1. **mapping Network**: initial, normally distributed latent code $z$ 를 intermediate latent code $w \sim \mathcal{W}$ 로 transform
2. **synthesis network G**: learned constant input `4x4x412` $Z_0$ 에서 N개의 layer를 거쳐 output image $Z_n = G(Z_0;w)$ 를 생성
  - N개의 layer:  consisting of `convolutions, nonlinearities, upsampling, and per-pixel noise`
  - skip connection, mixing regularization, path length regularization 기법들도 도입

> ⭐️ Our goal is **to make every layer of G equivariant w.r.t. the continuous signal**, so that all finer details transform together with the coarser features of a local neighborhood
>
> - 즉, transformation $t$ (translation & rotation)에 대해 equvariant 하도록 synthesis network의 operation $g$ 를 continuous 하게 만들어야함 

Generator의 operation들이 equivariance하게 만드는 것이 이 논문의 핵심이다.

저자들은 각 operation이 얼마나 equivariance 한지 평가할 수 있는 방법도 함께 report 하였다. 
- the peak signal-to-noise ratio (PSNR) in decibels (dB) between two sets of images
- 자세한 내용은 논문의 p5 를 참고

---


<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-4.png?raw=1' width = '800' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> figure5 </p></i></font>


## 3.2 Fourier features 

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-6.png?raw=1' width = '800' ></p>

> 이미지를 continuous하게 transformation(t&r) 하기 위해 **learned constant input을 Fourier fearture로 변경**하였다.


StyleGAN3에서는 `fourier-feature-networks`의 아이디어를 차용하여 <u>learned constant input을 Fourier fearture로 대체</u>하였다. 어떤 이점을 얻기 위해 Fourier Feature를 사용하였는지를 분석하고자 다음 논문을 간단하게 요약하였다.

- Paper: Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains (NeurIPS2020): [project](https://bmild.github.io/fourfeat/)
  - **Fourier featuring**란, coordinate space point를 frequency space로 embedding하는 function의 총칭이다. 
  - *transformer 계열의 모델들*에서는 feature의 위치 정보를 포함시키기 위해 sinusoidal function(fourier featuring function)을 사용하여 coordinate space에서 frequency space로의 embedding을 진행하는데, 이 역시 일종의 Positional Encoding이다 ([참고 링크](https://happy-jihye.github.io/nlp/nlp-8/#positional-encoding))
  - 즉, Fourier Featuring을 통해 frequency domain으로 embedding하는 것 자체가 **Positional Encoding**의 효과를 준다.
  - Fourier-Feature mapping function $\gamma$
$$\gamma(v)=\left[a_{1} \cos \left(2 \pi b_{1}^{T} v\right), a_{1} \sin \left(2 \pi b_{1}^{T} v\right), \ldots ., a_{m} \cos \left(2 \pi b_{m}^{T} v\right), a_{m} \sin \left(2 \pi b_{m}^{T} v\right)\right]^{T}$$

- Transformer등의 attention-based architecture들은 PE function으로 $\gamma$ 를 사용하며, 다음과 같이 정의된다.

$$a_{i}=1, b_{m}=10000^{m / d}, d: \text { dimension }$$

> 즉, stylegan3는 **Fourier Feature** (in continous한 frequency domain)으로 input을 변경하여 **infinit domain으로 확장**하였고, 동시에 **Postional Encoding**도 할 수 있도록 하였다. 

- stylegan2의 constant learned input과 마찬가지로 학습과정에서는 이 입력값은 고정된다
- Fourier Feature를 도입하면 FID가 약간 개선됨: 5.14 ➡ 4.79 (`figure5 (논문의 figure3)`)
- 또한, transformation(t&r) 가 가능해지는데, 대신 FID는 매우 떨어진다. (`figure6`)
- fourier feature를 사용함으로써 equivariance를 쉽게 측정할 수 있다. (`figure5`에서의 EQ-T, EQ-R)

- ✍🏻 참고할만한 링크 
  - [Position Encoding의 종류와 분석](https://blog-deepest.medium.com/position-encoding%EC%9D%98-%EC%A2%85%EB%A5%98%EC%99%80-%EB%B6%84%EC%84%9D-ab1816b0f62a)
  - [gjghks950.log/Fourier Features Let Networks Learn High-Frequency Functions in Low Dimensional Domains Review](https://velog.io/@gjghks950/Fourier-Features-Let-Networks-Learn-High-Frequency-Functions-in-Low-Dimensional-Domains-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0)

**Code**

`Fourier features`는 stylegan3 의 코드에서 [`SyntehsisInput`](https://github.com/NVlabs/stylegan3/blob/b1a62b91b18824cf58b533f75f660b073799595d/training/networks_stylegan3.py#L169)에 구현되어있다.
- `SyntehsisInput` block
  1. intermediate latent code $w$ 를 input으로 받아 affine 변환을 한 후, 
  2. 이 값을 learned transformation: (1) 먼저 image를 rotation한 후 (2) translation (3) 마지막으로는 user-specified transform
  3. sampling grid를 만들어서 fourier feature로 변환

```python
# Compute Fourier features.
x = (grids.unsqueeze(3) @ freqs.permute(0, 2, 1).unsqueeze(1).unsqueeze(2)).squeeze(3) # [batch, height, width, channel]
x = x + phases.unsqueeze(1).unsqueeze(2)
x = torch.sin(x * (np.pi * 2))
x = x * amplitudes.unsqueeze(1).unsqueeze(2)
```

---

### Transformed Fourier Features(config H)

- 논문의 3.2 절에 해당

StyleGAN3 Generator의 layer들은 equivariant하기 때문에 unaligned dataset이나 임의로 변형시킨 dataset에 대해서도 잘 학습이 된다. (만약 intermediate feature $z_i$를 변형시키면 final image $z_N$ 도 변형되어 생성)

그러나 layer 자체에서 global하게 transformation 하기에는 layer의 capability가 작다. 따라서 Input Fourier Features 자체를 변형시키는 방식으로 생성되는 이미지도 transformation되도록 한다.
- learned affine layer를 통해 input Fourier Features 가 global translation or rotation 되도록 만듦
- `SyntehsisInput` code의 이 부분
  
```python
# Apply learned transformation.
t = self.affine(w) # t = (r_c, r_s, t_x, t_y)
t = t / t[:, :2].norm(dim=1, keepdim=True) # t' = (r'_c, r'_s, t'_x, t'_y)
m_r = torch.eye(3, device=w.device).unsqueeze(0).repeat([w.shape[0], 1, 1]) # Inverse rotation wrt. resulting image.
m_r[:, 0, 0] = t[:, 0]  # r'_c
m_r[:, 0, 1] = -t[:, 1] # r'_s
m_r[:, 1, 0] = t[:, 1]  # r'_s
m_r[:, 1, 1] = t[:, 0]  # r'_c
m_t = torch.eye(3, device=w.device).unsqueeze(0).repeat([w.shape[0], 1, 1]) # Inverse translation wrt. resulting image.
m_t[:, 0, 2] = -t[:, 2] # t'_x
m_t[:, 1, 2] = -t[:, 3] # t'_y
transforms = m_r @ m_t @ transforms # First rotate resulting image, then translate, and finally apply user-specified transform.

# Transform frequencies.
phases = phases + (freqs @ transforms[:, :2, 2:]).squeeze(2)
freqs = freqs @ transforms[:, :2, :2]
```

**Transformation Result**

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-5.png?raw=1' width = '800' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> figure6 </p></i></font>

---
<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-4.png?raw=1' width = '800' ></p>
<font color='gray'><i><p align='center' style='font-size:9px'> figure5 </p></i></font>

## 3.3 Baseline Simplification

> (1) **per-pixel noise inputs**를 제거하였다.

- stylegan2에 삽입되는 per-pixel noise는 이미지의 세부적인 요소들을 독립적이게 학습하도록 만들기 때문에, 이미지가 hierarchical하게 학습되지 못한다.
- noise를 제거하면, `figure5 (논문의 figure3)`를 보면 FID가 그닥 개선되지는 않지만 훨씬 equivariance 해진다.

> (2) `StyleGAN2-ADA`에서 처럼 **the mapping network depth** 를 줄임
>
> (3) disable **mixing regularization and path length regularization**
>
> (4) **output skip connections** 제거

- FID score를 높이기 위해 2,3,4를 했었지만, 모델을 단순화하기 위해 FID는 약간 포기하고 2,3,4를 제거하였다.


## 3.4 Step-by-step redesign motivated by continuous interpretation

### 3.4.1 Boundaries and Upsampling (config E)

> **Boundaries**: 본 논문에서는 feature map을 무한한 공간으로 확장했다고 가정한다. 따라서 target canvas에 어느정도의 margin을 준 후, high-layer로 갈수록 이 확장된 canvas를 crop 하였다.

- border padding이 내부 이미지의 coordinate의 값을 어느정도 갖고 있기 때문에 이렇게 extension하는 과정 필요하다.
- 실험 결과, 10-pixel margin 정도면 충분하여 이를 사용했다 한다.

> **Upsampling**: 기존의 bilinear 2X upsampling filter를 windowed sinc filter로 대체하여 low-pass filtering도 함께 하도록 하였다.

- 참고✍🏻: [windowed sinc filter](https://www.analog.com/media/en/technical-documentation/dsp-book/dsp_book_Ch16.pdf) 
- $n=6$의 large Kaiser window : upsampling의 과정에서 output pixel은 6개의 input pixel에만 영향을 받고, downsampling의 과정에서 input pixel은 6개의 output pixel에만 영향을 줌
- `figure5 (논문의 figure3)`: resampling filter를 작게 설정하면($n=4$) translation equivariance가 안좋아지고, 이를 크게 설정하면($n=8$) training 속도가 느려짐

### 3.4.2 Filtered nonlinearities (config F)

[`2.1.3`](https://happy-jihye.github.io/gan/gan-33/#213-nonlinearity) 에서 ReLU가 당연히 equivariance는 만족하지만, bandlimit를 지키지 않으면 aliasing이 생길 수도 있다고 보고하였다. 따라서 non-linearity function을 지날 때 low-path filtering을 꼭 해야한다.

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-7.png?raw=1' width = '800' ></p>

- 저자들은 upsample-leaky ReLU-downsample의 sequence가 CUDA kernel에서 효과적으로 연산되도록 최적화를 했다고 한다. (10배 빨라짐 + memory saving)
- upsampling + downsampling 정도는 실험결과 $m=2$ 면 충분하다고 한다.

### 3.4.3 Non-critical sampling (config G)  & Flexible layer (config T)

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-8.png?raw=1' width = '500' ></p>

<span style='background-color: #E5EBF7;'> <b>Non-critical sampling (config G)</b> </span>

aliasing은 generator의 equivariance를 망치는 원인이기도 하다. 따라서 각각의 layer를 지날 때 aliasing이 생기지 않도록 해야한다.

- **config G**에서는 저해상도의 layer에서 aliasing이 안생기도록 cutoff frequency를 $f_{c}=s / 2-f_{h}$ 로 낮춤 !

<span style='background-color: #E5EBF7;'> <b>Flexible layer (config T)</b> </span>

이렇게 aliasing을 없애는 것은 중요하다. 그러나 이미지를 학습할 때 상위 layer로 갈수록 detail을 학습하는 것도 중요하다. 즉, high-frequency도 적절히 학습을 해야하는데, low-path filtering을 너무 강하게 걸어주다보면 aliasing은 안생기겠지만 high-frequency (detail)가 학습되지 못한다. 

- 따라서 **config T**에서는 layer를 flexible하게 조절한다. 

> 정리하자면, 
> - 저해상도의 layer에서는 aliasing이 안생기도록 **lower cutoff frequency를 통해 low-path filtering을 강하게 걸어주고**
> - 고해상도의 layer에서는 이미지의 detail을 학습하는게 중요하므로 **flexible하게 조절하여 high-frequency를 학습**하도록 한다.


### 3.4.4 Rotation equivariance (config R)

network를 rotation equivariant하게 변형하고자 할때에는 2가지를 변경한다.

1. `3x3 conv`를 `1x1 conv` 로 변경. 대신 feature map의 수를 2배로 늘린다
2. sinc-based downsampling filter를 radially symmetric jinc-based filter로 변경
  - 학습과정에서 trainable parameter가 56% 줄어드는 효과
  - FID는 비슷하며 EQ-R은 약간 향상됨


# 4. Results

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-9.png?raw=1' width = '800' ></p>

- 실험결과는 표 참고
- FID도 괜찮으며 equivarince도 좋다고 보고
- training 속도 개선 + 연산 최적화 진행
- unaligned image에 대해 실험했다는 점이 인상깊었다. unaligned image에 대해 projection 해봐야지 😉

**Internal representations**

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/alias-free-gan-10.png?raw=1' width = '800' ></p>


---

# 마치며..

> 오래전부터 읽기 시작한 논문이었는데, background를 하나씩 채우면서 읽다보니 완독하는데 시간이 걸린 논문이다 😂 
> 그만큼 신호처리에 대한 배경지식이 많이 필요했던 논문이라 읽기 어려웠다.
>
> 모델 자체의 architecture가 혁신적으로 바뀐 건 없지만, stylegan2가 가지고 있었던 다양한 문제들을 여러 논문의 모델들의 아이디어를 통해 풀어나간점이 흥미로웠다. 또, NVIDIA가 슬슬 image를 넘어 video나 animation을 위한 모델을 만드려고 시도하는 것 같다는 인상을 받았다.
>
> 여러모로 흥미로웠던 논문이다. official 코드가 공개된 만큼 다양하게 실험을 해봐야겠다 😊

---

# Reference

- [gjghks950.log/Fourier Features Let Networks Learn High-Frequency Functions in Low Dimensional Domains Review](https://velog.io/@gjghks950/Fourier-Features-Let-Networks-Learn-High-Frequency-Functions-in-Low-Dimensional-Domains-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0)