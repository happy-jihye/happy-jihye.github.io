---
title: "ë”¥ëŸ¬ë‹ ëª¨ë¸ ë°°í¬í•˜ê¸° #02 - TorchScript & Pytorch JIT"
excerpt: "AI Researcher ê´€ì ì—ì„œ ëª¨ë¸ ë°°í¬ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤."

categories:
 - DL
tags:
  - deeplearning
  - python  
  - ai
  - pytorch
search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 
---

> ë”¥ëŸ¬ë‹ ëª¨ë¸ ë°°í¬í•˜ê¸° ì‹œë¦¬ì¦ˆ 2í¸ì…ë‹ˆë‹¤ :)
> 
>  [1í¸ (`MLOps PipeLineê³¼ ì—°ì‚° ìµœì í™” / ëª¨ë¸ ê²½ëŸ‰í™”`)](https://happy-jihye.github.io/dl/torch-1/)ì„ ë¨¼ì € ì½ìœ¼ì‹œê¸¸ ê¶Œì¥í•©ë‹ˆë‹¤ ğŸ˜Š



## TorchScript & Pytorch JIT

ì§€ë‚œ í¬ìŠ¤íŒ…ì—ì„œ ê°„ë‹¨í•˜ê²Œ Pytorchì™€ Tensorflowì— ëŒ€í•´ ì„¤ëª…í•˜ì˜€ë‹¤. [`link`](https://happy-jihye.github.io/dl/torch-1/#framework) 

Production ë¶„ì•¼ì—ì„œ PytorchëŠ” Tensorflowì— ë¹„í•´ ì•½ì„¸ë¥¼ ë„ê³  ìˆëŠ”ë°, Facebook(Meta)ê°€ tensorflowë¥¼ ë”°ë¼ì¡ê³ ì ë‚´ë†“ì€ ê²ƒì´ **TorchScript**ì™€ **Pytorch JIT**ì´ë‹¤. ì´ë“¤ì€ Pytorch modelì„ ìµœì í™”í•˜ì—¬ model servingì„ ë³´ë‹¤ ì˜í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤€ë‹¤.

ë³´í†µ ëª¨ë¸ì„ productioní™” í•˜ë ¤ë©´, ë‘ê°€ì§€ê°€ í•„ìš”í•˜ë‹¤. 

1. **Portability**
    - ëª¨ë¸ì´ ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ export ë  ìˆ˜ ìˆì–´ì•¼ í•¨
    - Python interpreter process ì—ì„œë¿ë§Œì´ ì•„ë‹ˆë¼ C++ serverë‚˜ mobile /embedded device ì—ì„œë„ ì‘ë™ì´ ê°€ëŠ¥í•´ì•¼í•¨

2. **Performance**
    - inference latencyì™€ throughput, ëª¨ë‘ì˜ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œë„ ìµœì í™”ë¥¼ í•´ì•¼í•¨

Pytorch ëŠ” Pythonì˜ íŠ¹ì§•ì„ ë§ì´ ê°€ì§€ê³  ìˆëŠ” í”„ë ˆì„ì›Œí¬ì´ë‹¤. ë•Œë¬¸ì— Portabilityì™€ Performance, ì´ ë‘ê°€ì§€ ì¸¡ë©´ì—ì„œ ì•½ì„¸ë¥¼ ë³´ì˜€ê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ <u>TorchscriptëŠ” ì½”ë“œë¥¼ Eager modeì—ì„œ Script modeë¡œ ë³€í™˜</u>í•œë‹¤.

## Tools to Transition from Eager to Script

<p align='center'><img src='https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/python/ml-model-serving-3.png?raw=1' width = '700' ></p>

- **Eager Mode**: normal python runtime modeë¡œ `prototyping`, `training`, `experimenting`ì„ ìœ„í•´ ì‚¬ìš©ëœë‹¤
- **Script Mode**
    - production deploymentë¥¼ ìœ„í•´ ë³€í™˜í•œ ëª¨ë“œ
    - runtime ê³¼ì •ì—ì„œ Python Interpreterë¡œ ì‹¤í–‰ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë³‘ë ¬ ì—°ì‚°, ìµœì í™” ë“±ì´ ê°€ëŠ¥í•´ì§„ë‹¤.

ê·¸ë ‡ë‹¤ë©´ Eager modeì—ì„œ Script modeë¡œ ì–´ë–»ê²Œ ë³€í™˜í• ê¹Œ?

Pytorch modelì„ TorchScriptë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì—ëŠ” ë‘ê°€ì§€ ë°©ë²•ì´ ìˆë‹¤. <span style='background-color: #E5EBF7;'> **(1) Tracing ë°©ì‹**ê³¼ **(2) Annotation ë°©ì‹**ì´ë‹¤. </span>

### Tracing

> Eager To Script Mode with `torch.jit.trace()`

Tracing ë°©ë²•ì€ <u>ì–´ë–¤ ì…ë ¥ê°’(data instance)ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ê³ , ì´ ì…ë ¥ê°’ì˜ ëª¨ë¸ ì•ˆì—ì„œì˜ íë¦„ì„ í†µí•´ ëª¨ë¸ì„ ê¸°ë¡í•˜ëŠ” ë°©ì‹</u>ì´ë‹¤. ì¡°ê±´ë¬¸ì„ ë§ì´ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì˜ ê²½ìš° ì´ ë°©ì‹ì„ ì´ìš©í•˜ì—¬ ë³€í™˜í•˜ëŠ” ê²ƒì´ ì í•©í•˜ë‹¤.

ë³´í†µ Pytorch ëª¨ë¸ì„ Tracingì„ í†µí•´ Torchscriptë¡œ ë³€í™˜í•˜ë ¤ë©´, ëª¨ë¸ì˜ instanceë¥¼ ì˜ˆì‹œ inputê°’ê³¼ í•¨ê»˜ `torch.jit.trace` í•¨ìˆ˜ì— ë„˜ê²¨ì£¼ì–´ì•¼í•œë‹¤. 

```python
import torch
import torchvision

# An instance of your model.
model = torchvision.models.resnet18()

# An example input you would normally provide to your model's forward() method.
example = torch.rand(1, 3, 224, 224)

# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.
traced_script_module = torch.jit.trace(model, example)
```

traceëœ `ScriptModule`ì€ ì¼ë°˜ì ì¸ Pytorch moduleê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì…ë ¥ê°’ì„ ë°›ì•„ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.

```python
In[1]: output = traced_script_module(torch.ones(1, 3, 224, 224))
In[2]: output[0, :5]
Out[2]: tensor([-0.2698, -0.0381,  0.4023, -0.3010, -0.0448], grad_fn=<SliceBackward>)
```

Tracing ë°©ì‹ì€ eager modelì˜ ì½”ë“œë¥¼ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íš¨ê³¼ì ì¸ ë°©ë²•ì´ë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ë°©ì‹ì„ ì‚¬ìš©í•˜ë©´ Control-flowë‚˜ data structure, python constructê°€ ë³´ì¡´ë˜ì§€ ì•ŠëŠ”ë‹¤. ë”°ë¼ì„œ ì´ ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ëŠ” í•­ìƒ IRì„ ê²€ì‚¬í•˜ì—¬ pytorch modelì´ ì˜¬ë°”ë¥´ê²Œ ë™ì‘í•˜ëŠ”ì§€ë¥¼ í™•ì¸í•´ì¤˜ì•¼í•œë‹¤.

ì´ëŸ¬í•œ limitationì„ í•´ê²°í•˜ê¸° ìœ„í•´ Annotation ë°©ì‹ì´ ê³ ì•ˆë˜ì—ˆë‹¤.

### Annotation

> Eager To Script Mode with `torch.jit.script()`

ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì€ pytorch modelì´ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì. 
```python
import torch

class MyModule(torch.nn.Module):
    def __init__(self, N, M):
        super(MyModule, self).__init__()
        self.weight = torch.nn.Parameter(torch.rand(N, M))

    def forward(self, input):
        if input.sum() > 0:
          output = self.weight.mv(input)
        else:
          output = self.weight + input
        return output
```

`MyModule` modelì€ inputê°’ì— ë”°ë¼ ì˜í–¥ì„ ë°›ëŠ” Control-flow ë¥¼ ì‚¬ìš©í•˜ê³  ìˆê¸° ë•Œë¬¸ì— tracing ê¸°ë²•ì€ ì í•©í•˜ì§€ ì•Šë‹¤. ëŒ€ì‹  `torch.jit.script()`í•¨ìˆ˜ë¥¼ í†µí•´ ëª¨ë“ˆì„ compileí•˜ì—¬ `ScriptModule`ë¡œ ë³€í™˜í•œë‹¤.

ë˜í•œ, ì´ ë°©ì‹ì€ tracing modeì™€ ë‹¤ë¥´ê²Œ data sampleì€ ì „ë‹¬í•  í•„ìš”ê°€ ì—†ë‹¤. ì˜¤ì§ modelì˜ instanceë§Œ inputìœ¼ë¡œ ë„£ì–´ì£¼ë©´ ëœë‹¤.

```python
my_module = MyModule(10,20)
sm = torch.jit.script(my_module)
```

---

### Mixing Tracing and Scripting

ë‹¤ìŒê³¼ ê°™ì´ Tracingê³¼ Scriptingì„ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤.

**Example (calling a traced function in script)**
```python
import torch

def foo(x, y):
    return 2 * x + y

traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))

@torch.jit.script
def bar(x):
    return traced_foo(x, x)
```

**Example (calling a script function in a traced function)**
```python
import torch

def foo(x, y):
    return 2 * x + y

traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))

@torch.jit.script
def bar(x):
    return traced_foo(x, x)
```

> ì°¸ê³ í•  ë§Œí•œ ìë£Œ âœğŸ»
> - [PyTorch JIT and TorchScript](https://towardsdatascience.com/pytorch-jit-and-torchscript-c2a77bac0fff)
> - [TorchScript and PyTorch JIT / Deep Dive](https://www.youtube.com/watch?v=2awmrMRf0dA)
> - [LOADING A TORCHSCRIPT MODEL IN C++](https://pytorch.org/tutorials/advanced/cpp_export.html)

---


Pytorch codeë¥¼ torchscriptë¡œ ë³€í™˜í•˜ëŠ” ê±´ ê°„ë‹¨í•˜ë‹¤.

íŠ¹íˆ `huggingface`ì—ì„œ ì œê³µí•˜ëŠ” ë§ì€ ëª¨ë¸ë“¤ì€ pretrained modelì„ ë¶ˆëŸ¬ì˜¤ëŠ” ê³¼ì •ì—ì„œ script modeë¥¼ í•¨ê»˜ ì§€ì›í•˜ê¸° ë•Œë¬¸ì— ë”ìš±ë” ê°„ë‹¨í•˜ë‹¤.
- [Exporting transformers models (torchscripts)](https://huggingface.co/transformers/serialization.html#torchscript)

## Example 1: BERT

**Part 1**

- Initializes `BERT Tokenizer` & creates sample data

```python

from transformers import BertTokenizer, BertModel
import numpy as np
import torch
from time import perf_counter

def timer(f,*args):   
    
    start = perf_counter()
    f(*args)
    return (1000 * (perf_counter() - start))
    
script_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', torchscript=True)

# Tokenizing input text
text = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"
tokenized_text = script_tokenizer.tokenize(text)

# Masking one of the input tokens
masked_index = 8
tokenized_text[masked_index] = '[MASK]'
indexed_tokens = script_tokenizer.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# Creating a dummy input
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])

```

**Part 2**

- 2.1 Normal Pytorch Model

```python

# Example 1.1 BERT on CPU
native_model = BertModel.from_pretrained("bert-base-uncased")
np.mean([timer(native_model,tokens_tensor,segments_tensors) for _ in range(100)])

# Example 1.2 BERT on GPU
# Both sample data model need be on the GPU device for the inference to take place
native_gpu = native_model.cuda()
tokens_tensor_gpu = tokens_tensor.cuda()
segments_tensors_gpu = segments_tensors.cuda()
np.mean([timer(native_gpu,tokens_tensor_gpu,segments_tensors_gpu) for _ in range(100)])
```

- 2.1 TorchScript Model
    - `torchscript=True`

```python
script_model = BertModel.from_pretrained("bert-base-uncased", torchscript=True)

# Example 2.1 torch.jit.trace on CPU
traced_model = torch.jit.trace(script_model, [tokens_tensor, segments_tensors])
np.mean([timer(traced_model,tokens_tensor,segments_tensors) for _ in range(100)])

# Example 2.2 torch.jit.trace on GPU
traced_model_gpu = torch.jit.trace(script_model.cuda(), [tokens_tensor.cuda(), segments_tensors.cuda()])
np.mean([timer(traced_model_gpu,tokens_tensor.cuda(),segments_tensors.cuda()) for _ in range(100)])
```

- Runtime ì‹œê°„ ë¹„êµ

<p align='center'><img src='https://miro.medium.com/max/475/1*lrRh2DAzedekbvVXb5p8RA.png?raw=1' width = '500' ></p>

TorchScriptë¡œ Pytorch modelì„ ë³€í™˜í•˜ë©´, Pytorch JITì— ì˜í•´ ë¹ ë¥´ê²Œ Compile ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì½”ë“œë¥¼ ì‹¤í–‰ì‹œí‚¤ëŠ” ê²ƒì´ í›¨ì”¬ ë¹¨ë¼ì§„ë‹¤.

---

## Example 2: ResNet

```python
import torchvision
import torch
from time import perf_counter
import numpy as np

def timer(f,*args):   
    start = perf_counter()
    f(*args)
    return (1000 * (perf_counter() - start))
  
# Example 1.1 Pytorch cpu version

model_ft = torchvision.models.resnet18(pretrained=True)
model_ft.eval()
x_ft = torch.rand(1,3, 224,224)
np.mean([timer(model_ft,x_ft) for _ in range(10)])

# Example 1.2 Pytorch gpu version

model_ft_gpu = torchvision.models.resnet18(pretrained=True).cuda()
x_ft_gpu = x_ft.cuda()
model_ft_gpu.eval()
np.mean([timer(model_ft_gpu,x_ft_gpu) for _ in range(10)])

# Example 2.1 torch.jit.script cpu version

script_cell = torch.jit.script(model_ft, (x_ft))
np.mean([timer(script_cell,x_ft) for _ in range(10)])

# Example 2.2 torch.jit.script gpu version

script_cell_gpu = torch.jit.script(model_ft_gpu, (x_ft_gpu))
np.mean([timer(script_cell_gpu,x_ft.cuda()) for _ in range(100)])
```

- runtime

<p align='center'><img src='https://miro.medium.com/max/481/1*9OQaSUVR8XMygkQjvgpB_w.png?raw=1' width = '500' ></p>

---

## ë§ˆì¹˜ë©°..

ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” TorchScriptì™€ Pytorch JITì— ëŒ€í•´ ê°„ë‹¨í•˜ê²Œ ì•Œì•„ë³´ì•˜ë‹¤.

ìµœê·¼ì—ëŠ” Torchscriptë¥¼ `Just-In-Time (JIT) Compiler`ê°€ ì•„ë‹Œ, NVIDIAì—ì„œ ê°œë°œí•œ `TensorRT Compiler (Ahead-of-Time)`ë¥¼ ì´ìš©í•˜ì—¬ compileì„ í•˜ëŠ” ì¶”ì„¸ì´ë‹¤. í˜¹ì€, pytorch modelì„ TorchScriptê°€ ì•„ë‹Œ ONNX formatìœ¼ë¡œ ë³€í™˜í•œ í›„, ì´ë¥¼ TensorRT ë“±ì˜ compilerë¥¼ í†µí•´ ìµœì í™”í•˜ê¸°ë„ í•œë‹¤.



---

## reference


- [PyTorch JIT and TorchScript](https://towardsdatascience.com/pytorch-jit-and-torchscript-c2a77bac0fff)
- [TorchScript and PyTorch JIT / Deep Dive](https://www.youtube.com/watch?v=2awmrMRf0dA)
- [How to Convert a Model from PyTorch to TensorRT and Speed Up Inference](https://learnopencv.com/how-to-convert-a-model-from-pytorch-to-tensorrt-and-speed-up-inference/)
- [Torch-TensorRT](https://nvidia.github.io/Torch-TensorRT/)
- [NVIDIA TensorRT â€“ Inference ìµœì í™” ë° ê°€ì†í™”ë¥¼ ìœ„í•œ NVIDIAì˜ Toolkit](https://blogs.nvidia.co.kr/2020/02/19/nvidia-tensor-rt/)
- [pytorch ëª¨ë¸ ì €ì¥ê³¼ ONNX ì‚¬ìš©](https://gaussian37.github.io/dl-pytorch-deploy/)
- [Torch-TensorRT](https://nvidia.github.io/Torch-TensorRT/)
- [LOADING A TORCHSCRIPT MODEL IN C++](https://pytorch.org/tutorials/advanced/cpp_export.html)