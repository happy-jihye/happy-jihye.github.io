---
title: "[nlp] Seq2Seq Model - Sequence to Sequence Learning with Neural Networks ë…¼ë¬¸ ë¦¬ë·° ë° ì½”ë“œ ì‹¤ìŠµ"
excerpt: " "

categories: 
  - nlp
tags: 
  - deeplearning
  - ai
  - nlp
  - pytorch
  - seq2seq
layout: jupyter
search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 

# ìˆ˜ì‹
use_math: true
---

<p align="right">
  <a href="https://github.com/happy-jihye/Natural-Language-Processing/blob/main/code/1_Sequence_to_Sequence_Learning_with_Neural_Networks.ipynb" role="button" target="_blank">
    <img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
  </a>
  <a href="https://mybinder.org/v2/gh/happy-jihye/Natural-Language-Processing/main?filepath=code/1_Sequence_to_Sequence_Learning_with_Neural_Networks.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/assets/badges/binder.svg" alt="Open In Binder"/>
  </a>
  <a href="https://colab.research.google.com/github/happy-jihye/Natural-Language-Processing/blob/main/code/1_Sequence_to_Sequence_Learning_with_Neural_Networks.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
  </a>
</p>

**1 - Sequence to Sequence Learning with Neural Networks**

> 2021/03/26 Happy-jihye ğŸŒº
> 
> **Reference** : [pytorch-seq2seq/1 - Sequence to Sequence Learning with Neural Networks](https://github.com/bentrevett/pytorch-seq2seq)


- Seq2Seq ì‹œë¦¬ì¦ˆì—ì„œëŠ” Pytorchì™€ torch textë¥¼ ì´ìš©í•˜ì—¬ í•˜ë‚˜ì˜ `seq`ë¥¼ ë‹¤ë¥¸ `seq`ë¡œ ë°”ê¾¸ëŠ” ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•  ì˜ˆì •ì…ë‹ˆë‹¤. 
- ì´ë²ˆ ê¸€ì—ì„œëŠ” `ë…ì¼ì–´`ë¥¼ `ì˜ì–´`ë¡œ ë²ˆì—­í•˜ëŠ” translation modelì„ í•™ìŠµí•©ë‹ˆë‹¤. Seq2Seq model ëª¨ë¸ì€ ë²ˆì—­ ì™¸ì—ë„ ë‚´ìš© ìš”ì•½(Text Summarization), STT(Speech to Text)ë“±ì— ì‚¬ìš©ë©ë‹ˆë‹¤.

- ì´ë²ˆ ë…¸íŠ¸ë¶ì—ì„œëŠ” Googleì˜ [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) paperì˜ ëª¨ë¸ì„ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•  ì˜ˆì •ì…ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ Seq2Seqê°œë…ì„ ìµœì´ˆë¡œ Neural Machine Translationì— ì ìš©í•œ ëª¨ë¸ë¡œ, ìì—°ì–´ ì²˜ë¦¬ì— ìˆì–´ êµ‰ì¥íˆ ì¤‘ìš”í•œ ë…¼ë¬¸ì´ë‹ˆ í•œë²ˆì¯¤ì€ ì½ì–´ë³´ì‹œëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤ :)
  - Seq2Seq ê°œë…ì„ ìµœì´ˆë¡œ ì œì•ˆí•œ ë…¼ë¬¸ì€ [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation(2014)](https://arxiv.org/abs/1406.1078)ì…ë‹ˆë‹¤. [ì´ ê¸€](https://happy-jihye.github.io/nlp/nlp-7/)ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ í•™ìŠµí•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

--- 



## Seq2Seq

- ê°€ì¥ ì¼ë°˜ì ì¸ Seq2Seq ëª¨ë¸ì€ `encoder-decoder` ëª¨ë¸ì…ë‹ˆë‹¤. input ë¬¸ì¥ì„ RNNìœ¼ë¡œ single vectorë¡œ ì¸ì½”ë”©í•œ í›„, ì´ single vectorë¥¼ ë‹¤ì‹œ RNN ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ë””ì½”ë”©í•©ë‹ˆë‹¤.
- single vectorëŠ” **context vector**ë¼ê³ ë„ ë¶ˆë¦¬ë©°, ì „ì²´ ì…ë ¥ ë¬¸ì¥ì˜ ì¶”ìƒì ì¸ í‘œí˜„ìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/seq2seq1.png?raw=1)

**Encoder**
- ìœ„ì˜ ì´ë¯¸ì§€ëŠ” ëŒ€í‘œì ì¸ ë²ˆì—­ ì˜ˆì œë¡œ, "guten morgen"ì´ë¼ëŠ” source ë¬¸ì¥ì€ ë…¸ë€ìƒ‰ì˜ `embedding layer`ë¥¼ ê±¸ì³  ì´ˆë¡ìƒ‰ì˜ `encoder`ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤. 
- `<sos>` tokenì€ *start of sequence*, <eos> tokenì€ *end of sequence*ì˜ ì•½ìë¡œ ë¬¸ì¥ì˜ ì‹œì‘ê³¼ ëì„ ì•Œë¦¬ëŠ” tokenì…ë‹ˆë‹¤. 
- Encoder RNNì€ ì´ì „ time stepì˜ hidden stateì™€ í˜„ì¬ time stepì˜ ebeddingê°’ì„ inputìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤. ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

  $h_t = \text{EncoderRNN}(e(x_t), h_{t-1})$
  - ì—¬ê¸°ì„œ input sentenceëŠ” $X = \{x_1, x_2, ..., x_T\}$ë¡œ í‘œí˜„ë˜ë©°, $x_1$ ì€ `<sos>`, $x_2$ ëŠ” `guten`ì´ ë©ë‹ˆë‹¤. 
  - ë˜í•œ ì´ˆê¸° hidden state, $h_0$ëŠ” 0ì´ ë˜ê±°ë‚˜ í•™ìŠµëœ parameterë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.

- RNNë¡œëŠ” LSTM (Long Short-Term Memory)ë‚˜ GRU (Gated Recurrent Unit)ì™€ ê°™ì€ architectureë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**context vector**
- ìµœì¢… ë‹¨ì–´ì¸ $x_T$, `<eos>`ê°€ embedding layerë¥¼ í†µí•´ RNNì— ì „ë‹¬ë˜ë©´, ìš°ë¦¬ëŠ” ë§ˆì§€ë§‰ hidden stateì¸ $h_T$ì„ ì–»ì„ ìˆ˜ ìˆìœ¼ë©°, ì´ë¥¼ context vectorë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. 
- context vectorëŠ” ì „ì²´ ë¬¸ì¥ì„ ëŒ€í‘œí•˜ë©°, $h_T = z$ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Decoder**
- ì´ì œ ìš°ë¦¬ëŠ” context vectorì¸ $z$ë¥¼ output/target sentenceë¡œ ë””ì½”ë”©í•´ì•¼í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ë¬¸ì¥ì˜ ì• ë’¤ì— `<sos>`ì™€ `<eos>` tokenì„ ì¶”ê°€í•©ë‹ˆë‹¤.
- ë””ì½”ë”© ê³¼ì •ì„ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
  
  $s_t = \text{DecoderRNN}(d(y_t), s_{t-1})$

  - ì—¬ê¸°ì„œ í˜„ì¬ ë‹¨ì–´ë¥¼ embedding, $y$í•œ ê°’ì´ $d(y_t)$ì´ë©°, context vector $z = h_T$ëŠ” ì²«ë²ˆì§¸ hidden stateì¸ $s_0$ê³¼ë„ ê°™ìŠµë‹ˆë‹¤.

- ìš°ë¦¬ëŠ” decoderì˜ hidden state $s_t$ë¥¼ ë³´ë¼ìƒ‰ì˜ `Linear layer`ì— ë„£ìŒìœ¼ë¡œì¨ predictionê°’ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  $\hat{y}_t = f(s_t)$

- ì´ë•Œ, decoderì˜ ë‹¨ì–´ëŠ” ê° time stepë‹¹ í•˜ë‚˜ì”© ì°¨ë¡€ëŒ€ë¡œ ìƒì„±ë©ë‹ˆë‹¤. decoderë¥¼ ê±°ì¹˜ë©´ì„œ ë§ì€ ë‹¨ì–´ë“¤ì´ ìƒì„±ì´ ë˜ëŠ”ë°, `<eos>` tokenì´ ì¶œë ¥ë˜ë©´ decodingì„ ë©ˆì¶¥ë‹ˆë‹¤.
- ì˜ˆì¸¡ê°’  $\hat{Y} = \{ \hat{y}_1, \hat{y}_2, ..., \hat{y}_T \}$ì„ ì‹¤ì œ target senteceì˜ ê°’ $Y = \{ y_1, y_2, ..., y_T \}$ê³¼ ë¹„êµí•˜ì—¬ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. 

## 1. Preparing Data


```python
!apt install python3.7
!pip install -U torchtext==0.6.0
!python -m spacy download en
!python -m spacy download de
```


```python
import torch
import torch.nn as nn
import torch.optim as optim

from torchtext.datasets import Multi30k
from torchtext.data import Field, BucketIterator

import spacy
import numpy as np

import random
import math
import time

SEED = 1234

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
```

### **Tokenizers**
- tokenizersëŠ” ë¬¸ì¥ì„ ê°œë³„ tokenìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
  - e.g. "good morning!" becomes ["good", "morning", "!"]
- nlpë¥¼ ì‰½ê²Œ í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” python packageì¸ `spaCy`ë¥¼ ì´ìš©í•˜ì—¬, tokení™”ë¥¼ í•  ì˜ˆì •ì…ë‹ˆë‹¤.



```python
spacy_de = spacy.load('de')
spacy_en = spacy.load('en')
```

**Reversing the order of the words**

  
ì´ ë…¼ë¬¸ì—ì„œëŠ” ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ë°”ê¾¸ë©´ ìµœì í™”ê°€ ë” ì‰¬ì›Œì ¸ ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì§„ë‹¤ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ìœ„í•´ source ë¬¸ì¥ì¸ `ë…ì¼ì–´`ë¥¼ tokení™”ë¥¼ í•œ í›„ ì—­ìˆœìœ¼ë¡œ listì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.


```python
def tokenize_de(text):
  return [tok.text for tok in spacy_de.tokenizer(text)][::-1]

def tokenize_en(text):
  return [tok.text for tok in spacy_en.tokenizer(text)]
```

ë‹¤ìŒìœ¼ë¡œëŠ” **Field** ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. 


```python
SRC = Field(tokenize= tokenize_de,
            init_token = '<sos>',
            eos_token = '<eos>',
            lower = True)

TRG = Field(tokenize= tokenize_en,
            init_token = '<sos>',
            eos_token = '<eos>',
            lower = True)
```

- datasetìœ¼ë¡œëŠ” [Multi30k dataset](https://github.com/multi30k/dataset)ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” ì•½ 3ë§Œê°œì˜ ì˜ì–´, ë…ì¼ì–´, í”„ë‘ìŠ¤ì–´ ë¬¸ì¥ì´ ìˆëŠ” ë°ì´í„°ì´ë©° ê° ë¬¸ì¥ ë‹¹ 12ê°œì˜ ë‹¨ì–´ê°€ ìˆìŠµë‹ˆë‹¤.
- `exts`ëŠ” sourceì™€ targetìœ¼ë¡œ ì‚¬ìš©í•  ì–¸ì–´ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.


```python
train_data, valid_data, test_data = Multi30k.splits(exts= ('.de', '.en'),
                                                    fields = (SRC, TRG))
```

{:.output_stream}

```
downloading training.tar.gz

```

{:.output_stream}

```
training.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.21M/1.21M [00:02<00:00, 544kB/s]

```

{:.output_stream}

```
downloading validation.tar.gz

```

{:.output_stream}

```
validation.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46.3k/46.3k [00:00<00:00, 173kB/s]

```

{:.output_stream}

```
downloading mmt_task1_test2016.tar.gz

```

{:.output_stream}

```
mmt_task1_test2016.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66.2k/66.2k [00:00<00:00, 165kB/s]

```


```python
print(f"Number of training examples: {len(train_data.examples)}")
print(f"Number of validation examples: {len(valid_data.examples)}")
print(f"Number of testing examples: {len(test_data.examples)}")
```

{:.output_stream}

```
Number of training examples: 29000
Number of validation examples: 1014
Number of testing examples: 1000

```

- dataë¥¼ ì¶œë ¥í•´ë³¸ ê²°ê³¼, sourceë¬¸ì¥ì€ ì—­ìˆœìœ¼ë¡œ ì €ì¥ë˜ì–´ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
print(len(vars(train_data.examples[0])['src']))
print(len(vars(train_data.examples[1])['src']))

print(vars(train_data.examples[0]))
print(vars(train_data.examples[1]))
```

{:.output_stream}

```
13
8
{'src': ['.', 'bÃ¼sche', 'vieler', 'nÃ¤he', 'der', 'in', 'freien', 'im', 'sind', 'mÃ¤nner', 'weiÃŸe', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}
{'src': ['.', 'antriebsradsystem', 'ein', 'bedienen', 'schutzhelmen', 'mit', 'mÃ¤nner', 'mehrere'], 'trg': ['several', 'men', 'in', 'hard', 'hats', 'are', 'operating', 'a', 'giant', 'pulley', 'system', '.']}

```

### Build Vocabulary
- `build_vocab`í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ê° tokenì„ indexingí•´ì¤ë‹ˆë‹¤. ì´ë•Œ, sourceì™€ targetì˜ vocabularyëŠ” ë‹¤ë¦…ë‹ˆë‹¤.
- `min_freq`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì†Œ 2ë²ˆ ì´ìƒ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ë§Œ vocabularyì— ë„£ì–´ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ë•Œ, í•œë²ˆë§Œ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ëŠ” `<unk>` tokenìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.
- ì´ë•Œ, vocabularyëŠ” **training set**ì—ì„œë§Œ ë§Œë“¤ì–´ì ¸ì•¼í•©ë‹ˆë‹¤. *(validation/test setì— ëŒ€í•´ì„œëŠ” ë§Œë“¤ì–´ì§€ë©´ ì•ˆë¨!!)* 


```python
SRC.build_vocab(train_data, min_freq = 2)
TRG.build_vocab(train_data, min_freq = 2)
```


```python
print(f"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}")
print(f"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}")
```

{:.output_stream}

```
Unique tokens in source (de) vocabulary: 7855
Unique tokens in target (en) vocabulary: 5893

```

### Create the iterators
- `BucketIterator`ë¥¼ ì´ìš©í•˜ì—¬ batch sizeë³„ë¡œ tokenë“¤ì„ ë¬¶ê³ , ì–´íœ˜ë¥¼ ì½ì„ ìˆ˜ ìˆëŠ” tokenì—ì„œ indexë¡œ ë³€í™˜í•´ì¤ë‹ˆë‹¤.


```python
# for using GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```


```python
print(train_data)
```

{:.output_stream}

```
<torchtext.datasets.translation.Multi30k object at 0x7f0410c961d0>

```


```python
BATCH_SIZE = 128

train_iterator, valid_iterator, test_iterator = BucketIterator.splits(
    (train_data, valid_data, test_data),
    batch_size = BATCH_SIZE,
    device = device
)
```

- ë‹¤ìŒì€ batch sizeê°€ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•´ ì´í•´í•´ë³´ê¸° ìœ„í•´ ì²«ë²ˆì§¸ batchë¥¼ ì¶œë ¥í•´ë³¸ ì˜ˆì œì…ë‹ˆë‹¤. `BucketIterator`ë¥¼ í†µí•´ batchë¼ë¦¬ ë¬¶ìœ¼ë©´ [sequence length, batch size]ë¼ëŠ” tensorê°€ ìƒì„±ë˜ë©°, ì´ tensorëŠ” train_dataë¥¼ batch_sizeë¡œ ë‚˜ëˆˆ ê²°ê³¼ê°’ë§Œí¼ ìƒì„±ë©ë‹ˆë‹¤.
  - ì´ ì˜ˆì œì—ì„œëŠ” 128ì˜ í¬ê¸°ë¥¼ ê°€ì§„ batchê°€ ì´ 227ê°œ ìƒê¹ë‹ˆë‹¤.
- ë˜í•œ, batchì—ì„œ `sequence length`ëŠ” ê·¸ batch ë‚´ì˜ ê°€ì¥ ê¸´ ë¬¸ì¥ì˜ ê¸¸ì´ë¡œ ê²°ì •ë˜ë©° ê·¸ë³´ë‹¤ ì§§ì€ ë¬¸ì¥ë“¤ì— ëŒ€í•´ì„œëŠ” `<pad>` tokenìœ¼ë¡œ ë‚¨ì€ tensorê°’ì´ ì±„ì›Œì§‘ë‹ˆë‹¤.


```python
print(TRG.vocab.stoi[TRG.pad_token]) #<pad> tokenì˜ index = 1

for i, batch in enumerate(train_iterator):
    src = batch.src
    trg = batch.trg

    src = src.transpose(1,0)
    print(f"ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ text í¬ê¸°: {src.shape}")
    print(src[0])
    print(src[1])

    break

print(len(train_iterator))
print(len(train_iterator)*128)
```

{:.output_stream}

```
1
ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ text í¬ê¸°: torch.Size([128, 31])
tensor([   2,    4, 4334,   14,   22,   69,   25,   66,    5,    3,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1], device='cuda:0')
tensor([   2,    4, 1700,  118,  254,   23,  443,   10,  589,    0,   18,   98,
          60,   16,    8,    3,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1], device='cuda:0')
torch.Size([128])
227
29056

```

## Building the Seq2Seq Model

### Encoder
- EncoderëŠ” 2ê°œì˜ LSTM layerë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ë…¼ë¬¸ì—ì„œëŠ” 4ê°œì˜ layerë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ, ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” í•™ìŠµì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•´ 2ê°œì˜ layerë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.)
- RNNì—ì„œëŠ” ì²«ë²ˆì§¸ layerì˜ hidden stateë¥¼ $h_t^1 = \text{EncoderRNN}^1(e(x_t), h_{t-1}^1)$ë¡œ, ë‘ë²ˆì§¸ layerì˜ hidden stateë¥¼ $h_t^2 = \text{EncoderRNN}^2(h_t^1, h_{t-1}^2)$ë¡œ í‘œí˜„í–ˆë‹¤ë©´, LSTMì€ `cell state`ì¸  $c_t$ë„ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤.

![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/seq2seq2.png?raw=1)

- ë”°ë¼ì„œ LSTMì—ì„œì˜ multi-layer equationì„ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  $(h_t^1, c_t^1) = \text{EncoderLSTM}^1(e(x_t), (h_{t-1}^1, c_{t-1}^1))$
  $(h_t^2, c_t^2) = \text{EncoderLSTM}^2(h_t^1, (h_{t-1}^2, c_{t-1}^2))$
 
- RNN architectureì— ëŒ€í•œ ì„¤ëª…ì€ [ì´ ê¸€](https://happy-jihye.github.io/nlp/2_Updated_Sentiment_Analysis/#lstm-long-short-term-memory)ì— ìì„¸íˆ ì ì–´ë†“ì•˜ìŠµë‹ˆë‹¤.


```python
class Encoder(nn.Module):
  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
    super().__init__()

    self.hid_dim = hid_dim
    self.n_layers = n_layers

    self.embedding = nn.Embedding(input_dim, emb_dim)

    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)

    self.dropout = nn.Dropout(dropout)

  def forward(self, src):

    # src = [src len, batch size]
    embedded = self.dropout(self.embedding(src))

    # embedded = [src len, batch size, emb dim]

    outputs, (hidden, cell) = self.rnn(embedded)

    # hidden = [n layers * n directions, batch size, hid dim]
    # cell = [n layer * n directions, batch size, hid dim]

    # outputs = [src len, batch size, hid dim * n directions]
    ## outputì€ ì–¸ì œë‚˜ hidden layerì˜ topì— ìˆìŒ

    return hidden, cell
```

### Decoder
- decoderë„ encoderì™€ ë§ˆì°¬ê°€ì§€ë¡œ 2ê°œì˜ LSTM layerë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. (ë…¼ë¬¸ì—ì„œëŠ” 4ê°œì˜ layerë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.)
  ![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/seq2seq3.png?raw=1)

- ë‹¤ìŒì€ Decoderì˜ layerë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ì‹ì…ë‹ˆë‹¤.

  $(s_t^1, c_t^1) = \text{DecoderLSTM}^1(d(y_t), (s_{t-1}^1, c_{t-1}^1))\\
  (s_t^2, c_t^2) = \text{DecoderLSTM}^2(s_t^1, (s_{t-1}^2, c_{t-1}^2))$


```python
class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()

        self.output_dim = output_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        
        self.embedding = nn.Embedding(output_dim, emb_dim)
        
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)
        
        self.fc_out = nn.Linear(hid_dim, output_dim)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, input, hidden, cell):
        
        # input = [batch size]
        ## í•œë²ˆì— í•˜ë‚˜ì˜ tokenë§Œ decodingí•˜ë¯€ë¡œ forwardì—ì„œì˜ input tokenì˜ ê¸¸ì´ëŠ” 1ì…ë‹ˆë‹¤.
        
        # hidden = [n layers * n directions, batch size, hid dim]
        # cell = [n layers * n directions, batch size, hid dim]
        
        # n directions in the decoder will both always be 1, therefore:
        # hidden = [n layers, batch size, hid dim]
        # context = [n layers, batch size, hid dim]
        
        input = input.unsqueeze(0)
        
        # inputì„ 0ì°¨ì›ì— ëŒ€í•´ unsqueezeí•´ì„œ 1ì˜ sentence length dimensionì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        # input = [1, batch size]
        
        embedded = self.dropout(self.embedding(input))
        
        # embedding layerë¥¼ í†µê³¼í•œ í›„ì— dropoutì„ í•©ë‹ˆë‹¤.
        # embedded = [1, batch size, emb dim]
                
        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))
        
        # output = [seq len, batch size, hid dim * n directions]
        # hidden = [n layers * n directions, batch size, hid dim]
        # cell = [n layers * n directions, batch size, hid dim]
        
        # seq len and n directions will always be 1 in the decoder, therefore:
        # output = [1, batch size, hid dim]
        # hidden = [n layers, batch size, hid dim]
        # cell = [n layers, batch size, hid dim]
        
        prediction = self.fc_out(output.squeeze(0))
        
        #prediction = [batch size, output dim]
        
        return prediction, hidden, cell
```

## Seq2Seq

seq2seq modelì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
- encoderì— source(input) sentenceë¥¼ ì…ë ¥í•œë‹¤.
- encoderë¥¼ í•™ìŠµì‹œì¼œ ê³ ì •ëœ í¬ê¸°ì˜ context vectorë¥¼ ì¶œë ¥í•œë‹¤.
- context vectorë¥¼ decoderì— ë„£ì–´ ì˜ˆì¸¡ëœ target(output) sentenceë¥¼ ìƒì„±í•œë‹¤.

![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/seq2seq4.png?raw=1)

- ì´ë²ˆ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Encoderì™€ Decoderì—ì„œì˜ layerì˜ ìˆ˜ì™€ hidden/cell dimensionsì„ ë™ì¼í•˜ê²Œ ë§ì¶°ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” í•­ìƒ ê·¸ë˜ì•¼í•˜ëŠ” í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆì§€ë§Œ, layerì˜ ê°œìˆ˜ë‚˜ ì°¨ì›ì„ ë‹¤ë¥´ê²Œ í•´ì¤€ë‹¤ë©´ ì¶”ê°€ì ìœ¼ë¡œ ìƒê°í•´ì¤„ ë¬¸ì œë“¤ì´ ë§ì•„ì§ˆ ê²ƒì…ë‹ˆë‹¤. 
  - ex) ì¸ì½”ë“œì˜ ë ˆì´ì–´ëŠ” 2ê°œ, ë””ì½”ë”ì˜ ë ˆì´ì–´ëŠ” 1ê°œë¼ë©´ context vectorì˜ í‰ê· ì„ ë””ì½”ë”ì— ë„˜ê²¨ì¤˜ì•¼í•˜ë‚˜?
- targetë¬¸ì¥ê³¼ outputë¬¸ì¥ì˜ tensorëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
  ![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/seq2seq5.png?raw=1)

**Teacher Forcing**
![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/seq2seq6.png?raw=1)
- teacher forcingì€ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ë””ì½”ë”ì˜ ì˜ˆì¸¡ì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹  ì‹¤ì œ ëª©í‘œ ì¶œë ¥ì„ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì»¨ì…‰ì…ë‹ˆë‹¤. ([ì°¸ê³ ](https://tutorials.pytorch.kr/intermediate/seq2seq_translation_tutorial.html)) ì¦‰, `target word`(Ground Truth)ë¥¼ ë””ì½”ë”ì˜ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì¤Œìœ¼ë¡œì¨ í•™ìŠµì‹œ ë” ì •í™•í•œ ì˜ˆì¸¡ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- [ì°¸ê³ 2](https://blog.naver.com/PostView.nhn?blogId=sooftware&logNo=221790750668&categoryNo=0&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView)


```python
class Seq2Seq(nn.Module):

    def __init__(self, encoder, decoder, device):
        super().__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        
        assert encoder.hid_dim == decoder.hid_dim, \
            "Hidden dimensions of encoder and decoder must be equal!"
        assert encoder.n_layers == decoder.n_layers, \
            "Encoder and decoder must have equal number of layers!"
        
    def forward(self, src, trg, teacher_forcing_ratio = 0.5):
        
        #src = [src len, batch size]
        #trg = [trg len, batch size]
        #teacher_forcing_ratio is probability to use teacher forcing
        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time
        
        batch_size = trg.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim
        
        # outputì„ ì €ì¥í•  tensorë¥¼ ë§Œë“­ë‹ˆë‹¤.(ì²˜ìŒì—ëŠ” ì „ë¶€ 0ìœ¼ë¡œ)
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)
        
        # srcë¬¸ì¥ì„ encoderì— ë„£ì€ í›„ hidden, cellê°’ì„ êµ¬í•©ë‹ˆë‹¤.
        hidden, cell = self.encoder(src)
        
        # decoderì— ì…ë ¥í•  ì²«ë²ˆì§¸ inputì…ë‹ˆë‹¤.
        # ì²«ë²ˆì§¸ inputì€ ëª¨ë‘ <sos> tokenì…ë‹ˆë‹¤.
        # trg[0,:].shape = BATCH_SIZE 
        input = trg[0,:]  
        
        
        '''í•œë²ˆì— batch_sizeë§Œí¼ì˜ tokenë“¤ì„ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°
        ì¦‰, ì´ trg_lenë²ˆì˜ forë¬¸ì´ ëŒì•„ê°€ë©° ì´ forë¬¸ì´ ë‹¤ ëŒì•„ê°€ì•¼ì§€ë§Œ í•˜ë‚˜ì˜ ë¬¸ì¥ì´ decodingë¨
        ë˜í•œ, 1ë²ˆì˜ forë¬¸ë‹¹ 128ê°œì˜ ë¬¸ì¥ì˜ ê° tokenë“¤ì´ ë‹¤ê°™ì´ decodingë˜ëŠ” ê²ƒ'''
        for t in range(1, trg_len):
            
            # input token embeddingê³¼ ì´ì „ hidden/cell stateë¥¼ decoderì— ì…ë ¥í•©ë‹ˆë‹¤.
            # ìƒˆë¡œìš´ hidden/cell statesì™€ ì˜ˆì¸¡ outputê°’ì´ ì¶œë ¥ë©ë‹ˆë‹¤.
            output, hidden, cell = self.decoder(input, hidden, cell)

            #output = [batch size, output dim]

            # ê°ê°ì˜ ì¶œë ¥ê°’ì„ outputs tensorì— ì €ì¥í•©ë‹ˆë‹¤.
            outputs[t] = output
            
            # decide if we are going to use teacher forcing or not
            teacher_force = random.random() < teacher_forcing_ratio
            
            # predictionsë“¤ ì¤‘ì— ê°€ì¥ ì˜ ì˜ˆì¸¡ëœ tokenì„ topì— ë„£ìŠµë‹ˆë‹¤.
            # 1ì°¨ì› ì¤‘ ê°€ì¥ í° ê°’ë§Œì„ top1ì— ì €ì¥í•˜ë¯€ë¡œ 1ì°¨ì›ì€ ì‚¬ë¼ì§‘ë‹ˆë‹¤.
            top1 = output.argmax(1) 
            # top1 = [batch size]
            
            # teacher forcingê¸°ë²•ì„ ì‚¬ìš©í•œë‹¤ë©´, ë‹¤ìŒ inputìœ¼ë¡œ targetì„ ì…ë ¥í•˜ê³ 
            # ì•„ë‹ˆë¼ë©´ ì´ì „ stateì˜ ì˜ˆì¸¡ëœ ì¶œë ¥ê°’ì„ ë‹¤ìŒ inputìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
            input = trg[t] if teacher_force else top1
        
        return outputs
```


## Training the Seq2Seq Model


```python
INPUT_DIM = len(SRC.vocab)
OUTPUT_DIM = len(TRG.vocab)
ENC_EMB_DIM = 256
DEC_EMB_DIM = 256
HID_DIM = 512
N_LAYERS = 2
ENC_DROPOUT = 0.5
DEC_DROPOUT = 0.5

enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)

model = Seq2Seq(enc, dec, device).to(device)
```

- ì´ˆê¸° ê°€ì¤‘ì¹˜ê°’ì€ $\mathcal{U}(-0.08, 0.08)$ì˜ ì—°ì†ê· ë“±ë¶„í¬ë¡œë¶€í„° ì–»ì—ˆìŠµë‹ˆë‹¤.


```python
def init_weights(m):
    for name, param in m.named_parameters():
        nn.init.uniform_(param.data, -0.08, 0.08)
        
model.apply(init_weights)
```




{:.output_data_text}

```
Seq2Seq(
  (encoder): Encoder(
    (embedding): Embedding(7855, 256)
    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)
    (dropout): Dropout(p=0.5, inplace=False)
  )
  (decoder): Decoder(
    (embedding): Embedding(5893, 256)
    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)
    (fc_out): Linear(in_features=512, out_features=5893, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
```




```python
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'The model has {count_parameters(model):,} trainable parameters')
```

{:.output_stream}

```
The model has 13,899,013 trainable parameters

```

- optimizerí•¨ìˆ˜ë¡œëŠ” `Adam`ì„ ì‚¬ìš©í•˜ì˜€ê³ , loss functionìœ¼ë¡œëŠ” `CrossEntropyLoss`ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, `<pad>` tokenì— ëŒ€í•´ì„œëŠ” loss ê³„ì‚°ì„ í•˜ì§€ ì•Šë„ë¡ ì¡°ê±´ì„ ë¶€ì—¬í–ˆìŠµë‹ˆë‹¤.


```python
optimizer = optim.Adam(model.parameters())
```


```python
TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]

criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)
```

### Training


```python
def train(model, iterator, optimizer, criterion, clip):
    
    model.train()
    
    epoch_loss = 0
    
    for i, batch in enumerate(iterator):
        
        src = batch.src
        trg = batch.trg
        
        optimizer.zero_grad()
        
        output = model(src, trg)
        
        #trg = [trg len, batch size]
        #output = [trg len, batch size, output dim]
        
        output_dim = output.shape[-1]
        
        output = output[1:].view(-1, output_dim)
        trg = trg[1:].view(-1)
        
        #trg = [(trg len - 1) * batch size]
        #output = [(trg len - 1) * batch size, output dim]
        
        loss = criterion(output, trg)
        
        loss.backward()
        
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        
        optimizer.step()
        
        epoch_loss += loss.item()
        
    return epoch_loss / len(iterator)
```

### Evaluation


```python
def evaluate(model, iterator, criterion):
    
    model.eval()
    
    epoch_loss = 0
    
    with torch.no_grad():
    
        for i, batch in enumerate(iterator):

            src = batch.src
            trg = batch.trg

            output = model(src, trg, 0) #turn off teacher forcing

            #trg = [trg len, batch size]
            #output = [trg len, batch size, output dim]

            output_dim = output.shape[-1]
            
            output = output[1:].view(-1, output_dim)
            trg = trg[1:].view(-1)

            #trg = [(trg len - 1) * batch size]
            #output = [(trg len - 1) * batch size, output dim]

            loss = criterion(output, trg)
            
            epoch_loss += loss.item()
        
    return epoch_loss / len(iterator)
```


```python
def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs
```

### Train the model through multiple epochsPermalink


```python
N_EPOCHS = 10
CLIP = 1

best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):
    
    start_time = time.time()
    
    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)
    valid_loss = evaluate(model, valid_iterator, criterion)
    
    end_time = time.time()
    
    epoch_mins, epoch_secs = epoch_time(start_time, end_time)
    
    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'tut1-model.pt')
    
    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')
```

{:.output_stream}

```
Epoch: 01 | Time: 0m 38s
	Train Loss: 5.052 | Train PPL: 156.330
	 Val. Loss: 5.009 |  Val. PPL: 149.767
Epoch: 02 | Time: 0m 37s
	Train Loss: 4.483 | Train PPL:  88.471
	 Val. Loss: 4.817 |  Val. PPL: 123.627
Epoch: 03 | Time: 0m 37s
	Train Loss: 4.193 | Train PPL:  66.237
	 Val. Loss: 4.675 |  Val. PPL: 107.187
Epoch: 04 | Time: 0m 37s
	Train Loss: 4.006 | Train PPL:  54.940
	 Val. Loss: 4.543 |  Val. PPL:  93.994
Epoch: 05 | Time: 0m 37s
	Train Loss: 3.853 | Train PPL:  47.152
	 Val. Loss: 4.419 |  Val. PPL:  83.004
Epoch: 06 | Time: 0m 37s
	Train Loss: 3.717 | Train PPL:  41.151
	 Val. Loss: 4.419 |  Val. PPL:  83.041
Epoch: 07 | Time: 0m 37s
	Train Loss: 3.598 | Train PPL:  36.537
	 Val. Loss: 4.235 |  Val. PPL:  69.030
Epoch: 08 | Time: 0m 37s
	Train Loss: 3.462 | Train PPL:  31.871
	 Val. Loss: 4.120 |  Val. PPL:  61.552
Epoch: 09 | Time: 0m 37s
	Train Loss: 3.339 | Train PPL:  28.205
	 Val. Loss: 4.060 |  Val. PPL:  57.994
Epoch: 10 | Time: 0m 37s
	Train Loss: 3.212 | Train PPL:  24.839
	 Val. Loss: 4.076 |  Val. PPL:  58.898

```


```python
model.load_state_dict(torch.load('tut1-model.pt'))

test_loss = evaluate(model, test_iterator, criterion)

print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')
```
