---
title: "Chinchilla’s Death 리뷰"
excerpt: " "

categories: 
  - nlp
tags: 
  - deeplearning
  - ai
  - nlp
  - LLM

search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---

# Chinchilla’s Death

[Chinchilla’s Death](https://espadrine.github.io/blog/posts/chinchilla-s-death.html)

> Training Compute-Optimal Large Language Models (DeepMind, 2022)
>
> - for compute-optimal training, the model size and the number of training tokens should be scaled equally
> - **Chinchilla**: Gopher (280B)와 똑같은 compute budget을 사용. but 70B param & 4배 더 많은 data
> - 그동안의 LLM 모델들은 underfitting이 되어있었다. model size가 크다고 장땡이 아니라 학습하는 token도 비례하게 커져야한다. Chinchilla는 70B로 gopher보다 모델의 크기가 작지만, 더 많은 token으로 학습을 해서 성능이 더 좋았음

위 논문에는 한가지 가설이 있음

- larger model은 smaller model보다 성능이 항상 더 좋으며,
- 크기가 고정된 모델은 용량이 제한되어있다.

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/17-cd/0.png?raw=1" width = "500" ></p>

저자는 이 가설에 문제점을 제기

- 위 실험은 작은 모델이 큰 모델에 추월 당하자마자 훈련을 중단했기 때문에 성능이 안좋은거지.. 만약 더 오래 훈련하도록 놔둔다면 성능이 더 좋아질 수 있지 않을까?

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/17-cd/1.png?raw=1" width = "700" ></p>

실제로 다양한 size의 Llama1 모델을 학습시켜보니…

그동안 작은 모델의 성능이 안나왔던게.. 모델의 capacity 문제가 아니라 학습시간이 문제였음을 발견

- 7B를 오래 학습시킨다면 13B보다 좋았을수도 !
- 심지어 33B는 65B보다 성능이 좋음 (`모델의 size가 클수록 성능이 좋을 것이라는` Chinchilla의 가정이 깨지는 것!)

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/17-cd/2.png?raw=1" width = "700" ></p>

Llama2

- Llama1보다 40% 더 많은 dataset로 학습
- grouped-query attention을 써서 Llama1보다 훨씬 더 빠르게 inference가 가능
- Llama1보다 context size가 2배, cosine schedule도 훨씬 더 김
    - Llama2의 context length = 4096 tokens

---

**Llama1 VS Llama2**

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/17-cd/3.webp?raw=1" width = "700" ></p>

- 저자는 Llama1과 2를 비교해보면 Llama1의 가설이 대략 맞을 수도 있다고 보고하고 있음
    - 학습 초기에 작은 모델은 큰 모델보다 성능이 안좋지만.. (chinchilla에 따라), 학습시간이 길어지다보면 linear로 loss가 떨어지는 좋은 상태에 도달한 후 큰 모델보다 성능이 더 좋아진다
- 통상적으로 믿어지는 것과 다르게, larger model의 성능이 더 안좋았음
- 단기적인 관점에서 llama1 → 2로 갈 때, 모델이 작을수록 성능이 나빠짐 (결과론적으로는 Llama2가 1보다 학습을 오래시켰기 때문에 성능이 좋음)
    - 이것은 lr cosine scheduling 문제일 가능성이 크다
- 큰 모델을 조금 학습시키는 것보다 7B 짜리 모델을 1T token에 7 epoch 정도 학습하는게 좋을 수도 있음

---

### [결론] Chinchlla’s Death

Chinchilla는 `모델의 사이즈가 클수록 성능이 좋을 것` 이라는 강력한 가정 하에, 모델에 적합한 최적의 학습 토큰 수를 찾으려 했음. 그러나 위 레포트에 따르면, 모델 사이즈보다 `어떻게 학습할 것인가 (lr 세팅)와 얼마나 학습할 것인가` 가 더 중요하다고 하고 있음

즉, 모델이 작다고 학습도 조금만 시키는게 아니라 작은 모델도 “오래” “잘” 학습시키면 큰 모델만큼의 성능이 나올 수 있다고 주장!