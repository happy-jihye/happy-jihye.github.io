---
title: "[2] Network - CNN, RNN, LSTM, Transformer"
excerpt: " "

categories: 
  - nlp
tags: 
  - deeplearning
  - ai
  - nlp
  - LLM

search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---

## [1] Vision
### [1-1] CNN

- ConvNet
    - convolutional layer + fc layer + pooling layer 등을 쌓아 생성
    - spatial structure를 유지하면서도 feature extraction이 가능
    - conv-layer
        - input data에 대해 여러개의 filter를 적용하여 feature map을 추출. 입력데이터의 spatial pattern을 인식하도록 학습
    - pooling layer
        - feature map의 크기를 줄이는 역할 → 모델 계산 복잡도를 낮추고, 작은 변화에 invariance 하게 학습이 가능해짐 + overfitting 방지
    - cnn에서 자주 쓰는 activation function은?
        - ReLU 계열 (gradient vanishing 문제 완화, 연산이 빠름)
    - [CNN은 translation variance (equivariance)하다](https://ganghee-lee.tistory.com/43)
        - max pooling + CNN의 weight sharing + learn local features, softmax를 통한 확률값 계산
    
    <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/21-network/0/0.png?raw=1" width = "600" ></p>
    
- [CNN 계열 image classification network](https://wikidocs.net/147016)
    - [1-1] LeNet (layer 8개 이하)
        - 1998, Yann LeCun 연구팀이 제시한 단순한 CNN
        - feature를 잘 추출하고, 이미지의 공간적인 topology를 잘 반영
        
        <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/21-network/0/1.png?raw=1" width = "600" ></p>
        
    - [1-2] AlexNet (layer 8개 이하)
        - 처음으로 ReLU activation을 사용 (연산량이 빠름)
        - overfitting 방지를 위해
            - data augmentation, dropout, batchnorm layer를 도임
        
        <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/21-network/0/2.png?raw=1" width = "500" ></p>
        
    - [2-1] VGGNet (layer 22개 이하)
        - 3x3의 작은 filter를 사용 (이전에는 5x5를 사용)
            - 필터의 사이즈를 줄이고 모델의 depth를 키워 더 효율적인 receptive field를 갖게 함. & layer 당 더 작은 parameter
            - layer의 depth를 키워 (activation function이 더 많아짐) → 더 많은 non-linearity를 줌
        - padding을 통해 image size를 유지
    - [2-2] GoogleNet (layer 22개 이하)
        
        <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/21-network/0/3.png?raw=1" width = "700" ></p>
        
        - inception module을 반복하는 형태
            - inception module안에 1x1 conv를 추가해서 dimension을 줄여줌 (bottle neck layer라고 부름. 깊은 network에서 중요한 정보를 뽑아주는 역할 + 비선형성을 증가시켜 복잡한 함수도 approximation해주는 역할)
            - max pooling 을 통해 conv 연산 후 dimension을 줄여줌
    - [3] ResNet
        - CNN에서 layer가 깊다고 성능이 좋은 것은 아니었음. 실제로 20층 이상의 layer를 쌓으면 성능이 degradation 됨 → ResNet: residual learning을 통해 모델이 깊어져도 학습이 잘되게 구현
        - skip connection

---

## [2] NLP

### [2-1] **Text Embedding**

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/21-network/1/0.jpg?raw=1" width = "800" ></p>

- perplexity, PPL
    - 언어모델이 test set의 문장에 대해서도 높은 확률을 부여하는지 평가하는 것. 낮을수록 좋다
    - 수식 근사 관련 [링크1](https://heiwais25.github.io/nlp/2019/10/13/Language-model-3/), [링크2](https://soundprovider.tistory.com/entry/LM-Perplexity-%EA%B0%9C%EB%85%90)

### [2-2] Language Model

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/21-network/1/3.png?raw=1" width = "700" ></p>

### [2-3] RNN / LSTM

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/21-network/1/1.jpg?raw=1" width = "800" ></p>

### [2-4] Transformer

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/21-network/1/2.jpg?raw=1" width = "800" ></p>

- [attention과 self-attention의 개념](https://questionet.tistory.com/46)
    - self-attention
        - Query: 현재 위치의 토큰이 다른 위치의 토큰과 어떤 관련이 있는지를 알아내기 위한 질의 벡터
        - Key: 모든 위치의 토큰들이 Query에 대해 어떤 관련이 있는지를 나타내는 벡터
        - Value: Attention 스코어에 기반하여 가중 평균을 계산할 때 사용되는 벡터
        - 이렇게 계산된 Attention 값은 입력 시퀀스의 각 위치에 대해 다른 위치들의 정보를 종합한 컨텍스트 벡터로 해석할 수 있음. 이를 통해 Transformer는 장기 의존성 문제를 해결하고, 시퀀스 내의 토큰들 간의 관계를 효과적으로 모델링 가능
- Transformer
    
    Transformer는 RNN(Recurrent Neural Network)이나 CNN(Convolutional Neural Network)과는 다른 아키텍처로, 시퀀스 데이터 처리에 널리 사용되는 모델
    
    1. 순차적 처리 vs 병렬 처리
        - RNN: 시퀀스 데이터를 순차적으로 처리함. 각 타임스텝의 출력이 다음 타임스텝의 입력으로 사용됨.
        - CNN: 일반적으로 고정된 크기의 입력을 처리하며, 필터를 사용하여 local 특징을 추출합니다.
        - Transformer: 입력 시퀀스 전체를 한 번에 처리하며, 병렬 연산이 가능함. → 학습 속도가 크게 향상
    2. 장기 의존성 문제 해결 방식
        - RNN: LSTM이나 GRU와 같은 변형을 사용하여 장기 의존성 문제를 해결.
        - CNN: 장기 의존성 문제를 직접적으로 해결하기 어려우며, 일반적으로 지역적 특징에 초점을 맞춤.
        - Transformer: Self-Attention 메커니즘을 사용하여 장기 의존성 문제를 해결함. 입력 시퀀스의 모든 위치 간의 관계를 직접 모델링 가능.
    3. 위치 정보 반영 방식
        - RNN: 순차적 처리로 인해 위치 정보가 자연스럽게 반영됨.
        - CNN: 위치 정보를 반영하기 위해 위치 임베딩(Positional Embedding)을 추가로 사용할 수 있습니다.
        - Transformer: 위치 정보를 반영하기 위해 위치 인코딩(Positional Encoding)을 사용. 이는 입력 임베딩에 위치 정보를 더해줌.
    4. 컨텍스트 크기
        - RNN: 이론적으로 무한한 컨텍스트 크기를 가질 수 있지만, 실제로는 그래디언트 소실 문제로 인해 제한됨.
        - CNN: 일반적으로 고정된 크기의 컨텍스트 윈도우를 사용.
        - Transformer: Self-Attention을 통해 입력 시퀀스의 모든 위치를 참조할 수 있어 이론적으로 무한한 컨텍스트 크기를 가질 수 있음