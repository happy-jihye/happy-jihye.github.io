---
title: "[Paper Review] LLaMA: Open and Efficient Foundation Language Models 논문 리뷰"
excerpt: " "

categories: 
  - nlp
tags: 
  - deeplearning
  - ai
  - nlp
  - LLM

search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---

Paper : [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)

> We introduce **LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens**, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, **LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA65B is competitive with the best models, Chinchilla-70B and PaLM-540B.** We release all our models to the research community1 .

## 1. Introduction

**Background**
- 기존에는 Assumption이 있었음 - `모델이 클수록 (param 수가 많을수록) 성능이 좋을 것이다`
- `Chinchilla: Training Compute-Optimal Large Language Models (DeepMind, 2022)`
    - chinchilla 논문에서는 동일한 compute budget이 주어졌을 때, 단순히 모델을 키운다고 성능이 좋아지는 게 아니라, 이에 비례해서 data의 양도 늘리는 게 중요하다고 주장

**LlaMA**
- Chinchilla, PaLM, GPT-3 등의 연구들은 inference budget을 고려하지 않아서 실질적으로 서비스 등에 활용하기 어려웠음 → LLaMA: 주어진 training budget 내에서 얼마나 모델 성능을 키울 수 있을지에 focus를 두지 않고,  inference budget에 주목
- Chinchilla 논문의 scaling law에서는 작은 모델을 오래 학습시키면 좋다고만 주장했지, 작은 모델을 오래 학습하진 않았음
  - Chinchilla: 10B model을 200B token으로 학습
  - **LLaMA: 7B model을 1T token으로 학습해봤더니 성능이 더 좋았다!**

**Summary**

- **LLaMA (7B ~ 65B models): SoTA의 성능**
    - LLaMA-13B outperforms GPT-3 (175B)
    - LLaMA-65B outperforms Chinchilla-70B and PaLM-540B.
- GPT3, Chinchilla, PaLM과 다르게 공개된 데이터만을 사용해서 학습

---

## 2. Approach

We train large transformers on a large quantity of textual data using a standard optimizer.

### 2.1 Pre-training Data

**[1] Dataset**

- open source data만을 사용해서 학습

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/12-llama/1.png?raw=1" width = "500" ></p>

**[2] Tokenizer**

- BPE (byte-pair encoding) 알고리즘에 따라 tokenizing
- tokenization 후, 전체 데이터: 1.4T tokens
    - Table 1에서 wikipedia & Books 데이터만 2 epoch 학습. 나머지 data들은 1 epoch씩 학습

---

### 2.2 Architecture

Transformer architecture를 따르되 약간을 변형

1. Pre-normalization (GPT3)
    - 학습 안정성을 위해 output 대신 transformer의 sub-layer의 input들을 normalization
    - RMSNorm을 사용
2. SwiGLU activation function (PaLM)
    - 성능 향상을 위해 ReLU 대신 SwiGLU를 사용
    - PaLM 처럼 4d 대신 2/3 4d dimension을 사용했다함
3. Rotary Embeddings (GPTNeo)
    - positional embedding을 제거하고 RoPE (Rotary positional Embedding) 도입

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/12-llama/2.png?raw=1" width = "800" ></p>

### 2.3 Optimizer

Our models are trained using **the AdamW optimize**r (Loshchilov and Hutter, 2017), with the following hyper-parameters: β1 = 0.9, β2 = 0.95. We use **a cosine learning rate schedule**, such that the final learning rate is equal to 10% of the maximal learning rate. We use **a weight decay of 0.1 and gradient clipping of 1.0**. We use 2, 000 warmup steps, and vary the learning rate and batch size with the size of the model (see Table 2 for details).

### 2.4 Efficient Implementation

학습 속도를 올리기 위해 optimization을 진행

1. memory usage & runtime을 줄이기 위해 multi-head attention을 진행
    - by xformers library
    - This is achieved by not storing the attention weights and not computing the key/query scores that are masked due to the causal nature of the language modeling task.
2. To further improve training efficiency, we **reduced the amount of activations that are recomputed during the backward pass with checkpointing**. More precisely, we save the activations that are expensive to compute, such as the outputs of linear layers.

그 결과, 65B 모델을 학습시킬 때,

- 380 tokens/sec/GPU 소요
- 2048개의 A100 GPU (with 80GB RAM)

⇒ 총 1.4T tokens의 dataset을 학습했기 때문에 21일 소요 (`(1.4*10^12)/(380*2048*(3600*24*))= 약 20.82 Days`

---

## 3. Main Results

- 총 20개의 benchmark에 대해 zero-shot & few-shot 평가
- [EleutherAI의 lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) 으로 모델 평가
    - 아니라 answer 전체에 대한 각 token의 확률값을 aggregation해서 구하는 방식

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/12-llama/3.png?raw=1" width = "800" ></p>

## 4. Instruction Finetuning
<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/12-llama/4.png?raw=1" width = "350" ></p>

- MMLU 벤치마크셋에 대해 instuction finetuning
- 다른 instruction finetuned 모델들보다 성능은 좋으나 SoTA는 아님
