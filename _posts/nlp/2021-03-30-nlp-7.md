---
title: "[nlp] Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation ë…¼ë¬¸ ë¦¬ë·° ë° ì½”ë“œ ì‹¤ìŠµ"
excerpt: " "

categories: 
  - nlp
tags: 
  - deeplearning
  - ai
  - nlp
  - pytorch
  - seq2seq
layout: jupyter
search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 

---

<p align="right">
  <a href="https://github.com/happy-jihye/Natural-Language-Processing/blob/main/code/2_Learning_Phrase_Representations_using_RNN_Encoder_Decoder_for_Statistical_Machine_Translation.ipynb" role="button" target="_blank">
    <img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
  </a>
  <a href="https://mybinder.org/v2/gh/happy-jihye/Natural-Language-Processing/main?filepath=code/2_Learning_Phrase_Representations_using_RNN_Encoder_Decoder_for_Statistical_Machine_Translation.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/assets/badges/binder.svg" alt="Open In Binder"/>
  </a>
  <a href="https://colab.research.google.com/github/happy-jihye/Natural-Language-Processing/blob/main/code/2_Learning_Phrase_Representations_using_RNN_Encoder_Decoder_for_Statistical_Machine_Translation.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
  </a>
</p>


> 2021/03/29 Happy-jihye ğŸŒº
> 
> **Reference** : [pytorch-seq2seq/2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://github.com/bentrevett/pytorch-seq2seq)
> 
> **paper** : [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation(2014)](https://arxiv.org/abs/1406.1078)


--- 

# 0. Introduction

![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/seq2seq9.png?raw=1)

- ì´ë²ˆ ë…¸íŠ¸ë¶ì—ì„œëŠ” [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation(2014)](https://arxiv.org/abs/1406.1078) paperì˜ ëª¨ë¸ì„ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•  ì˜ˆì •ì…ë‹ˆë‹¤.

- ì´ ë…¼ë¬¸ì€ ë‘ ê°€ì§€ ë‚´ìš©ìœ¼ë¡œ ìœ ëª…í•©ë‹ˆë‹¤. <u>í•˜ë‚˜ëŠ” ê¸°ê³„ë²ˆì—­ Neural Machine Translation(NMT) ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì“°ì´ê³  ìˆëŠ” Seq2Seq architectureì˜ ì œì•ˆì´ê³ , ë‘ë²ˆì§¸ëŠ” LSTMì˜ ëŒ€ì•ˆì¸ Gated Recurrent Unit(GRU)ì˜ ë„ì…ì…ë‹ˆë‹¤.</u>
  
  - ì´ ë…¼ë¬¸ì€ Seq2Seq modelì„ ì œì‹œí•œ ë…¼ë¬¸ì´ì§€, ì´ë¥¼ NMT ë¶„ì•¼ì— ì‚¬ìš©í•œ ë…¼ë¬¸ì€ ì•„ë‹™ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ë‹¹ì‹œ í™œìš©ë˜ë˜ Statical Machine Translation(SMT)ë¶„ì•¼ì˜ í•œ íŒŒíŠ¸ë¡œì„œ **RNN Encoder-Decoder model**ì„ ì œì•ˆí•˜ì˜€ìŠµë‹ˆë‹¤. 
  - ì‹¤ì œë¡œ ì´ ëª¨ë¸ì„ NMT ë¶„ì•¼ì— ì ìš©í•œ ë…¼ë¬¸ì€ [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)ì…ë‹ˆë‹¤.
  - [SMT vs NMT](https://smartlion.co.kr/news-%EC%8B%A0%EA%B2%BD%EA%B8%B0%EA%B3%84%EB%B2%88%EC%97%ADnmt%EC%9D%98%EC%8B%9C%EB%8C%80/)
  
- Sequence to Sequence Learning with Neural Networks, LSTM ë“±ì— ëŒ€í•´ ê³µë¶€í•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ ì´ ê¸€ë“¤([Seq2Seq-NMT](https://happy-jihye.github.io/nlp/1_Sequence_to_Sequence_Learning_with_Neural_Networks/)ê³¼ [Understanding LSTM Network](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))ì„ ì°¸ê³ í•˜ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤ :)

# 1. Paper Review

## **RNN Encoder-Decoder**

<p align="center"><img src="https://raw.githubusercontent.com/happy-jihye/Natural-Language-Processing/main/images/seq2seq10.png" width = "400" ></p>

ì´ë²ˆ ì‹œê°„ì— ë°°ìš¸ ëª¨ë¸ì˜ architectureëŠ” ê°„ë‹¨í•©ë‹ˆë‹¤.

- **RNN Encoder-Decoder**ì€ encoderì™€ decoder ì—­í• ì„ í•˜ëŠ” 2ê°œì˜ Recurrent Neural Network(RNN)ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, **Encoder**ëŠ” ê°€ë³€ ê¸¸ì´ì˜ `source sequence`ë¥¼ ê³ ì •ëœ í¬ê¸°ì˜ `context vector`ë¡œ ë§Œë“¤ê³   **Decoder**ëŠ” ì´ `context vector`ë¥¼ ë‹¤ì‹œ ê°€ë³€ ê¸¸ì´ì˜ `target sequence`ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

context vectorëŠ” ëª¨ë“  decoderì˜ ë…¸ë“œë“¤ì— ê´€ì—¬ë¥¼ í•˜ë©°, ë²ˆì—­ì´ ë¬¸ì¥ ë‹¨ìœ„ê°€ ì•„ë‹Œ, ë‹¨ì–´ë‚˜ êµ¬ë¬¸ ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ ë˜ê¸° ë•Œë¬¸ì— ì´ ëª¨ë¸ì€ í†µê³„ ê¸°ê³„ ë²ˆì—­(Statistical Machine Translation, SMT)ë¥¼ ë”°ë¥¸ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì¦‰, **RNN Encoder-Decoder**ëŠ” ê°€ë³€ ê¸¸ì´ì˜ inputê³¼ outputì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì„ í•™ìŠµí•˜ëŠ” ëª¨ë¸ì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$p(y_1,..,y_{T'}~\vert~x_1,...,x_T)$$

### Encoder

Encoderì€ RNNêµ¬ì¡°ë¡œ ë˜ì–´ìˆìœ¼ë©°, ì´ êµ¬ì¡°ë¥¼ ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

$$\mathbf{h}_{<t>}=f(\mathbf{h}_{<t-1>},x_t)$$

### Decoder

Decoder ì—­ì‹œ RNNêµ¬ì¡°ë¡œ ë˜ì–´ìˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ, decoderì˜ hidden stateì—ì„œëŠ” ì´ì „ì˜ outputì¸  $y_{t-1}$ê³¼ encoderì˜ ê²°ê³¼ì¸ context vectorë¥¼ ì¶”ê°€ inputìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.

$$\mathbf{h}_{<t>}=f(\mathbf{h}_{<t-1>}, y_{t-1}, \mathbf{c})$$

outputì„ ì¡°ê±´ë¶€ í™•ë¥ ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$p(y_t\vert y_{t-1}, y_{t-2},...,y_1,\mathbf{c})=g(\mathbf{h}_{<t>},y_{t-1},\mathbf{c})$$

- ì—¬ê¸°ì„œ $f, g$ ëŠ” softmaxì™€ ê°™ì€ activation functionì…ë‹ˆë‹¤.

---

Encoderì™€ Decoderë¡œ êµ¬ì„±ëœ **RNN Encoder-Decoder** ëŠ” ì•„ë˜ì˜ ì‹ì¸ `conditional log-likelihood`ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤.

$$\max_\theta\frac{1}{N}\sum^N_{n=1}\log p_\theta(\mathbf{y}_n\vert\mathbf{x}_n)$$


- ì´ë•Œ, $Î¸$ëŠ” ëª¨ë¸ì˜ parameterë¥¼ ëœ»í•˜ê³  $(\mathbf{x}_n, \mathbf{y}_n)$ëŠ” training dataì˜ input sequence, output sequence ìŒì…ë‹ˆë‹¤.

## GRU (Hidden Unit that Adaptively Remembers and Forgets)

ì´ ë…¼ë¬¸ì—ì„œëŠ” RNN Encoder-Decoder model(ì¼ëª… Seq2Seq)ì™¸ì—ë„ ë†€ë¼ìš´ architectureì¸ GRUë¥¼ ì œì‹œí–ˆìŠµë‹ˆë‹¤. ì´ëŠ” LSTMì„ ìˆ˜ì •í•œ ê²ƒìœ¼ë¡œ, LSTMê³¼ ë¹„ìŠ·í•œ ì¼ì„ í•˜ì§€ë§Œ ì—°ì‚°ì´ ë” ê°„ë‹¨í•˜ë©° êµ¬ì¡° ì—­ì‹œ ë” ê°„ë‹¨í•©ë‹ˆë‹¤.

GRUì˜ êµ¬ì¡°ë¥¼ ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

<p align="center"><img src="https://raw.githubusercontent.com/happy-jihye/Natural-Language-Processing/main/images/seq2seq11.png" width = "500" ></p>

ì´ì œë¶€í„°ëŠ” ìœ„ì˜ êµ¬ì¡°ì—ì„œ hidden unitì´ ì–´ë–»ê²Œ í™œì„±í™”ë˜ëŠ”ì§€ë¥¼ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

ì²«ë²ˆì§¸ë¡œ `reset gate` ì¸ $r_j$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤.

$$r_j=\sigma\big([\mathbf{W}_r\mathbf{x}]_j+[\mathbf{U}_r\mathbf{h}_{<t-1>}]_j \big)$$

- ì—¬ê¸°ì„œ $W_r, U_r$ì€ ê°€ì¤‘ì¹˜ ë²¡í„°ì´ë©°, $Ïƒ$ëŠ” logistic sigmoid functionì…ë‹ˆë‹¤.
- ì´ reset gateì˜ ê°’ì´ 0ì— ê°€ê¹Œì›Œì§€ë©´, ì´ì „ hidden state ê°’ì´ ë¬´ì‹œë˜ê³  í˜„ì¬ì˜ inputë§Œì´ hidden stateì— ì˜í–¥ì„ ì¤ë‹ˆë‹¤.

ë‘ë²ˆì§¸ë¡œ `update gate` ì¸ $z_j$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤.

$$z_j=\sigma\big([\mathbf{W}_z\mathbf{x}]_j+[\mathbf{U}_z\mathbf{h}_{<t-1>}]_j \big)$$

- update gateëŠ” ì–¼ë§ˆë‚˜ ë§ì€ ì •ë³´ë¥¼ updateí• ì§€ ê²°ì •í•˜ëŠ” ê°’ìœ¼ë¡œ LSTMì˜ `memory cell`ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤.

ìš°ë¦¬ëŠ” ì´ ë‘ê°œì˜ gateë¥¼ ì‚¬ìš©í•˜ì—¬ hidden unitì˜ ê°’ì„ ê³„ì‚°í•˜ë©°, ì´ëŠ” LSTMê³¼ ìœ ì‚¬í•˜ê²Œ ë™ì‘ì„ í•©ë‹ˆë‹¤.

$$\begin{matrix}
h_j^{<t>}=z_jh_j^{<t-1>}+(1-z_j)\tilde{h}_j^{<t>}\\ \\
\tilde{h}_j^{<t>}=\phi\big([\mathbf{W}\mathbf{x}]_j+[\mathbf{U}(\mathbf{r}\odot\mathbf{h}_{<t-1>})]_ j \big)
\end{matrix}$$

## Statistical Machine Translation

ê¸°ì¡´ì— í”íˆ ì‚¬ìš©ë˜ë˜ í†µê³„ì  ê¸°ê³„ ë²ˆì—­ ë°©ì‹ì€ ì£¼ì–´ì§„ ë¬¸ì¥ $e$ ì— ëŒ€í•œ `translation function` ì¸ $f$ë¥¼ ì°¾ëŠ” ê²ë‹ˆë‹¤. ì¦‰, ë‹¤ìŒì˜ ì‹ì„ ìµœëŒ€í™”í•˜ê¸° ìœ„í•œ ì‹ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$
p(\mathbf{f}\vert\mathbf{e})\propto p(\mathbf{e}\vert\mathbf{f})p(\mathbf{f})
$$

í•˜ì§€ë§Œ ì‹¤ì œ ê³„ì‚°ì—ì„œëŠ” $p(\mathbf{f}\vert\mathbf{e})$ ë³´ë‹¤ $log p(\mathbf{f}\vert\mathbf{e})$ë¥¼ ìµœëŒ€í™” í•˜ëŠ” ê²ƒì´ ì‰¬ìš°ë¯€ë¡œ ë‹¤ìŒì˜ ì‹ì„ ìµœëŒ€í™”í•©ë‹ˆë‹¤. 

$$\log p(\mathbf{f}\vert\mathbf{e})=\sum^N_{n=1}w_n f_n(\mathbf{f},\mathbf{e})+\log Z(\mathbf{e})$$

- ì—¬ê¸°ì„œ $f_n$ ì€ në²ˆì§¸ featureì´ë©°, $w_n$ ì€ ê°€ì¤‘ì¹˜ ì…ë‹ˆë‹¤. ê° ê°€ì¤‘ì¹˜ëŠ” BLEU scoreë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤.
- $Z(e)$ ëŠ” normalization ìƒìˆ˜ì…ë‹ˆë‹¤.


# 2. Code Practice

## Preparing Data


```python
!apt install python3.7
!pip install -U torchtext==0.6.0
!python -m spacy download en
!python -m spacy download de
```


```python
import torch
import torch.nn as nn
import torch.optim as optim

from torchtext.datasets import Multi30k
from torchtext.data import Field, BucketIterator

import spacy
import numpy as np

import random
import math
import time

SEED = 1234

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
```

### **Tokenizers**
- tokenizersëŠ” ë¬¸ì¥ì„ ê°œë³„ tokenìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
  - e.g. "good morning!" becomes ["good", "morning", "!"]
- nlpë¥¼ ì‰½ê²Œ í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” python packageì¸ `spaCy`ë¥¼ ì´ìš©í•˜ì—¬, tokení™”ë¥¼ í•  ì˜ˆì •ì…ë‹ˆë‹¤.



```python
spacy_de = spacy.load('de')
spacy_en = spacy.load('en')
```


```python
def tokenize_de(text):
  return [tok.text for tok in spacy_de.tokenizer(text)]

def tokenize_en(text):
  return [tok.text for tok in spacy_en.tokenizer(text)]
```

ë‹¤ìŒìœ¼ë¡œëŠ” **Field** ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. 


```python
SRC = Field(tokenize= tokenize_de,
            init_token = '<sos>',
            eos_token = '<eos>',
            lower = True)

TRG = Field(tokenize= tokenize_en,
            init_token = '<sos>',
            eos_token = '<eos>',
            lower = True)
```

- datasetìœ¼ë¡œëŠ” [Multi30k dataset](https://github.com/multi30k/dataset)ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” ì•½ 3ë§Œê°œì˜ ì˜ì–´, ë…ì¼ì–´, í”„ë‘ìŠ¤ì–´ ë¬¸ì¥ì´ ìˆëŠ” ë°ì´í„°ì´ë©° ê° ë¬¸ì¥ ë‹¹ 12ê°œì˜ ë‹¨ì–´ê°€ ìˆìŠµë‹ˆë‹¤.
- `exts`ëŠ” sourceì™€ targetìœ¼ë¡œ ì‚¬ìš©í•  ì–¸ì–´ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.


```python
train_data, valid_data, test_data = Multi30k.splits(exts= ('.de', '.en'),
                                                    fields = (SRC, TRG))
```

{:.output_stream}

```
downloading training.tar.gz

```

{:.output_stream}

```
training.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.21M/1.21M [00:01<00:00, 705kB/s]

```

{:.output_stream}

```
downloading validation.tar.gz

```

{:.output_stream}

```
validation.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46.3k/46.3k [00:00<00:00, 174kB/s]

```

{:.output_stream}

```
downloading mmt_task1_test2016.tar.gz

```

{:.output_stream}

```
mmt_task1_test2016.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66.2k/66.2k [00:00<00:00, 159kB/s]

```


```python
print(f"Number of training examples: {len(train_data.examples)}")
print(f"Number of validation examples: {len(valid_data.examples)}")
print(f"Number of testing examples: {len(test_data.examples)}")
```

{:.output_stream}

```
Number of training examples: 29000
Number of validation examples: 1014
Number of testing examples: 1000

```
```python
print(len(vars(train_data.examples[0])['src']))
print(len(vars(train_data.examples[1])['src']))

print(vars(train_data.examples[0]))
print(vars(train_data.examples[1]))
```

{:.output_stream}

```
13
8
{'src': ['zwei', 'junge', 'weiÃŸe', 'mÃ¤nner', 'sind', 'im', 'freien', 'in', 'der', 'nÃ¤he', 'vieler', 'bÃ¼sche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}
{'src': ['mehrere', 'mÃ¤nner', 'mit', 'schutzhelmen', 'bedienen', 'ein', 'antriebsradsystem', '.'], 'trg': ['several', 'men', 'in', 'hard', 'hats', 'are', 'operating', 'a', 'giant', 'pulley', 'system', '.']}

```

### Build Vocabulary
- `build_vocab`í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ê° tokenì„ indexingí•´ì¤ë‹ˆë‹¤. ì´ë•Œ, sourceì™€ targetì˜ vocabularyëŠ” ë‹¤ë¦…ë‹ˆë‹¤.
- `min_freq`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì†Œ 2ë²ˆ ì´ìƒ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ë§Œ vocabularyì— ë„£ì–´ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ë•Œ, í•œë²ˆë§Œ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ëŠ” `<unk>` tokenìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.
- ì´ë•Œ, vocabularyëŠ” **training set**ì—ì„œë§Œ ë§Œë“¤ì–´ì ¸ì•¼í•©ë‹ˆë‹¤. *(validation/test setì— ëŒ€í•´ì„œëŠ” ë§Œë“¤ì–´ì§€ë©´ ì•ˆë¨!!)* 


```python
SRC.build_vocab(train_data, min_freq = 2)
TRG.build_vocab(train_data, min_freq = 2)
```


```python
print(f"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}")
print(f"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}")
```

{:.output_stream}

```
Unique tokens in source (de) vocabulary: 7855
Unique tokens in target (en) vocabulary: 5893

```

### Create the iterators
- `BucketIterator`ë¥¼ ì´ìš©í•˜ì—¬ batch sizeë³„ë¡œ tokenë“¤ì„ ë¬¶ê³ , ì–´íœ˜ë¥¼ ì½ì„ ìˆ˜ ìˆëŠ” tokenì—ì„œ indexë¡œ ë³€í™˜í•´ì¤ë‹ˆë‹¤.


```python
# for using GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```


```python
BATCH_SIZE = 128

train_iterator, valid_iterator, test_iterator = BucketIterator.splits(
    (train_data, valid_data, test_data),
    batch_size = BATCH_SIZE,
    device = device
)
```

- ë‹¤ìŒì€ batch sizeê°€ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•´ ì´í•´í•´ë³´ê¸° ìœ„í•´ ì²«ë²ˆì§¸ batchë¥¼ ì¶œë ¥í•´ë³¸ ì˜ˆì œì…ë‹ˆë‹¤. `BucketIterator`ë¥¼ í†µí•´ batchë¼ë¦¬ ë¬¶ìœ¼ë©´ [sequence length, batch size]ë¼ëŠ” tensorê°€ ìƒì„±ë˜ë©°, ì´ tensorëŠ” train_dataë¥¼ batch_sizeë¡œ ë‚˜ëˆˆ ê²°ê³¼ê°’ë§Œí¼ ìƒì„±ë©ë‹ˆë‹¤.
  - ì´ ì˜ˆì œì—ì„œëŠ” 128ì˜ í¬ê¸°ë¥¼ ê°€ì§„ batchê°€ ì´ 227ê°œ ìƒê¹ë‹ˆë‹¤.
- ë˜í•œ, batchì—ì„œ `sequence length`ëŠ” ê·¸ batch ë‚´ì˜ ê°€ì¥ ê¸´ ë¬¸ì¥ì˜ ê¸¸ì´ë¡œ ê²°ì •ë˜ë©° ê·¸ë³´ë‹¤ ì§§ì€ ë¬¸ì¥ë“¤ì— ëŒ€í•´ì„œëŠ” `<pad>` tokenìœ¼ë¡œ ë‚¨ì€ tensorê°’ì´ ì±„ì›Œì§‘ë‹ˆë‹¤.


```python
print(TRG.vocab.stoi[TRG.pad_token]) #<pad> tokenì˜ index = 1

for i, batch in enumerate(train_iterator):
    src = batch.src
    trg = batch.trg

    src = src.transpose(1,0)
    print(f"ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ text í¬ê¸°: {src.shape}")
    print(src[0])
    print(src[1])

    break

print(len(train_iterator))
print(len(train_iterator)*128)
```

{:.output_stream}

```
1
ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ text í¬ê¸°: torch.Size([128, 33])
tensor([   2,    8,   67,  217,   12,   33,  214,    9,   35,   17,  101,   17,
         998,   20, 1787,   93,    4,    3,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1], device='cuda:0')
tensor([  2,  43,  41,  57, 215,   9,  14,   7, 555,   9,  18, 101,   7, 234,
          9,  22, 354,  14, 337, 119,  69,   4,   3,   1,   1,   1,   1,   1,
          1,   1,   1,   1,   1], device='cuda:0')
227
29056

```

## Building the Seq2Seq Model

### Encoder
- EncoderëŠ” 1ê°œì˜ GRU layerë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. LSTMê³¼ëŠ” ë‹¬ë¦¬ GRUì—ì„œëŠ” ê° dropoutì´ RNNì˜ ê° layerê°„ì— ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì— dropoutì„ GRUì˜ ì¸ìˆ˜ë¡œ ì£¼ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.

- ë˜í•œ, <u>GRUëŠ” LSTMê³¼ ë‹¬ë¦¬ cell stateë¥¼ RNN networkì˜ ì…ì¶œë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</u>

  $h_t = \text{GRU}(e(x_t), h_{t-1})$
  
  $(h_t, c_t) = \text{LSTM}(e(x_t), h_{t-1}, c_{t-1})$

  $h_t = \text{RNN}(e(x_t), h_{t-1})$

- Encoderì˜ ìµœì¢…ì‹ì„ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 

  $h_t = \text{EncoderGRU}^1(e(x_t), h_{t-1})$

- ë§ˆì§€ë§‰ RNNì„ ê±°ì¹˜ê³  ë‚˜ë©´, context vectorì¸ $z=h_T$ë¥¼ ì–»ê²Œ ë©ë‹ˆë‹¤.

<p align="center"><img src="https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/seq2seq7.png?raw=1" width = "700" ></p>


-  GRUëŠ” LSTMê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë‚´ì§€ë§Œ, ë©”ëª¨ë¦¬ë¥¼ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë“ˆë¡œ í˜„ì¬ì—ë„ LSTMì˜ ëŒ€ìš©ìœ¼ë¡œ ë§ì´ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤ :) GRUì˜ ì•„í‚¤í…ì²˜ì— ëŒ€í•´ì„œëŠ” [ì´ ê¸€](https://blog.floydhub.com/gru-with-pytorch/)ì„ ì°¸ê³ í•˜ì„¸ìš” :)


```python
class Encoder(nn.Module):
  def __init__(self, input_dim, emb_dim, hid_dim, dropout):
    super().__init__()

    self.hid_dim = hid_dim

    self.embedding = nn.Embedding(input_dim, emb_dim)

    self.rnn = nn.GRU(emb_dim, hid_dim)

    self.dropout = nn.Dropout(dropout)

  def forward(self, src):

    # src = [src len, batch size]
    embedded = self.dropout(self.embedding(src))

    # embedded = [src len, batch size, emb dim]

    ## cell stateê°€ ì—†ìŠµë‹ˆë‹¤ !
    outputs, hidden = self.rnn(embedded)

    # outputs = [src len, batch size, hid dim * n directions]
    # hidden = [n layers * n directions, batch size, hid dim]

    ## outputì€ ì–¸ì œë‚˜ hidden layerì˜ topì— ìˆìŠµë‹ˆë‹¤.

    return hidden
```

### Decoder
- decoderë„ encoderì™€ ìœ ì‚¬í•˜ì§€ë§Œ, í•œê°€ì§€ ë‹¤ë¥¸ ì ì€ ëª¨ë“  ë„¤íŠ¸ì›Œí¬ì— `context vector`ê°€ ê´€ì—¬í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤.
- GRUì— embedding vectorë¿ë§Œ ì•„ë‹ˆë¼ context vectorë„ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì—, GRUì˜ input dimensionì€ `emb_dim + hid_dim`ê°€ ë©ë‹ˆë‹¤.
- ë˜í•œ ìµœì¢… outputì˜ ì…ë ¥ì—ëŠ” context vector, hidden state, embedding vectorê°€ ê´€ì—¬í•˜ë¯€ë¡œ dimensionì´ `emb_dim + hid_dim * 2`ì…ë‹ˆë‹¤.

  <p align="center"><img src="https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/seq2seq8.png?raw=1" width = "400" ></p>

- ë‹¤ìŒì€ Decoderì˜ layerë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ì‹ì…ë‹ˆë‹¤.

  $s_t = \text{DecoderGRU}(d(y_t), s_{t-1}, z))$

  $\hat{y}_{t+1} = f(d(y_t), s_t, z)$


```python
class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim, dropout):
        super().__init__()

        self.output_dim = output_dim
        self.hid_dim = hid_dim
        
        self.embedding = nn.Embedding(output_dim, emb_dim)
        
        # input : context vec + embedding vec
        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)
        
        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, input, hidden, context):
        
        # input = [batch size]
        ## í•œë²ˆì— í•˜ë‚˜ì˜ tokenë§Œ decodingí•˜ë¯€ë¡œ forwardì—ì„œì˜ input tokenì˜ ê¸¸ì´ëŠ” 1ì…ë‹ˆë‹¤.
        
        # hidden = [n layers * n directions, batch size, hid dim]
        # cell = [n layers * n directions, batch size, hid dim]
        
        #n layers and n directions in the decoder will both always be 1, therefore:
        # hidden = [1, batch size, hid dim]
        # context = [1, batch size, hid dim]
        
        input = input.unsqueeze(0)
        
        # inputì„ 0ì°¨ì›ì— ëŒ€í•´ unsqueezeí•´ì„œ 1ì˜ sentence length dimensionì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        # input = [1, batch size]
        
        embedded = self.dropout(self.embedding(input))
        
        # embedding layerë¥¼ í†µê³¼í•œ í›„ì— dropoutì„ í•©ë‹ˆë‹¤.
        # embedded = [1, batch size, emb dim]
                
        emb_con = torch.cat((embedded, context), dim = 2)
        
        # emb_con = [1, batch size, emb dim + hid dim]

        output, hidden = self.rnn(emb_con, hidden)

        # output = [seq len, batch size, hid dim * n directions]
        # hidden = [n layers * n directions, batch size, hid dim]
        
        # seq len and n directions will always be 1 in the decoder, therefore:
        # output = [1, batch size, hid dim]
        # hidden = [1, batch size, hid dim]

        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim = 1)
        
        # output = [batch size, emb dim + hid dim * 2]

        prediction = self.fc_out(output)
        
        #prediction = [batch size, output dim]
        
        return prediction, hidden

```

## Seq2Seq

seq2seq modelì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
- encoderì— source(input) sentenceë¥¼ ì…ë ¥í•œë‹¤.
- encoderë¥¼ í•™ìŠµì‹œì¼œ ê³ ì •ëœ í¬ê¸°ì˜ context vectorë¥¼ ì¶œë ¥í•œë‹¤.
- context vectorë¥¼ decoderì— ë„£ì–´ ì˜ˆì¸¡ëœ target(output) sentenceë¥¼ ìƒì„±í•œë‹¤.

  ![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/seq2seq9.png?raw=1)



```python
class Seq2Seq(nn.Module):

    def __init__(self, encoder, decoder, device):
        super().__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        
        assert encoder.hid_dim == decoder.hid_dim, \
            "Hidden dimensions of encoder and decoder must be equal!"
        
    def forward(self, src, trg, teacher_forcing_ratio = 0.5):
        
        #src = [src len, batch size]
        #trg = [trg len, batch size]
        #teacher_forcing_ratio is probability to use teacher forcing
        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time
        
        batch_size = trg.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim
        
        # outputì„ ì €ì¥í•  tensorë¥¼ ë§Œë“­ë‹ˆë‹¤.(ì²˜ìŒì—ëŠ” ì „ë¶€ 0ìœ¼ë¡œ)
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)
        
        # srcë¬¸ì¥ì„ encoderì— ë„£ì€ í›„ context vectorë¥¼ êµ¬í•©ë‹ˆë‹¤.
        context = self.encoder(src)
        
        # decoderì˜ initial hidden stateëŠ” context vectorì…ë‹ˆë‹¤.
        hidden = context

        # decoderì— ì…ë ¥í•  ì²«ë²ˆì§¸ inputì…ë‹ˆë‹¤.
        # ì²«ë²ˆì§¸ inputì€ ëª¨ë‘ <sos> tokenì…ë‹ˆë‹¤.
        # trg[0,:].shape = BATCH_SIZE 
        input = trg[0,:]  
        
        
        '''í•œë²ˆì— batch_sizeë§Œí¼ì˜ tokenë“¤ì„ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°
        ì¦‰, ì´ trg_lenë²ˆì˜ forë¬¸ì´ ëŒì•„ê°€ë©° ì´ forë¬¸ì´ ë‹¤ ëŒì•„ê°€ì•¼ì§€ë§Œ í•˜ë‚˜ì˜ ë¬¸ì¥ì´ decodingë¨
        ë˜í•œ, 1ë²ˆì˜ forë¬¸ë‹¹ 128ê°œì˜ ë¬¸ì¥ì˜ ê° tokenë“¤ì´ ë‹¤ê°™ì´ decodingë˜ëŠ” ê²ƒ'''
        for t in range(1, trg_len):
            
            # input token embeddingê³¼ ì´ì „ hidden stateì™€ context stateë¥¼ decoderì— ì…ë ¥í•©ë‹ˆë‹¤.
            # ìƒˆë¡œìš´ hidden stateì™€ ì˜ˆì¸¡ outputê°’ì´ ì¶œë ¥ë©ë‹ˆë‹¤.
            output, hidden = self.decoder(input, hidden, context)

            #output = [batch size, output dim]

            # ê°ê°ì˜ ì¶œë ¥ê°’ì„ outputs tensorì— ì €ì¥í•©ë‹ˆë‹¤.
            outputs[t] = output
            
            # decide if we are going to use teacher forcing or not
            teacher_force = random.random() < teacher_forcing_ratio
            
            # predictionsë“¤ ì¤‘ì— ê°€ì¥ ì˜ ì˜ˆì¸¡ëœ tokenì„ topì— ë„£ìŠµë‹ˆë‹¤.
            # 1ì°¨ì› ì¤‘ ê°€ì¥ í° ê°’ë§Œì„ top1ì— ì €ì¥í•˜ë¯€ë¡œ 1ì°¨ì›ì€ ì‚¬ë¼ì§‘ë‹ˆë‹¤.
            top1 = output.argmax(1) 
            # top1 = [batch size]
            
            # teacher forcingê¸°ë²•ì„ ì‚¬ìš©í•œë‹¤ë©´, ë‹¤ìŒ inputìœ¼ë¡œ targetì„ ì…ë ¥í•˜ê³ 
            # ì•„ë‹ˆë¼ë©´ ì´ì „ stateì˜ ì˜ˆì¸¡ëœ ì¶œë ¥ê°’ì„ ë‹¤ìŒ inputìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
            input = trg[t] if teacher_force else top1
        
        return outputs
```


## Training the Seq2Seq Model


```python
INPUT_DIM = len(SRC.vocab)
OUTPUT_DIM = len(TRG.vocab)
ENC_EMB_DIM = 256
DEC_EMB_DIM = 256
HID_DIM = 512
ENC_DROPOUT = 0.5
DEC_DROPOUT = 0.5

enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)

model = Seq2Seq(enc, dec, device).to(device)
```

- ì´ˆê¸° ê°€ì¤‘ì¹˜ê°’ì€ $\mathcal{N}(0, 0.01)$ì˜ ì •ê·œë¶„í¬ë¡œë¶€í„° ì–»ì—ˆìŠµë‹ˆë‹¤.


```python
def init_weights(m):
    for name, param in m.named_parameters():
        nn.init.normal_(param.data, mean = 0, std = 0.01)
        
model.apply(init_weights)
```




{:.output_data_text}

```
Seq2Seq(
  (encoder): Encoder(
    (embedding): Embedding(7855, 256)
    (rnn): GRU(256, 512)
    (dropout): Dropout(p=0.5, inplace=False)
  )
  (decoder): Decoder(
    (embedding): Embedding(5893, 256)
    (rnn): GRU(768, 512)
    (fc_out): Linear(in_features=1280, out_features=5893, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
```




```python
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'The model has {count_parameters(model):,} trainable parameters')
```

{:.output_stream}

```
The model has 14,220,293 trainable parameters

```

- optimizerí•¨ìˆ˜ë¡œëŠ” `Adam`ì„ ì‚¬ìš©í•˜ì˜€ê³ , loss functionìœ¼ë¡œëŠ” `CrossEntropyLoss`ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, `<pad>` tokenì— ëŒ€í•´ì„œëŠ” loss ê³„ì‚°ì„ í•˜ì§€ ì•Šë„ë¡ ì¡°ê±´ì„ ë¶€ì—¬í–ˆìŠµë‹ˆë‹¤.


```python
optimizer = optim.Adam(model.parameters())
```


```python
TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]

criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)
```

### Training


```python
def train(model, iterator, optimizer, criterion, clip):
    
    model.train()
    
    epoch_loss = 0
    
    for i, batch in enumerate(iterator):
        
        src = batch.src
        trg = batch.trg
        
        optimizer.zero_grad()
        
        output = model(src, trg)
        
        #trg = [trg len, batch size]
        #output = [trg len, batch size, output dim]
        
        output_dim = output.shape[-1]
        
        output = output[1:].view(-1, output_dim)
        trg = trg[1:].view(-1)
        
        #trg = [(trg len - 1) * batch size]
        #output = [(trg len - 1) * batch size, output dim]
        
        loss = criterion(output, trg)
        
        loss.backward()
        
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        
        optimizer.step()
        
        epoch_loss += loss.item()
        
    return epoch_loss / len(iterator)
```

### Evaluation


```python
def evaluate(model, iterator, criterion):
    
    model.eval()
    
    epoch_loss = 0
    
    with torch.no_grad():
    
        for i, batch in enumerate(iterator):

            src = batch.src
            trg = batch.trg

            output = model(src, trg, 0) #turn off teacher forcing

            #trg = [trg len, batch size]
            #output = [trg len, batch size, output dim]

            output_dim = output.shape[-1]
            
            output = output[1:].view(-1, output_dim)
            trg = trg[1:].view(-1)

            #trg = [(trg len - 1) * batch size]
            #output = [(trg len - 1) * batch size, output dim]

            loss = criterion(output, trg)
            
            epoch_loss += loss.item()
        
    return epoch_loss / len(iterator)
```


```python
def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs
```

### Train the model through multiple epochsPermalink


```python
N_EPOCHS = 10
CLIP = 1

best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):
    
    start_time = time.time()
    
    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)
    valid_loss = evaluate(model, valid_iterator, criterion)
    
    end_time = time.time()
    
    epoch_mins, epoch_secs = epoch_time(start_time, end_time)
    
    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'tut2-model.pt')
    
    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')
```

{:.output_stream}

```
Epoch: 01 | Time: 0m 36s
	Train Loss: 5.041 | Train PPL: 154.550
	 Val. Loss: 5.141 |  Val. PPL: 170.908
Epoch: 02 | Time: 0m 36s
	Train Loss: 4.377 | Train PPL:  79.604
	 Val. Loss: 5.104 |  Val. PPL: 164.637
Epoch: 03 | Time: 0m 36s
	Train Loss: 4.060 | Train PPL:  58.001
	 Val. Loss: 4.731 |  Val. PPL: 113.397
Epoch: 04 | Time: 0m 37s
	Train Loss: 3.766 | Train PPL:  43.194
	 Val. Loss: 4.479 |  Val. PPL:  88.112
Epoch: 05 | Time: 0m 36s
	Train Loss: 3.473 | Train PPL:  32.222
	 Val. Loss: 4.165 |  Val. PPL:  64.397
Epoch: 06 | Time: 0m 36s
	Train Loss: 3.213 | Train PPL:  24.857
	 Val. Loss: 3.995 |  Val. PPL:  54.303
Epoch: 07 | Time: 0m 37s
	Train Loss: 2.993 | Train PPL:  19.937
	 Val. Loss: 3.856 |  Val. PPL:  47.268
Epoch: 08 | Time: 0m 37s
	Train Loss: 2.726 | Train PPL:  15.267
	 Val. Loss: 3.880 |  Val. PPL:  48.448
Epoch: 09 | Time: 0m 37s
	Train Loss: 2.543 | Train PPL:  12.714
	 Val. Loss: 3.810 |  Val. PPL:  45.146
Epoch: 10 | Time: 0m 36s
	Train Loss: 2.352 | Train PPL:  10.511
	 Val. Loss: 3.768 |  Val. PPL:  43.309

```


```python
model.load_state_dict(torch.load('tut2-model.pt'))

test_loss = evaluate(model, test_iterator, criterion)

print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')
```

{:.output_stream}

```
| Test Loss: 3.703 | Test PPL:  40.569 |

```
