---
title: "[3-2] A Survey of Large Language Models - Adaptation of LLMs"
excerpt: " "

categories: 
  - nlp
tags: 
  - deeplearning
  - ai
  - nlp
  - LLM

search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---


## [1] Instruction Tuning

### 1.1 Formatted Instance Construction

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/23-llm/0.png?raw=1" width = "700" ></p>

- Formatting NLP Task Datasets: 기존의 NLP task dataset에 자연어 task description을 추가
- Formatting Daily Chat Data: 실제 사용자가 제출한 query를 task description으로 사용하고, 사람이 직접 이에 대한 답변을 작성
- Formatting Synthetic Data: 기존 instance를 LLM에 입력하여 다양한 task description과 instance를 자동으로 생성

**Key Factors for Instance Construction.**

- Scaling the instructions.
    - task 수를 늘리면 일반화 능력이 향상되지만, 일정 수준 이상이 되면 추가 이득이 미미함
- Formatting design
    - task description이 가장 중요한 요소이며, 적절한 수의 demonstration을 사용하는 것도 도움됨
- diversity와 quality가 quantity보다 더 중요함

### 1.2 Instruction Tuning Strategies

- 서로 다른 task의 비율 균형을 맞추는 것이 중요함
- Pre-training data를 활용하여 regularization 효과를 얻을 수 있음
- Task-formatted instruction과 daily chat instruction 데이터의 균형을 맞추기 위해 multi-stage tuning 전략을 사용할 수 있음
- Multi-turn chat data의 경우 전체 대화를 한 번에 입력하고 chatbot response에 대해서만 loss를 계산하는 것이 효율적
- LLM의 정체성 정보를 포함하는 instruction을 만들어 fine-tuning하는 것이 실제 애플리케이션 배포에 도움될 수 있음

### 1.3 The Effect of Instruction Tuning

- 적절한 양의 instance로 tuning해도 성능이 크게 향상됨
- 특정 task에 대한 demonstration 없이도 자연어 지시를 이해하고 수행하는 능력을 갖게 됨
- 의료, 법률 등 특정 도메인에 특화된 전문가 모델을 만드는데 효과적임

### 1.4 Empirical Analysis for Instruction Tuning

**Instruction Datasets**

1. Task-specific instructions:
    - FLAN-T5 데이터셋 사용
    - 1,836개의 task와 1,500만개 이상의 instruction으로 구성
    - 비교를 위해 80,000개의 instruction을 무작위로 샘플링
2. Daily chat instructions:
    - ShareGPT instruction set 사용
    - 63,000개의 실제 사용자 instruction으로 구성
    - Vicuna의 핵심 instruction으로 사용됨
3. Synthetic instructions:
    - Self-Instruct-52K 데이터셋 사용
    - 52,000개의 instruction과 82,000개의 instance input-output 쌍으로 구성
    - 생성된 instruction이 사람이 작성한 seed task와 유사한 분포를 가짐

**Improvement Strategies**

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/23-llm/1.png?raw=1" width = "800" ></p>

- Instruction complexity 향상
- topic diversity를 키움
- instruction 수를 늘림
- instruction difficulty를 조절 (난이도 균형을 맞추기)

**Results & Analysis**

- Task-specific instruction은 QA에는 적합하지만 chat에는 부적합할 수 있음
- 서로 다른 종류의 instruction을 혼합하면 LLM의 종합적 능력 향상에 도움됨
- Instruction의 complexity와 diversity를 높이면 성능 향상으로 이어짐
- 단순히 instruction 수를 늘리는 것은 큰 도움이 되지 않으며, 난이도 균형을 맞추는 것도 항상 좋은 것은 아님
- 더 큰 모델일수록 더 나은 instruction following 성능을 보임

---

## [2] Alignment Tuning

**Alignment Criteria**

- Helpfulness: 사용자의 작업을 효율적으로 해결하고, 필요한 경우 적절한 수준의 민감성과 신중함을 보여야 함
- Honesty: 정확한 내용을 제시하고, 불확실성에 대해 적절하게 전달해야 함
- Harmlessness: 공격적이거나 차별적인 언어를 생성하지 않아야 하며, 위험한 행동을 거절할 수 있어야 함

### 2.1 Collecting Human Feedback

**Human Labeler Selection**

- Labeler는 높은 수준의 교육을 받고 영어에 능숙해야 함
- Researcher의 의도와 human labeler 간의 불일치를 해결하기 위해 screening process를 거쳐 높은 agreement를 보이는 labeler 선택
- "Super rater"를 사용하여 human feedback의 높은 품질 보장

**Human Feedback Collection**

- Ranking-based approach.: 후보 출력을 비교하여 선호도 순위를 도출
- Question-based approach: Researcher가 설계한 특정 질문에 대한 답변을 통해 상세한 피드백 제공
- Rule-based approach: 일련의 규칙을 사용하여 alignment 기준을 충족하는지 테스트하고 평가 수집

### 2.2  Reinforcement Learning from Human Feedback (RLHF)

- Human feedback을 통한 강화학습(RLHF)은 수집된 human feedback 데이터로 LLM을 fine-tuning하여 alignment 기준(helpfulness, honesty, harmlessness 등)을 개선하는 데 사용됨
- RLHF 시스템은 정렬될 pretrained LM, human feedback에서 학습하는 reward model, LM을 학습시키는 RL 알고리즘으로 구성됨

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/23-llm/2.png?raw=1" width = "500" ></p>

- RLHF의 주요 단계
    1. Supervised fine-tuning
        - LM이 원하는 동작을 수행하도록 paired prompt-output 데이터셋으로 초기 fine-tuning
        - prompt-output: 다양한 task들로 human labeler rk wlrwjq wpwkr
    2. Reward model training
        - LM이 생성한 출력 텍스트에 대해 human labeler가 선호도를 annotation하고, 이를 바탕으로 reward model 학습
        - InstructGPT에서는 6B GPT-3 모델로 RM을 학습
        - reinforcement learning from AI feedback (RLAIF)
            - 최근에는 human labeler 대신 AI agent가 preference annotation을 직접 함
    3. RL fine-tuning
        - RL 알고리즘을 통해 Pretrained LM을 alignment
        - reward model의 신호를 사용하여 [PPO (Proximal Policy Optimization)](https://velog.io/@fragrance_0/AI-Dict-5-2.-Natural-Language-Processing-%EB%AA%A8%EB%8D%B8) 알고리즘으로 최적화
- RLHF의 실용적인 전략
    - Effective reward model training
        - (1) InstructGPT는 6B GPT model, 즉 작은 모델을 사용했다면, 최근에는 reward model 학습 시 큰 모델을 사용 → LLM의 output을 더 잘 판단
        - (2) reward 모델을 pretraining된 checkpoint로 초기화
            - LLaMA2-chat
                - reword model로 pretrained chat model checkpoints를 사용 → aligned 된 모델과 reward 모델 간의 정보격차를 극복
        - (3) regularization 적용
            - large-scale reward model 을 binary classification task (응답이 좋은지 나쁜지만을 평가)로만 학습하면 overfitting이 될 수 있음 → humman-annotated alignment dataset의 input prompt에 대한 적절한 답변을 생성하도록 모델을 학습한 후 LM loss로 업데이트하면, regularization의 효과를 볼 수 있음 (overfitting 방지)
        - (4) 다중 reward model 학습: alignment criteria 가 여러개이므로 각 기준 별 reward 모델을 학습
    - Effective RL training
        - RL 학습에서 supervised fine-tuning으로 사전 학습, rejection sampling 활용, 여러 번 반복 수행 등
- **Process-supervised RLHF**
    - reward 점수를 어떻게 잘줄지?
    - **Outcome-supervision signals**
        - LLM이 생성한 전체 텍스트의 품질을 정량적 점수로 평가
    - **Process-supervision signals**
        - process-supervised reward models (PRM)
        - 생성된 콘텐츠 내의 각 개별 구성 요소(문장, 단어, 추론 단계 등)에 대한 평가를 제공하여 미세한 수준의 supervision signal을 통해 학습을 가이드하고 원치 않는 생성 내용을 개선
        - **OpenAI의 PRM800k 데이터셋**
            - 12K개의 process-annotated 수학 문제(MATH 데이터셋)와 이에 대한 LLM이 생성한 75K개의 솔루션으로 구성
            - 각 수학 문제의 추론 단계가 positive, negative, neutral로 레이블링됨
            - Process-supervised reward model (PRM) 학습에 활용되며, 각 레이블의 예측 확률이 RLHF 과정에서 supervision signal로 사용됨
        - **Expert Iteration을 통한 Process-Supervision Signals 활용**
            - **Policy Improvement Stage**: Expert policy가 체계적인 검색 절차를 통해 샘플을 생성하고, PRM이 제공하는 process-supervision signal이 expert policy를 가이드하여 샘플의 품질을 향상시킴
            - **Distillation Stage**: Policy improvement stage에서 expert policy가 생성한 샘플을 사용하여 supervised fine-tuning을 통해 base policy를 개선

### 2.3   Alignment without RLHF

**RLHF의 단점**

- 여러 LM 을 학습시켜야함 → 리소스 많이 듦
- RLHF의 PPO 알고리즘은 복잡하고 hyperparameter에 민감함
- → 최근에는 RL 없이 supervised fine-tuning (SFT) 만으로 align을 맞추는 연구들이 많음

**Alignment Data Collection**

- Reward model based approaches: 기존 reward models을 활용하여 high-quality responses를 선택하고 supervised fine-tuning에 사용
- LLM based generative approaches: 강력한 LLMs을 활용하여 human-aligned data를 자동으로 생성
- LLM based interactive approaches: LLM agents로 구성된 simulated interaction environment를 구축하여 외부 feedback signals을 통해 스스로 개선

**Supervised Alignment Tuning**

- Primary training objective: Alignment data를 기반으로 전통적인 cross-entropy loss를 사용하여 sequence-to-sequence learning 수행
- Auxiliary optimization objectives: Primary cross-entropy loss 외에도 ranking loss나 contrastive learning을 사용하여 alignment data로부터의 학습을 강화

### 2.4 Remarks on SFT and RLHF

**Overall Comparison with RL Formulation**

- RLHF와 SFT는 LLMs을 위한 두 가지 주요 adaptation tuning 방법임
- RLHF는 reward model을 먼저 학습한 다음, RL training을 통해 LLM을 개선함
- SFT는 teacher-forcing 접근 방식을 채택하여 demonstration output의 likelihood를 직접 최적화함

**Pros and Cons of SFT**

- SFT는 다양한 benchmarks에서 LLMs의 성능을 향상시키는 효과적인 방법으로 입증됨
- SFT는 주로 abilities를 unlock하지만 새로운 abilities를 주입하지는 않음
- Demonstration data가 LLMs의 지식이나 능력 범위를 벗어나면 hallucination 행동을 조장할 수 있음
- 고품질의 instruction data가 SFT 단계에서 LLMs의 효과적인 학습을 위한 주요 요인임

**Pros and Cons of RLHF**

- RLHF는 유해한 responses를 완화하고 model capacity를 향상시키는 데 효과적임
- RLHF는 annotators 간의 불일치를 크게 완화할 수 있음
- RLHF는 본질적으로 LLMs이 스스로 생성한 responses를 대조하여 올바른 policies를 학습하도록 장려함
- RLHF는 고전적인 RL 알고리즘의 단점을 상속받음 (예: sample inefficiency, training instability)
- RLHF는 복잡한 반복 최적화 프로세스에 human annotators가 관여하며, 많은 중요한 세부 사항들이 전체 모델 성능에 중요한 영향을 미침

### 2.5 RLHF 대체 방법론

참고

https://tech.scatterlab.co.kr/luda-rlhf/

https://tech.scatterlab.co.kr/alt-rlhf/

- 학습시 RLHF보다 더 적은 수의 모델을 이용하여 사용량이 적으면서도 학습 속도가 빠르게 최적화한다.
- 간단한 학습 방식과 더 적은 하이퍼파라미터 튜닝으로 안정적으로 학습이 진행된다.
- 기존 RLHF 통해 fine-tuning한 모델보다 더 좋은 성능을 낸다.

**Rejection Sampling (Best of N)**

- prompt가 주어지면 LLM이 N개의 output을 샘플링 → 리워드 모델이 점수 계산
- 그 중 점수가 가장 높은 샘플로 SFT

**RRHF (Rank Responses to Align Language Models with Human Feedback without tears)**

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/23-llm/3.png?raw=1" width = "700" ></p>

- 여러 모델을 사용하여 답변을 구축해둠 (Policy 모델 `- 학습해야할 타켓 모델` 외에도 ChatGPT, GPT4, 사람 등 다양한 모델 사용 가능) → Reward 모델을 통해 랭킹을 맥임
- ranking loss를 최소화하는 방향으로 학습 → 랭킹이 높은 (리워드가 높은) 답변의 확률은 높아지고, 랭킹이 낮은 답변의 확률은 낮아지도록 학습

**SLiC-HF: Sequence Likelihood Calibration with Human Feedback**

- RRHF와 비슷. 다만, 단일 답변 후보를 인풋으로 받는 방식 (point-wise reward model)이 아니라 두 답변 후보를 입력으로 받아서 어느 답변이 더 좋은지를 계산 (pair-wise reward model)
- 우열을 메긴 답변 후보쌍들을 기반으로 Rank Calibration Loss를 구함 → loss가 최소화하는 방향으로 학습 → 랭킹이 높은 답변의 확률이 높아지게 학습

**DPO (Direct Preference Optimization)**

- 리워드 모델 학습용 데이터셋을 직접 사용하여 positive 답변에 대한 확률은 높아지도록, negative 답변에 대한 확률은 낮아지도록 학습
- 학습할 때 리워드 모델을 사용하지 않음. 레퍼런스 모델만을 사용
- RLHF 보다 안정적

---

## [3] Parameter-Efficient Model Adaption

### 3.1 Parameter-Efficient Fine-Tuning Methods (PEFT)

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/23-llm/4.png?raw=1" width = "800" ></p>

- [a] Adapter tuning
    
    <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/23-llm/5.png?raw=1" width = 550" ></p>
    
    - 작은 neural network module(adapter)을 Transformer에 통합하여 파라미터 수를 줄임
    - 기존 LM의 파라미터는 freezing (Near-identity), adapter module만을 추가한 후 fine-tuning
- [b] [Prefix tuning](https://ffighting.net/deep-learning-paper-review/language-model/prefix-tuning/)
    - prompt만 잘 구성해줘도 LLM의 성능을 크게 개선할 수 있다. → prompt prefix만을 학습
    - trainable continuous vector(prefix)를 각 Transformer layer에 추가하여 파라미터 효율적으로 최적화함
        - prefix vector를 바로 학습하는 것보다 parameterization 기법을 사용하는게 더 효과적이었다고 함. (MLP 함수를 학습하여 작은 matrix를 prefix의 parameter matrix에 매핑하는 reparameterization trick)
    - 학습 후에는 MLP는 사용하지 않고 오직 prefix-vector 만을 사용 → task-specific performance 향상
    - **P-tuning v2**
        - Transformer 에 layer-wise prompt vector를 추가하여 multi-task learning이 가능하도록 함
- [c] Prompt tuning
    - trainable prompt vector를 input layer에 통합하여 downstream task에 적용함
    - 개별 테스크마다 prompt의 파라미터를 업데이트하는 soft prompt의 학습방식 사용
    - embedding 공간에서 프롬프트를 최적화하는 방식
- [d] LoRA
    - FT 할때, trainable parameter 수를 줄이기 위해 각 dense layer의 update matrix를 low-rank constraint로 근사함
    - Parameter matrix W를 최적화하는 경우를 고려할 때, 업데이트 과정은 W ← W + ∆W와 같이 일반적인 형태로 쓸 수 있음
    - LoRA의 기본 아이디어는 원래 matrix W ∈ R^(m×n)를 고정하고, parameter update ∆W를 low-rank decomposition matrix로 근사하는 것
        - 즉, ∆W = A · B^⊤, 여기서 A ∈ R^(m×k) 및 B ∈ R^(n×k)는 task adaptation을 위한 trainable parameter이고, k ≪ min(m, n)는 reduced rank임
    - LoRA는 메모리와 저장 공간 사용량(예: VRAM)을 크게 절약할 수 있음

### 3.2 Parameter-Efficient Fine-Tuning on LLMs

- LLM의 등장으로 downstream task에 대한 효율적인 adaptation 방법에 대한 연구가 증가하고 있음
- 특히 LoRA가 LLaMA, BLOOM 등의 open-source LLM에 널리 적용되고 있음
    - Alpaca-LoRA는 7B LLaMA를 사람의 instruction에 fine-tuning한 Alpaca의 경량화 버전임
    - LLaMA-Adapter는 각 Transformer layer에 learnable prompt vector를 삽입하고, zero-initialized attention을 통해 training을 개선함
- GPT-J, BLOOM, LLaMA에 대해 여러 efficient tuning 방법을 비교한 실험 연구도 있었음
- PEFT 라이브러리는 여러 efficient tuning 방법을 구현하여 제공하고 있음
- 기존 연구는 주로 작은 pre-trained language model에 대해 수행되었으며, LLM에 대한 광범위한 연구는 아직 부족함

## [4] Memory-Efficient Model Adaptation

- LLM은 inference 시 많은 메모리 footprint를 차지하여 실제 적용이 어려움
- 모델 quantization을 통해 LLM의 메모리 사용량을 줄이는 방법에 대해 논의함

### 4.1 Background for Quantization

- Quantization은 보통 neural network의 weight와 activation을 floating-point number에서 integer로 매핑하는 과정을 의미함
- INT8 quantization이 많이 사용되며, 간단한 quantization 함수를 예시로 들어 설명함

### 4.2 Quantization Methods for LLMs

- Quantization-aware training(QAT)과 post-training quantization(PTQ)의 두 가지 주요 quantization 방식이 있음
- LLM은 parameter 수가 많아 PTQ 방식이 선호되며, activation의 큰 outlier로 인해 quantization이 어려움
- **post-training quantization(PTQ)**
    1. Mixed-precision decomposition (LLM.int8())
        - overflow를 방지하기 위한 방법
        - LLM.int8()은 outlier가 있는 차원과 나머지 차원을 분리하여 mixed-precision decomposition을 수행함 (각각 fp16과 int8로 계산)
    2. Fine-grained quantization
        - ZeroQuant는 activation에 token-wise quantization, weight에 group-wise quantization을 적용함
        - 일반적으로 그룹 크기 128을 사용
    3. SmoothQuant
        - weight와 activation 간의 quantization difficulty를 조절하기 위해 스케일링 변환을 함
    4. GPTQ와 AWQ
        - layerwise quantization 방법을 개선하여 매우 큰 모델도 3~4 bit precision으로 quantization이 가능하게 함
- QLoRA는 quantized model에 작은 tunable adapter를 추가하여 효율적이고 고성능의 quantization이 가능하게 함
- LLaMA에 data-free distillation을 적용한 QAT 방법도 연구되었음

### 4.3 Empirical Analysis and Findings

- Quantization은 LLM 배포 시 메모리 사용량과 지연 시간을 줄이는 데 많이 사용되고 있음
- 기존 연구에서 quantization이 LLM 성능에 미치는 영향에 대한 주요 발견을 요약함
    - INT8 weight quantization은 성능 저하 없이 메모리 사용량을 크게 줄일 수 있음
    - Activation은 weight보다 quantization이 어려움
    - Efficient fine-tuning을 활용한 quantization 방법이 quantized LLM의 성능을 높이는 데 효과적임
- 직접 quantization 실험을 통해 quantized model의 inference 성능을 분석한 결과를 제시함