---
title: "[Paper Review] An Image is Worth 16X16 Words: Transformers for Image Recognition at Scale ë…¼ë¬¸ ë¶„ì„"
excerpt: " "

categories: 
  - nlp
tags: 
  - deeplearning
  - ai
  - nlp
  - vision
  - transformer
layout: jupyter
search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 

---


> ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” NLPì˜ Transformer modelì„ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì— ì ìš©í•œ **Vision Transformer (ViT)** ë…¼ë¬¸ì„ ë¦¬ë·°í•  ì˜ˆì •ì´ë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ViTë¥¼ SoTAì¸ CNN ëª¨ë¸ê³¼ê³¼ ë¹„êµí–ˆì„ ë•Œ í›Œë¥­í•œ ê²°ê³¼ë¥¼ ëƒˆë‹¤ê³  ì£¼ì¥í•œë‹¤.
>
> ì£¼ìš” taskëŠ” image classificationì´ë©° pre-trained ëœ large-scaleì˜ dataë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.

- Paper : [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) (2020 / Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby)

## 1. Introduction

[Transformer model(Vaswani et al, 2017)](https://arxiv.org/abs/1706.03762)ì€ NMT(Neural Machine Translation)ì„ ìœ„í•´ ì œì•ˆë˜ì—ˆìœ¼ë©°, ì•„ì§ê¹Œì§€ë„ NLP ë¶„ì•¼ì—ì„œ ì§€ë°°ì ì´ë‹¤. TransformerëŠ” ì—°ì‚°ì´ íš¨ìœ¨ì ì´ê³ , í™•ì¥ì„±ì´ ì¢‹ê¸° ë•Œë¬¸ì— ì•„ì£¼ í° dataì˜ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í–ˆê³ *(100Bì˜ parameter)*, í˜„ì¬ SoTAì¸ BERT, GPT ë“±ì˜ architectureë“¤ì€ ëª¨ë‘ transformerë¥¼ ë°œì „ì‹œì¼œ ì•„ì£¼ ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ì„ ë§Œë“¤ì—ˆë‹¤.

NLPë¶„ì•¼ì—ì„œ attentionì´ ë‘ê°ì„ ë‚˜íƒ€ë‚´ì, computer visionì—ë„ ì´ë¥¼ ì ìš©í•˜ìëŠ” ì›€ì§ì„ì´ ë‚˜íƒ€ë‚¬ë‹¤. 

- **combing CNN-like architectures with self-attention** : [Non-local neural networks(2018)](https://arxiv.org/abs/1711.07971), [End-to-end object detection with transformers(2020)](https://arxiv.org/abs/2005.12872)
- **replacing the convolutions entirely** : [Stand-Alone Self-Attention in Vision Models(2019)](https://arxiv.org/abs/1906.05909),  [Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation(2020)](https://arxiv.org/abs/2003.07853)

ì´ì™€ ê°™ì´ ë‹¤ì–‘í•œ ì—°êµ¬ë“¤ì´ ìˆì—ˆìœ¼ë‚˜, attention patternì´ íŠ¹ì´í•˜ì—¬ hardware acceleratorë¥¼ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ scalingí•˜ê¸°ê°€ ì‰½ì§€ ì•Šë‹¤ëŠ” ë‹¨ì ì´ ìˆì—ˆê³ , ê³„ì†í•´ì„œ **large-scale image recognition**ë¶„ì•¼ì—ì„œëŠ” ResNetì´ SoTAë¡œ ì‚¬ìš©ë˜ì—ˆë‹¤.

> **ë³¸ ë…¼ë¬¸ì—ì„œëŠ” NLPì˜ transformerì˜ í™•ì¥ì„±ì„ ì»´í“¨í„° ë¹„ì „ì— ì ìš©í•˜ê¸° ìœ„í•´ Transformer architectureë¥¼ ìµœì†Œí•œë§Œ ìˆ˜ì •í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ë„ë¡ í•œë‹¤.** (imageë¥¼ patchë‹¨ìœ„ë¡œ splití•œ í›„ ì´ë¥¼ ì„ í˜•ë³€í™˜ í•˜ì—¬ Transformerì˜ inputìœ¼ë¡œ ë„£ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.)
>
> ëª¨ë¸ì€ ImageNetê³¼ ê°™ì€ ì¤‘ê°„ ì‚¬ì´ì¦ˆì˜ datasetì—ì„œëŠ” ì˜ ë™ì‘í•˜ì§€ ì•Šìœ¼ë©°, ì´ ê²½ìš°ì—ëŠ” ResNetê³¼ ê°™ì€ ê¸°ì¡´ì˜ vision modelë“¤ì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤. **ë‹¤ë§Œ, ImageNet-21k datasetì´ë‚˜ JFT-300M datasetê³¼ ê°™ì€ large-scaleì˜ dataì— ëŒ€í•´ì„œëŠ” ê¸°ì¡´ì˜ ëª¨ë¸ë“¤ë³´ë‹¤ ë” ì¢‹ì€ ì •í™•ë„ë¥¼ ë³´ì˜€ê³  SoTAì˜ ê²°ê³¼ë¥¼ ëƒˆë‹¤.**

## 2. Related Work

Deep learning modelì—ì„œ ìˆ˜ë§ì€ ë°ì´í„°ë“¤ì„ ì „ë¶€ ë‹¤ í•™ìŠµí•˜ëŠ” ê²ƒì€ ì–´ë µë‹¤. ë”°ë¼ì„œ ëŒ€ë¶€ë¶„ì˜ **Large Transformer-based model**ì€ `Transfer Learning` ì˜ ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤. ì´ëŠ” large [text corpus](https://en.wikipedia.org/wiki/Text_corpus)ì—ì„œ ë¯¸ë¦¬ í•™ìŠµëœ ëª¨ë¸(`pretrained model`)ì˜ ì¼ë¶€ë¥¼ ìˆ˜ì •í•˜ëŠ” `fine tuning` ë°©ì‹ì´ë‹¤.

ì‹¤ì œë¡œ [BERT](https://arxiv.org/abs/1810.04805) ì—ì„œëŠ” denoising self-supervised pre-training taskë¥¼ ì‚¬ìš©í–ˆê³ , GPTì—ì„œëŠ” language modelingì˜ ë°©ì‹ìœ¼ë¡œ pre-trainingì„ í–ˆë‹¤.

- Transfer Learning (ì¶œì²˜ : [simonjisu/FARM_tutorial](https://github.com/simonjisu/FARM_tutorial) 

    1.Â **PretrainedÂ LanguageÂ Modeling**

    - ëŒ€ëŸ‰ì˜Â í…ìŠ¤íŠ¸Â ë°ì´í„°ë¥¼Â ì´ìš©í•´Â ë¹„ì§€ë„í•™ìŠµ(unsupervisedÂ learning)ìœ¼ë¡œÂ ì–¸ì–´Â ëª¨ë¸ë§ì€Â ì§„í–‰í•œë‹¤.Â ì–¸ì–´Â ëª¨ë¸ë§ì´ë€Â ì¸ê°„ì˜Â ì–¸ì–´ë¥¼Â ì»´í“¨í„°ë¡œÂ ëª¨ë¸ë§í•˜ëŠ”Â ê³¼ì •ì´ë‹¤.Â ì‰½ê²ŒÂ ë§í•˜ë©´,Â ëª¨ë¸ì—ê²ŒÂ ë‹¨ì–´ë“¤ì„Â ì…ë ¥í–ˆì„Â ë•Œ,Â ì œì¼Â ë§ì´Â ë˜ëŠ”Â ë‹¨ì–´(í† í°)ì„Â ë±‰ì–´ë‚´ê²ŒÂ í•˜ëŠ”Â ê²ƒì´ë‹¤.

    - ê³¼ê±°ì—ëŠ”Â ë‹¨ì–´(í† í°)ì˜Â ìˆœì„œê°€Â ì¤‘ìš”í–ˆì—ˆë‹¤.Â ì¦‰,Â ì¼ì •Â ë‹¨ì–´ë“¤ì˜Â ì‹œí€€ìŠ¤ $x_{1:t-1}$Â ê°€Â ì£¼ì–´ì§€ë©´,Â $t$ë²ˆì§¸Â ë‹¨ì–´ì¸Â $x_t$ ë¥¼Â ì˜Â í•™ìŠµì‹œí‚¤ëŠ”Â ê²ƒì´ì—ˆë‹¤.Â ì´ë¥¼Â *AutoÂ RegressiveÂ Modeling*ì´ë¼ê³ ë„Â í•œë‹¤.

    - ê·¸ëŸ¬ë‚˜,Â MaskedÂ LanguageÂ ModelingÂ ë°©ë²•ì´Â ë“±ì¥í–ˆëŠ”ë°,Â ì´ëŠ”Â ëœë¤ìœ¼ë¡œÂ ë§ì¶°ì•¼í• Â ë‹¨ì–´ë¥¼Â ê°€ë¦°Â ë‹¤ìŒì—Â ê°€ë ¤ì§„Â ë‹¨ì–´Â *$x_{mask}$*ê°€Â í¬í•¨ëœÂ ì‹œí€€ìŠ¤Â $x_{1:t}$Â ë¥¼Â ëª¨ë¸ì—ê²ŒÂ ì…ë ¥í•˜ì—¬Â ë§ì¶”ëŠ”Â í•™ìŠµÂ ë°©ë²•ì´ë‹¤.Â ì´ëŸ¬í•œÂ ë°©ë²•ì´Â ì¢‹ì€Â ì„±ê³¼ë¥¼Â ê±°ë‘ë©´ì„œ,Â ìµœê·¼ì—ëŠ”Â ëª¨ë“ Â ì–¸ì–´ëª¨ë¸ë§Â ê¸°ë²•ë“¤ì´Â MLMì„Â ê¸°ë°˜ìœ¼ë¡œÂ í•˜ê³ Â ìˆë‹¤.

    2.Â **Fine-tuning**

    PLM(PretrainedÂ LanguageÂ Model)ì„Â ë§Œë“¤ê³ Â ë‚˜ë©´,Â ê°ê¸°Â ë‹¤ë¥¸Â downstreamÂ taskì—Â ë”°ë¼ì„œÂ fine-tuningì„Â í•˜ê²ŒÂ ëœë‹¤.Â DownstreamÂ taskì€Â êµ¬ì²´ì ìœ¼ë¡œÂ í’€ê³ Â ì‹¶ì€Â ë¬¸ì œë¥¼Â ë§í•˜ë©°,Â ì£¼ë¡œÂ ë‹¤ìŒê³¼Â ê°™ì€Â ë¬¸ì œë“¤ì´ë‹¤.

    - í…ìŠ¤íŠ¸Â ë¶„ë¥˜Â TextÂ ClassificationÂ -Â ì˜ˆì‹œ:Â ì˜í™”Â ëŒ“ê¸€Â ê¸ì •/ë¶€ì •Â ë¶„ë¥˜í•˜ê¸°
    - ê°œì²´ëª…ì¸ì‹Â NER(NamedÂ EntityÂ Recognition)Â -Â ì˜ˆì‹œ:Â íŠ¹ì •Â ê¸°ê´€ëª…,Â ì¸ëª…Â ë°Â ì‹œê°„Â ë‚ ì§œÂ ë“±Â í† í°ì—Â ì•Œë§ëŠ”Â íƒœê·¸ë¡œÂ ë¶„ë¥˜í•˜ê¸°
    - ì§ˆì˜ì‘ë‹µÂ QuestionÂ andÂ AnsweringÂ -Â ì˜ˆì‹œ:Â íŠ¹ì •Â ì§€ë¬¸ê³¼Â ì§ˆì˜(query)ê°€Â ì£¼ì–´ì§€ë©´Â ëŒ€ë‹µí•˜ê¸°

    - [https://jeinalog.tistory.com/13](https://jeinalog.tistory.com/13) ë„ ì°¸ê³ 


Self-attentionì„ imageì— ì ìš©í•˜ë ¤ë©´ ê° pixelì´ ë‹¤ë¥¸ ëª¨ë“  pixelì— attendí•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤. ë‹¤ë§Œ, ì´ë¥¼ ìœ„í•´ì„œëŠ” quadratic costê°€ ë“¤ë©°, ì‹¤ì œ image sizeë¡œ scalingë˜ì§€ë„ ì•ŠëŠ”ë‹¤. 

ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ë…¼ë¬¸ë“¤ë„ ìˆë‹¤.

- [Image Transfer(2018)](https://arxiv.org/abs/1802.05751)ì€ self-attentionì‹œì— query pixelì´ globalí•˜ê²Œ attendë¥¼ í•˜ì§€ ì•Šê³ , local neighborhoodì—ê²Œë§Œ attendë¥¼ í•˜ë„ë¡ ìˆ˜ì •í–ˆë‹¤.
- [Stand-Alone Self-Attention in Vision Models(2019)](https://arxiv.org/abs/1906.05909), [On the relationship between selfattention and convolutional layers(2020)](https://arxiv.org/abs/1911.03584) ì€ convolutionì„ ëŒ€ì‹ í•˜ì—¬ local multi-head dot-product self attentionì„ ì‚¬ìš©í–ˆë‹¤.
- [Sparse Transformer(2019)](https://arxiv.org/abs/1904.10509) ì´ë¯¸ì§€ì— global self-attentionì„ í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” scalable approximation ë°©ì‹ì„ ê³ ì•ˆí–ˆë‹¤.
    - scale attentionì˜ ëŒ€ì•ˆìœ¼ë¡œ [Axial attention in multidimensional transformers(2019)](https://arxiv.org/abs/1912.12180) ì€ ë‹¤ì–‘í•œ sizeì˜ blockì„ ì‚¬ìš©í•˜ì˜€ê³ , [Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation(2020)](https://arxiv.org/abs/2003.07853) ëŠ” ë‹¤ì–‘í•œ ì¶•ì„ ì‚¬ìš©í•˜ì—¬ self-attentionì„ íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“¤ì—ˆë‹¤.


ìœ„ì˜ ë…¼ë¬¸ë“¤ì—ì„œ ì†Œê°œëœ specialized attention architectureë¥¼ ì‚¬ìš©í•˜ë©´ computer vision taskë¥¼ ë” ì˜í•  ìˆ˜ ìˆì§€ë§Œ, ì´ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œëŠ” ë³µì¡í•œ HW acceleratorê°€ í•„ìš”í•˜ë‹¤.

CNNê³¼ self-attentionì„ í•©ì¹˜ë ¤ëŠ” ì‹œë„ë„ ë§ë‹¤.

- [Attention augmented convolutional networks(2019)](https://arxiv.org/abs/1904.09925)
- object detection : [End-to-end object detection with transformers(2020)](https://arxiv.org/abs/2005.12872)
- video processing : [Videobert: A joint model for video and language representation learning](https://arxiv.org/abs/1904.01766)
- image classification : [Visual transformers: Token-based image representation and processing for computer vision](https://arxiv.org/abs/2006.03677)

ì´ ë…¼ë¬¸ì—ì„œëŠ” full-sized imageì— ëŒ€í•´ global self-attentionì„ ì ìš©í•˜ì§€ ì•Šê³ , ê°€ì¥ ìµœê·¼ì˜ ì—°êµ¬ì¸ [iGPT](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf)ì²˜ëŸ¼ image resolutionê³¼ color spaceë¥¼ ì¤„ì¸ í›„ image pixelì— Transformerëª¨ë¸ì„ ì ìš©í•œë‹¤. iGPT modelì€ unsupervised ë°©ì‹ì„ ì‚¬ìš©í•´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í–ˆìœ¼ë©°, ì´í›„ fine-tuning ì´ë‚˜ linearë¡œ ê·¼ì‚¬í•˜ëŠ” ë°©ì‹ì„ ì¨ì„œ ImageNetì—ì„œì˜ ì •í™•ë„ë¥¼ 72%ê¹Œì§€ ë§Œë“¤ì—ˆë‹¤.

## 3. Architecture

### 3.1 Vision Transformer (ViT)

![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit1.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit1.PNG?raw=1)

ë³¸ ë…¼ë¬¸ì€ ë…¼ë¬¸ ì œëª©ì—ì„œë¶€í„° scalabilityë¥¼ ê°•ì¡°í•œë‹¤. ë”°ë¼ì„œ ViT ëª¨ë¸ì—ì„œëŠ” original Transformerì„ ìµœì†Œí•œë§Œ ìˆ˜ì •í•˜ì—¬ í° datasetì—ì„œ í•™ìŠµí•  ë•Œì—ë„ íš¨ìœ¨ì ì¸ implementationì´ ê°€ëŠ¥í•˜ë„ë¡ í•˜ì˜€ë‹¤.

> ğŸ¤” ì ë‹¹í•œ ì‚¬ì´ì¦ˆì˜ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œëŠ” ì„±ëŠ¥ì´ ì˜ ì•ˆë‚˜ì™€ì„œ scalabilityë¥¼ ê°•ì¡°í•˜ëŠ” ê²ƒ ê°™ê¸°ë„,,  

ëª¨ë¸ì€ ìœ„ì™€ ê°™ì´ ìƒê²¼ê³ , transformerì—ì„œì˜ encoder architectureë§Œì„ ì°¨ìš©í•œë‹¤.

ë˜í•œ, ê·¸ë™ì•ˆì˜ transformerì—ì„œëŠ” 1Dì˜ sequenceë§Œì„ ë‹¤ë¤˜ë‹¤ë©´ ViTëŠ” `2-dimension` ì˜ imageë¥¼ inputìœ¼ë¡œ ë°›ëŠ”ë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ìœ„í•´ `H x W x C`ë¡œ ì´ë£¨ì–´ì§„   $\mathbf{x}\in \mathbb{ R }^{ H\times W\times C }$ì˜ ì´ë¯¸ì§€ë¥¼ flattenëœ 2D patches $x_{p}\in \mathbb{R}^{N \times (P^{2}\cdot C)}$ë¡œ ë³€í™˜í•œë‹¤.

- $(P,P)$ëŠ” image patchì˜ resolutionì´ë©°, $N=HW/P^2$ì˜ ê´€ê³„ë¥¼ ë§Œì¡±í•œë‹¤.

**Patch Embedding**

- ê¸°ì¡´ì˜ TransformerëŠ” ëª¨ë“  layerì—ì„œ $D_{model}$ì˜ constant latent vectorë¥¼ ì‚¬ìš©í–ˆë‹¤. ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ë°©ì‹ì„ ViTì—ì„œë„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì•„ë˜ì˜ ì‹ì„ ì‚¬ìš©í•˜ì—¬ 2Dì˜ flatten paddingì„ `D-dimension` ì˜ linear projectionì— íˆ¬ì˜í•˜ì˜€ë‹¤.

    $$ \mathbf{z}_{0}=\left[\mathbf{x}_{\text {class }} ; \mathbf{x}_{p}^{1}  \mathbf{E} ; \mathbf{x}_{p}^{2} \mathbf{E} ; \cdots ; \mathbf{x}_{p}^{N} \mathbf{E}\right]+\mathbf{E}_{p o s}, \quad \mathbf{E} \in \mathbb{R}^{\left(P^{2} \cdot C\right) \times D}, \mathbf{E}_{p o s} \in \mathbb{R}^{(N+1) \times D} $$

    - $x_p$ì™€ embedding vector $E$ ë¥¼ ë‚´ì í•˜ë©´ : $x_{p}\mathbf{E}\in \mathbb{R}^{N \times D}$

- BERTì˜ CLS token?

**Position embedding**

- ìœ„ì¹˜ ì •ë³´ë¥¼ ì£¼ê¸° ìœ„í•´ patch embeddingì— positional embeddingê°’ì„ ë”í•´ì¤€ë‹¤.
- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” 2-Dì˜ positional embeddingì„ ì‚¬ìš©í–ˆì„ ë•Œì™€ í•™ìŠµê°€ëŠ¥í•œ 1-D positional embeddingì„ ì‚¬ìš©í–ˆì„ ë•Œì˜ ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ì—¬ **standard learnable 1-D position embedding** ì„ ì‚¬ìš©í•œë‹¤ê³  í•œë‹¤.


- Appendix D.3 

    ì‹¤í—˜ì—ì„œëŠ” 4ê°€ì§€ì˜ positional embedding ë°©ì‹ì„ ë¹„êµí–ˆë‹¤.


    1. No positional information : inputì´ bag of patcheì™€ ê°™ë‹¤

    2. 1-D positional embedding

    3. 2-D positional embedding 

    4. Relative positional embedding : 1-dimensional Relative Attentionì„ ì‚¬ìš©í•˜ì—¬ patchë“¤ ê°„ì— ìƒëŒ€ì ì¸ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ì˜€ë‹¤.

    ---

    ![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit9.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit9.PNG?raw=1)

    `1-D` ì™€ `2-D` ì˜ positional embeddingì„ í† ëŒ€ë¡œ 3ê°€ì§€ì˜ ì‹¤í—˜ì„ ì§„í–‰í–ˆë‹¤.

    (1) Default : inputì˜ ì˜¤ë¥¸ìª½ì— positional embeddingì„ ì¶”ê°€

    (2) Every layer : ê° layerì˜ ì‹œì‘ë¶€ë¶„ì—ì„œ inputê³¼ positional embeddingì„ ë”í•˜ê³  í•™ìŠµì‹œí‚¤ê¸°

    (3) Every layer-Shared : í•™ìŠµëœ positional embeddingì„ ê° layerì˜ ì‹œì‘ë¶€ì— ì¶”ê°€í•´ì£¼ê¸°

    ---

    ì‹¤í—˜ ê²°ê³¼ë¥¼ ì¢…í•©í•´ë³´ë©´, Pos Embì„ ì•ˆí•œ ê²½ìš°ì™€ í•œ ê²½ìš°ì˜ ì°¨ì´ëŠ” ë‘ë“œëŸ¬ì¡Œë‹¤. ë‹¤ë§Œ, ëª‡ì°¨ì›ì˜ Pos Embì„ í–ˆëƒëŠ” ì¤‘ìš”í•˜ì§€ ì•Šì•˜ë‹¤. 

    ViTê°€ `pixel-level` ì´ ì•„ë‹ˆë¼ `patch-level` ë‹¨ìœ„ë¡œ í•™ìŠµì„ í•˜ê¸° ë•Œë¬¸ì— spatial informationì„ ì–¼ë§ˆë‚˜ encodingí•˜ëŠ”ì§€ëŠ” ì¤‘ìš”í•˜ì§€ ì•Šë‹¤. patch-levelì˜ inputì€ original pixel-levelì¸ `224 x 224` dimensionì™€ ë¹„êµí•˜ë©´ ë§¤ìš° ì‘ì€ spatial dimensionsì„ ì‚¬ìš©í•œë‹¤. (ex.  `14 x 14` )  ë”°ë¼ì„œ ì´ ì‘ì€ í•´ìƒë„ì˜ positional embeddingì„ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œ 2-Dì™€ ê°™ì€ ê³ ì°¨ì›ì„ ì‚¬ìš©í•  í•„ìš”ëŠ” ì—†ë‹¤. 

    ---

    ![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit10.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit10.PNG?raw=1)


**ViT Block**

Transformerì˜ encoderëŠ” ëª¨ë“  blockë§ˆë‹¤ Residual connectionê³¼ Layernorm(LN)ì„ ì ìš©í•˜ì˜€ë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ì€ Multi-Layer Perceptron(MLP)ê°€ ëœë‹¤.

MLPëŠ” 2ê°œì˜ [GELU(Gaussain Error Linear Unit)](https://arxiv.org/pdf/1606.08415.pdf) non-linearity layerë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤ê³  í•œë‹¤. 

$$ \begin{aligned}\mathbf{z}_{\ell}^{\prime} &=\operatorname{MSA}\left(\operatorname{LN}\left(\mathbf{z}_{\ell-1}\right)\right)+\mathbf{z}_{\ell-1}, & & \ell=1 \ldots L \\\mathbf{z}_{\ell} &=\operatorname{MLP}\left(\operatorname{LN}\left(\mathbf{z}_{\ell}^{\prime}\right)\right)+\mathbf{z}_{\ell}^{\prime}, & \ell &=1 \ldots L \\\mathbf{y} &=\operatorname{LN}\left(\mathbf{z}_{L}^{0}\right) & &\end{aligned} $$

- <details>
  <summary> MSA (Multi-Head Self-Attention) </summary>
    
    
    ë‹¤ìŒì€ transformerì˜ Self-Attention(ì´í•˜ SA)ì‹ì´ë‹¤.

    $$ \begin{aligned}&[\mathbf{q}, \mathbf{k}, \mathbf{v}]=\mathbf{z} \mathbf{U}_{q k v}\  \quad\quad \mathbf{z} \in \mathbb{R}^{N \times D}\quad    \mathbf{U}_{q k v} \in \mathbb{R}^{D \times 3 D_{h}}\end{aligned}$$

    $$A=\operatorname{softmax}\left(\mathbf{q k}^{\top} / \sqrt{D_{h}}\right) \quad A \in \mathbb{R}^{N \times N}$$

    $$\operatorname{SA}(\mathbf{z})=A \mathbf{v}$$

    self-attentionì„ kê°œì˜ headë¡œ ë‚˜ëˆ  ê³„ì‚°í•˜ëŠ” ê²Œ MSAì´ë‹¤.

    $$\operatorname{MSA}(\mathbf{z})=\left[\mathrm{SA}_{1}(z) ; \mathrm{SA}_{2}(z) ; \cdots ; \mathrm{SA}_{k}(z)\right] \mathbf{U}_{m s a} \quad \mathbf{U}_{m s a} \in \mathbb{R}^{k \cdot D_{h} \times D}$$
  </details>

**Hybrid Architecture**

> ì´ë¯¸ì§€ë¥¼ patchë¡œ splití•˜ëŠ” ëŒ€ì‹ , CNNì˜ feature mapì—ì„œ input sequenceë¥¼ ìƒì„±í•  ìˆ˜ë„ ìˆë‹¤. ì´ ëª¨ë¸ì„ **í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸**ì´ë¼ê³  ë¶€ë¥´ëŠ”ë°, ì´ë•ŒëŠ”  ì•„ë˜ì˜ ë°©ì •ì‹ì˜ patch embedding projection **E** ê°€ CNN feature mapì—ì„œ ì¶”ì¶œëœ patchë¡œ ë³€ê²½ëœë‹¤.

$$ \mathbf{z}_{0}=\left[\mathbf{x}_{\text {class }} ; \mathbf{x}_{p}^{1}  \mathbf{E} ; \mathbf{x}_{p}^{2} \mathbf{E} ; \cdots ; \mathbf{x}_{p}^{N} \mathbf{E}\right]+\mathbf{E}_{p o s}, \quad \mathbf{E} \in \mathbb{R}^{\left(P^{2} \cdot C\right) \times D}, \mathbf{E}_{p o s} \in \mathbb{R}^{(N+1) \times D} $$

ì´ ê²½ìš°ì—ëŠ” patchë¡œ `1x1` ì˜ spatial sizeë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— input sequenceëŠ” ì‰½ê²Œ flattenë  ìˆ˜ ìˆë‹¤.

### 3.2 Fine-Tuning and Higher Resolution

> ì¼ë°˜ì ìœ¼ë¡œëŠ” **ViTë¥¼ large datasetì—ì„œ pre-trainingí•œ í›„, ë” ì‘ì€ downstream taskì— ëŒ€í•´ fine-tuningí•œë‹¤.** 
> 
> pre-traingì„ í•˜ëŠ” ê²ƒë³´ë‹¤ ë†’ì€ resolutionìœ¼ë¡œ fine-tuningì„ í•˜ëŠ”ê²Œ ì„±ëŠ¥ì— ë„ì›€ì´ ë ë•Œê°€ ë§ë‹¤. ë”°ë¼ì„œ pre-trained prediction headë¥¼ ì œê±°í•˜ê³  0ìœ¼ë¡œ ì´ˆê¸°í™”ëœ `D x K` ì˜ feedforward layerë¥¼ ë¶™ì˜€ë‹¤. (`K` ëŠ” downstream class)

ë§Œì•½ imageì˜ í™”ì§ˆì´ ë†’ë‹¤ë©´, sequence lengthëŠ” ë” ê¸¸ì–´ì§ˆ ê²ƒì´ë‹¤. ê·¸ëŸ°ë° ViTëŠ” ì œí•œëœ ê¸¸ì´ì˜ sequenceë§Œì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ pre-trained position embeddingì´ ë”ì´ìƒ ì˜ë¯¸ê°€ ì—†ì–´ì§€ê²Œ ëœë‹¤.

ë”°ë¼ì„œ ì´ ê²½ìš°ì—ëŠ” pre-trained position embeddingì´ ì›ë³¸ ì´ë¯¸ì§€ì˜ locationì— ë”°ë¼ 2D interpolationì„ ìˆ˜í–‰í•˜ë„ë¡ ì¡°ì •í•´ì¤€ë‹¤.

## 4. Experiments

- Resnet, ViT, hybrid modelì„ í‰ê°€
- ë‹¤ì–‘í•œ sizeì˜ dataì™€ ë§ì€ benchmark taskë¥¼ ì‚¬ìš©

> ì´ ë…¼ë¬¸ì˜ ì €ìë“¤ì€ ViTê°€ ë” ë‚®ì€ pre-training costë¡œ ëŒ€ë¶€ë¶„ì˜ recognition benchmarkì—ì„œ SoTAë¥¼ ë‹¬ì„±í–ˆë‹¤ê³  ì£¼ì¥í•œë‹¤.

### 4.1 Setup

**Datasets**

Pre-train dataset

- **ILSVRC-2012 ImageNet dataset(ImageNet) -** 1k classes / 1.3M images
- **ImageNet-21k** - 21k classes / 14M images
- **JFT** - 18k classes / 303M images

Transfer Learning dataset

- ImageNet, cleaned up ReaL labels
- CIFAR 10/100
- Oxford-IIIT Pets
- Oxford Flowers-102

Evaluate

- 19-task VTAB classification suite

**Model Variants**

![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit6.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit6.PNG?raw=1)

- base ViT configurationìœ¼ë¡œëŠ”  BERTì™€ ìœ ì‚¬í•˜ê²Œ ì„¤ì •(HugeëŠ” ìƒˆë¡œ ì¶”ê°€)
- Vit-L/16 : Lì€ large, 16ì€ 16 x 16 patch

CNNì˜ baselineìœ¼ë¡œëŠ” ResNetì„ ì‚¬ìš©í–ˆê³ , Batch Normalization layerëŒ€ì‹  Group Normalizationì„ ì‚¬ìš©í–ˆë‹¤.

**Training & Fine-tuning**

**Training**

- Adam optimizer
    - ë…¼ë¬¸ì˜ ì‹¤í—˜ì—ì„œëŠ” Adamì´ SGDë³´ë‹¤ ë¹¨ëìŒ
- *Î²*1=0.9, *Î²*2=0.999, *batch_size*=4096
- weight decay: 0.1

**Fine-tuning**

- SGD with momentum,
    - Fine-tuningì€ SGDì‚¬ìš©
- *batch_size*=512
- using linear learning rate warmup and decay
- Higher resolution: 512 for ViT-L/16 and 518 for ViT-H/14

**Metrics**

downstream datasetëŠ” few-shotì´ë‚˜ fine-tuning accuracyë¥¼ í†µí•´ í‰ê°€í•œë‹¤.

ë³´í†µ fine-tuning performanceë¥¼ ë³´ì§€ë§Œ ë¹ ë¥´ê²Œ ê²°ê³¼ë¥¼ ë³´ê³  ì‹¶ì€ ê²½ìš°ì—ëŠ” linear few-shot accuracyë¥¼ ë³´ê¸°ë„ í•œë‹¤.

### 4.2 Comparison to State of the Art

![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit7.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit7.PNG?raw=1)

í•™ìŠµ ì‹œê°„ê³¼ ê²°ê³¼ì´ë‹¤.

- Big Transfer(BiT)ëŠ” large ResNetìœ¼ë¡œ supervised transfer learningì„ ìˆ˜í–‰í–ˆë‹¤.
- Noisy StudentëŠ” large EfficientNetìœ¼ë¡œ í•™ìŠµí–ˆë‹¤.

> ë§Œê°œê°€ ë„˜ëŠ” TPUë¥¼ ì¨ì„œ í•™ìŠµí•œ ê±¸ ë³´ë©´, ì—­ì‹œ êµ¬ê¸€ì€ êµ¬ê¸€ì´ë‹¤...
> ìµœê·¼ NLPìª½ ì—°êµ¬ëŠ” í° ê·œëª¨ì˜ íšŒì‚¬ê°€ ì•„ë‹ˆë©´ ëª»í•œë‹¤ëŠ” ë§ì´ ë§ëŠ” ê²ƒ ê°™ë‹¤,, 

ìœ„ì˜ ê²°ê³¼ëŠ” BiTì™€ ViTë¥¼ ë¹„êµí•œ ë‚´ìš©ì¸ë° ViTê°€ BiTë³´ë‹¤ ë” ì ì€ resourceë¥¼ ì‚¬ìš©í•¨ì—ë„ í›¨ì”¬ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ê³  ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤.

> (ê°œì¸ì ì¸ ìƒê°ìœ¼ë¡œ) JFT datasetì€ êµ¬ê¸€ì—ì„œë§Œ ì‚¬ìš©ë˜ëŠ” ê³µê°œë˜ì§€ ì•Šì€ ë°ì´í„°ì…‹ì¸ë°, ì´ë¥¼ ì‚¬ìš©í•œ ê²°ê³¼ë¥¼ ë…¼ë¬¸ì— ì“°ëŠ”ê±´ ì•½ê°„ ì• ë§¤í•œ ê²ƒ ê°™ë‹¤ ğŸ¤”

 

![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit2.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit2.PNG?raw=1)](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit2.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit2.PNG?raw=1))

í° ì‚¬ì´ì¦ˆì˜ pre-trained data(ImageNet-21kì™€ JFT-300M)ë¥¼ ì‚¬ìš©í•´ì„œ ì‹¤í—˜ì„ í–ˆì„ ë•Œ, ì´ì „ ì—°êµ¬ë“¤ê³¼ ë¹„êµí•´ë³´ë©´ ViTëŠ” State of the Art ì˜ ê²°ê³¼ë¥¼ ë‚¸ë‹¤ ! ğŸ˜

### 4.3 Pre-training Data Requirements

![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit8.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit8.PNG?raw=1)

- small datasetì— ëŒ€í•´ì„œë„ ì¢‹ì€ performanceë¥¼ ë‚¼ ìˆ˜ ìˆë„ë¡ weight decay, dropout, label smoothingì˜ ì„¸ê°€ì§€ regularization parameterë¥¼ optimizeí–ˆë‹¤.
- ImageNetê³¼ ê°™ì€ ì‘ì€ pre-trained datasetì„ ì‚¬ìš©í•˜ë©´, ì •ê·œí™”ë¥¼ ë§ì´ í•´ë„ ViTì˜ ì„±ëŠ¥ì´ BiTë³´ë‹¤ ë–¨ì–´ì§„ë‹¤. (Figure3)
- ì´ë¥¼ ë³´ë©´, ViT ëª¨ë¸ì€ í° ë°ì´í„°ì…‹ì—ì„œë§Œ ì¢‹ì€ ì„±ëŠ¥ì„ ëƒ„ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- pre-training sampleì˜ ìˆ˜ê°€ ì ì„ ë•Œì—ëŠ” ViTì˜ ì„±ëŠ¥ì´ ResNetë³´ë‹¤ ë–¨ì–´ì§„ë‹¤.(Figure4)
    - TransformerëŠ” CNNì— ë¹„í•´ [inductive biases](https://coding-chobo.tistory.com/97)ê°€ ë¶€ì¡±í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµë°ì´í„°ê°€ ì ì€ ê²½ìš°ì—ëŠ” í•™ìŠµì´ ì˜ ì•ˆëœë‹¤. ì‰½ê²Œ ë§í•˜ìë©´ transformerëŠ” CNNì— ë¹„í•´ ì´ë¯¸ì§€ì˜ íŒ¨í„´ì„ í•™ìŠµí•˜ê¸° ê°€ ì‰½ì§€ ì•Šë‹¤. (translation equivarianceë‚˜ localityë¥¼ í•™ìŠµí•˜ê¸°ê°€ ì–´ë ¤ì›€)

        ë”°ë¼ì„œ ìœ„ì˜ ê²°ê³¼ì²˜ëŸ¼ transformerëŠ” ë°ì´í„°ì…‹ì˜ í¬ê¸°ê°€ í´ ë•Œì— ë” í•™ìŠµì´ ì˜ëœë‹¤

 

### 4.4 Scaling Study

![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit3.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit3.PNG?raw=1)

![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit11.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit11.PNG?raw=1)

JFT-300M datasetì„ í†µí•´ ë‹¤ì–‘í•œ ëª¨ë¸ì— ëŒ€í•´ scalingì—°êµ¬ë¥¼ í•´ë³´ì•˜ë‹¤.

ì´ë•Œ, data sizeëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì— ë³‘ëª©í˜„ìƒì´ ë˜ì§€ ì•Šìœ¼ë©° ê° modelì˜ pre-training cost ë‹¹ ì„±ëŠ¥ì„ ë¹„êµí•´ë³¸ë‹¤.

- ViTëŠ” ì „ë°˜ì ìœ¼ë¡œ ResNetë³´ë‹¤ íš¨ìœ¨ì  : performance/compute trade-offì—ì„œ 2-4ë°° ë” ì¢‹ì€ ì„±ëŠ¥ì„ ëƒ„
- datasetì˜ í¬ê¸°ê°€ ì‘ì€ ê²½ìš°ì—ëŠ” Hybridê°€ ViTë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ëƒ„

### 4.5 Inspecting Vision Transformer

![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit12.gif?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit12.gif?raw=1)

ViTëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìˆœì„œëŒ€ë¡œ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•œë‹¤.

1. ì²«ë²ˆì§¸ layerëŠ” flattened patchë¥¼ lower Dimension spaceìœ¼ë¡œ ì„ í˜• íˆ¬ì˜í•œë‹¤.

    ![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit13.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit13.PNG?raw=1)

    Figure7ì˜ ì™¼ìª½ ê·¸ë¦¼ì€ í•™ìŠµëœ embedding filterì˜ êµ¬ì„±ìš”ì†Œë¥¼ ë³´ì—¬ì¤€ë‹¤. 

2. í•™ìŠµëœ position embeddingì„ patch representationì— ì¶”ê°€í•œë‹¤.

    Figure7ì˜ ì¤‘ê°„ ì‚¬ì§„ì„ ë³´ë©´ ê°€ê¹Œì´ì— ìˆëŠ” patchë“¤ì˜ position embeddingì´ ë¹„ìŠ·í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

3. Self-attentionì„ í†µí•´ í•˜ë‹¨ì˜ layerë¶€í„° ìƒë‹¨ì˜ layerê¹Œì§€ ì „ì²´ imageì— ëŒ€í•œ ì •ë³´ë¥¼ í†µí•©í•œë‹¤. ê° ì •ë³´ë“¤ì„ ëª¨ìœ¼ë©´ image spaceì—ì„œ ì´ë¯¸ì§€ê°„ í‰ê·  ê±°ë¦¬ì¸ attention distanceë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.  

## 5. Conclusion

> âœğŸ» ì´ ë…¼ë¬¸ì€ Transformerë¥¼ image recognitionì— ì§ì ‘ì ìœ¼ë¡œ ì ìš©í–ˆë‹¤. 
> Computer Visionì— self-attentionì„ ì ìš©í•œ ì´ì „ì˜ ì—°êµ¬ë“¤ê³¼ëŠ” ë‹¬ë¦¬, ViTì—ì„œëŠ” architectureì— image-specific inductive biasesë¥¼ ì ìš©í•˜ì§€ ì•Šì•˜ë‹¤. ëŒ€ì‹  imageë¥¼ patchë‹¨ìœ„ë¡œ ìª¼ê°œ Transformer encoderì—ì„œ ì²˜ë¦¬í•´ì¤¬ë‹¤.
> 
> ì´ì™€ ê°™ì€ ê°„ë‹¨í•˜ë©´ì„œë„ í™•ì¥ê°€ëŠ¥í•œ modelì€ pre-trainingëœ í° ë°ì´í„°ì…‹ê³¼ ê²°í•©í–ˆì„ ë•Œ ì•„ì£¼ ì¢‹ì€ ì„±ëŠ¥ì„ ëƒˆìœ¼ë©°, ë§ì€ image classification datasetì—ì„œ SoTAì˜ ê²°ê³¼ë¥¼ ëƒˆë‹¤.

ë‚¨ì•„ìˆëŠ” Challenge

- ViTë¥¼ detectionì´ë‚˜ segmentationì— ì ìš©í•˜ëŠ” ì—°êµ¬
- pre-training methods
  
## 6. Opinion

> ğŸ¤” ì´ ë…¼ë¬¸ì€ Transformerë¥¼ Vision ë¶„ì•¼ì— ì ìš©í–ˆë‹¤ëŠ” ì ì—ì„œ ì˜ì˜ë¥¼ ê°€ì§€ëŠ” ê±°ì§€, ë”±íˆ íšê¸°ì ì¸ architectureë¥¼ ì œì‹œí•˜ì§€ëŠ” ëª»í–ˆë‹¤ê³  ìƒê°í•œë‹¤.(ì œì‹œí•œ ideaë¼ê³ ëŠ” image patchì •ë„?)
> ë¬¼ë¡  SoTAì˜ ê²°ê³¼ë¥¼ ëƒˆê¸°ëŠ” í•˜ì§€ë§Œ, Googleì´ ì•„ë‹ˆì—ˆìœ¼ë©´ ê°€ëŠ¥í–ˆì„ê¹Œ? ì‹¶ê¸°ë„ í•˜ê³ ..
> ë§ì´ ê¸°ëŒ€ë¥¼ í•œ ë…¼ë¬¸ì´ì—ˆëŠ”ë°, parameterì™€ position embedding ì •ë„ë§Œì„ ë°”ê¿”ë³´ë©´ì„œ ì‹¤í—˜í•œê²Œ ë‹¤ì¸ ê²ƒ ê°™ì•„ ì•„ì‰½ë‹¤. êµ¬ê¸€ ë§Œì˜ ë°ì´í„°ì…‹ì¸ JFTë¥¼ ì‚¬ìš©í•œ ê²ƒë„ ê·¸ë ‡ê³ ..
> 
> ê´€ë ¨ ì—°êµ¬ë“¤ì´ ë” ë‹¤ì–‘í•˜ê²Œ ì§„í–‰ë˜ì–´ì„œ researchì˜ ê·œëª¨ê°€ í¬ì§€ ì•Šë”ë¼ë„ í•™ìŠµì´ ê°€ëŠ¥í•œ ê²½ëŸ‰í™”ëœ ëª¨ë¸ì´ ë‚˜ì™”ìœ¼ë©´ ì¢‹ê² ë‹¤ 