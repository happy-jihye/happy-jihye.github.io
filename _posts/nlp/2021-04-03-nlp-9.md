---
title: "[Paper Review] An Image is Worth 16X16 Words: Transformers for Image Recognition at Scale ë…¼ë¬¸ ë¶„ì„"
excerpt: " "

categories: 
  - nlp
tags: 
  - deeplearning
  - ai
  - nlp
  - vision
  - transformer
layout: jupyter
search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 

---


> ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” NLPì˜ ëŒ€í‘œ ëª¨ë¸ì¸ Transformerë¥¼ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì— ì ìš©í•œ **Vision Transformer (ViT)** ë…¼ë¬¸ì„ ë¦¬ë·°í•  ì˜ˆì •ì´ë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ViTë¥¼ SoTAì¸ CNN ëª¨ë¸ê³¼ê³¼ ë¹„êµí–ˆì„ ë•Œ í›Œë¥­í•œ ê²°ê³¼ë¥¼ ëƒˆë‹¤ê³  ì£¼ì¥í•œë‹¤.
>
> ì£¼ìš” taskëŠ” image classificationì´ë©° pre-trained ëœ large-scaleì˜ dataë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.

 

## 1. Introduction

[Transformer model(Vaswani et al, 2017)](https://arxiv.org/abs/1706.03762)ì€ NMT(Neural Machine Translation)ì„ ìœ„í•´ ì œì•ˆë˜ì—ˆìœ¼ë©°, ì•„ì§ê¹Œì§€ë„ NLP ë¶„ì•¼ì—ì„œ ì§€ë°°ì ì´ë‹¤. TransformerëŠ” ì—°ì‚°ì´ íš¨ìœ¨ì ì´ê³ , í™•ì¥ì„±ì´ ì¢‹ê¸° ë•Œë¬¸ì— ì•„ì£¼ í° dataì˜ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í–ˆê³ *(100Bì˜ parameter)*, í˜„ì¬ SoTAì¸ BERT, GPT ë“±ì˜ architectureë“¤ì€ ëª¨ë‘ transformerë¥¼ ë°œì „ì‹œì¼œ ì•„ì£¼ ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ì„ ë§Œë“¤ì—ˆë‹¤.

NLPë¶„ì•¼ì—ì„œ attentionì´ ë‘ê°ì„ ë‚˜íƒ€ë‚´ì, computer visionì—ë„ ì´ë¥¼ ì ìš©í•˜ìëŠ” ì›€ì§ì„ì´ ë‚˜íƒ€ë‚¬ë‹¤. 

- **combing CNN-like architectures with self-attention** : [Non-local neural networks(2018)](https://arxiv.org/abs/1711.07971), [End-to-end object detection with transformers(2020)](https://arxiv.org/abs/2005.12872)
- **replacing the convolutions entirely** : [Stand-Alone Self-Attention in Vision Models(2019)](https://arxiv.org/abs/1906.05909),  [Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation(2020)](https://arxiv.org/abs/2003.07853)

ì´ì™€ ê°™ì´ ë‹¤ì–‘í•œ ì—°êµ¬ë“¤ì´ ìˆì—ˆìœ¼ë‚˜, attention patternì´ íŠ¹ì´í•˜ì—¬ hardware acceleratorë¥¼ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ scalingí•˜ê¸°ê°€ ì‰½ì§€ ì•Šë‹¤ëŠ” ë‹¨ì ì´ ìˆì—ˆê³ , ê³„ì†í•´ì„œ **large-scale image recognition**ë¶„ì•¼ì—ì„œëŠ” ResNetì´ SoTAë¡œ ì‚¬ìš©ë˜ì—ˆë‹¤.

**ë³¸ ë…¼ë¬¸ì—ì„œëŠ” NLPì˜ transformerì˜ í™•ì¥ì„±ì„ ì»´í“¨í„° ë¹„ì „ì— ì ìš©í•˜ê¸° ìœ„í•´ Transformer architectureë¥¼ ìµœì†Œí•œë§Œ ìˆ˜ì •í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ë„ë¡ í•œë‹¤. (**imageë¥¼ patchë‹¨ìœ„ë¡œ splití•œ í›„ ì´ë¥¼ ì„ í˜•ë³€í™˜ í•˜ì—¬ Transformerì˜ inputìœ¼ë¡œ ë„£ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.)
****
ëª¨ë¸ì€ ImageNetê³¼ ê°™ì€ ì¤‘ê°„ ì‚¬ì´ì¦ˆì˜ datasetì—ì„œëŠ” ì˜ ë™ì‘í•˜ì§€ ì•Šìœ¼ë©°, ì´ ê²½ìš°ì—ëŠ” ResNetê³¼ ê°™ì€ ê¸°ì¡´ì˜ vision modelë“¤ì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤. **ë‹¤ë§Œ, ImageNet-21k datasetì´ë‚˜ JFT-300M datasetê³¼ ê°™ì€ large-scaleì˜ dataì— ëŒ€í•´ì„œëŠ” ê¸°ì¡´ì˜ ëª¨ë¸ë“¤ë³´ë‹¤ ë” ì¢‹ì€ ì •í™•ë„ë¥¼ ë³´ì˜€ê³  SoTAì˜ ê²°ê³¼ë¥¼ ëƒˆë‹¤.**

## 2. Related Work

Deep learning modelì—ì„œ ìˆ˜ë§ì€ ë°ì´í„°ë“¤ì„ ì „ë¶€ ë‹¤ í•™ìŠµí•˜ëŠ” ê²ƒì€ ì–´ë µë‹¤. ë”°ë¼ì„œ ëŒ€ë¶€ë¶„ì˜ **Large Transformer-based model**ì€ `Transfer Learning` ì˜ ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤. ì´ëŠ” large [text corpus](https://en.wikipedia.org/wiki/Text_corpus)ì—ì„œ ë¯¸ë¦¬ í•™ìŠµëœ ëª¨ë¸(`pretrained model`)ì—ì„œ ì¼ë¶€ë¥¼ ìˆ˜ì •í•˜ëŠ” `fine tuning` ë°©ì‹ì´ë‹¤.

ì‹¤ì œë¡œ [BERT](https://arxiv.org/abs/1810.04805) ì—ì„œëŠ” denoising self-supervised pre-training taskë¥¼ ì‚¬ìš©í–ˆê³ , GPTì—ì„œëŠ” language modelingì˜ ë°©ì‹ìœ¼ë¡œ pre-trainingì„ í–ˆë‹¤.

- **Transfer Learning ì„¤ëª… (ì¶œì²˜ : [simonjisu/FARM_tutorial](https://github.com/simonjisu/FARM_tutorial) )**

Self-attentionì„ imageì— ì ìš©í•˜ë ¤ë©´ ê° pixelì´ ë‹¤ë¥¸ ëª¨ë“  pixelì— attendí•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤. ë‹¤ë§Œ, ì´ë¥¼ ìœ„í•´ì„œëŠ” quadratic costê°€ ë“¤ë©°, ì‹¤ì œ image sizeë¡œ scalingë˜ì§€ë„ ì•ŠëŠ”ë‹¤. 

ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ë…¼ë¬¸ë“¤ë„ ìˆë‹¤.

- [Image Transfer(2018)](https://arxiv.org/abs/1802.05751)ì€ self-attentionì‹œì— query pixelì´ globalí•˜ê²Œ attendë¥¼ í•˜ì§€ ì•Šê³ , local neighborhoodì—ê²Œë§Œ attendë¥¼ í•˜ë„ë¡ ìˆ˜ì •í–ˆë‹¤.
- [Stand-Alone Self-Attention in Vision Models(2019)](https://arxiv.org/abs/1906.05909), [On the relationship between selfattention and convolutional layers(2020)](https://arxiv.org/abs/1911.03584) ì€ convolutionì„ ëŒ€ì‹ í•˜ì—¬ local multi-head dot-product self attentionì„ ì‚¬ìš©í–ˆë‹¤.
- [Sparse Transformer(2019)](https://arxiv.org/abs/1904.10509) ì´ë¯¸ì§€ì— global self-attentionì„ í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” scalable approximation ë°©ì‹ì„ ê³ ì•ˆí–ˆë‹¤.
    - scale attentionì˜ ëŒ€ì•ˆìœ¼ë¡œ [Axial attention in multidimensional transformers(2019)](https://arxiv.org/abs/1912.12180) ì€ ë‹¤ì–‘í•œ sizeì˜ blockì„ ì‚¬ìš©í•˜ì˜€ê³ , [Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation(2020)](https://arxiv.org/abs/2003.07853) ëŠ” ë‹¤ì–‘í•œ ì¶•ì„ ì‚¬ìš©í•˜ì—¬ self-attentionì„ íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“¤ì—ˆë‹¤.

ìœ„ì˜ ë…¼ë¬¸ë“¤ì—ì„œ ì†Œê°œëœ specialized attention architectureë¥¼ ì‚¬ìš©í•˜ë©´ computer vision taskë¥¼ ë” ì˜í•  ìˆ˜ ìˆì§€ë§Œ, ì´ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œëŠ” ë³µì¡í•œ HW acceleratorê°€ í•„ìš”í•˜ë‹¤.

CNNê³¼ self-attentionì„ í•©ì¹˜ë ¤ëŠ” ì‹œë„ë„ ë§ë‹¤.

- [Attention augmented convolutional networks(2019)](https://arxiv.org/abs/1904.09925)
- object detection : [End-to-end object detection with transformers(2020)](https://arxiv.org/abs/2005.12872)
- video processing : [Videobert: A joint model for video and language representation learning](https://arxiv.org/abs/1904.01766)
- image classification : [Visual transformers: Token-based image representation and processing for computer vision](https://arxiv.org/abs/2006.03677)

ì´ ë…¼ë¬¸ì—ì„œëŠ” full-sized imageì— ëŒ€í•´ global self-attentionì„ ì ìš©í•˜ì§€ ì•Šê³ , ê°€ì¥ ìµœê·¼ì˜ ì—°êµ¬ì¸ [iGPT](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf)ì²˜ëŸ¼ image resolutionê³¼ color spaceë¥¼ ì¤„ì¸ í›„ image pixelì— Transformerëª¨ë¸ì„ ì ìš©í•œë‹¤. iGPT modelì€ unsupervised ë°©ì‹ì„ ì‚¬ìš©í•´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í–ˆìœ¼ë©°, ì´í›„ fine-tuning ì´ë‚˜ linearë¡œ ê·¼ì‚¬í•˜ëŠ” ë°©ì‹ì„ ì¨ì„œ ImageNetì—ì„œì˜ ì •í™•ë„ë¥¼ 72%ê¹Œì§€ ë§Œë“¤ì—ˆë‹¤.

## 3. Architecture

### 3.1 Vision Transformer (ViT)

![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit1.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit1.PNG?raw=1)

ë³¸ ë…¼ë¬¸ì€ ë…¼ë¬¸ ì œëª©ì—ì„œë¶€í„° scalabilityë¥¼ ê°•ì¡°í•œë‹¤. ë”°ë¼ì„œ ViT ëª¨ë¸ì—ì„œëŠ” original Transformerì„ ìµœì†Œí•œë§Œ ìˆ˜ì •í•˜ì—¬ í° datasetì—ì„œ í•™ìŠµí•  ë•Œì—ë„ íš¨ìœ¨ì ì¸ implementationì´ ê°€ëŠ¥í•˜ë„ë¡ í•˜ì˜€ë‹¤.

ğŸ¤” ì ë‹¹í•œ ì‚¬ì´ì¦ˆì˜ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œëŠ” ì„±ëŠ¥ì´ ì˜ ì•ˆë‚˜ì™€ì„œ scalabilityë¥¼ ê°•ì¡°í•˜ëŠ” ê²ƒ ê°™ê¸°ë„,,  

ëª¨ë¸ì€ ìœ„ì™€ ê°™ì´ ìƒê²¼ê³ , transformerì—ì„œì˜ encoder architectureë§Œì„ ì°¨ìš©í•œë‹¤.

ë˜í•œ, ê·¸ë™ì•ˆì˜ transformerì—ì„œëŠ” 1Dì˜ sequenceë§Œì„ ë‹¤ë¤˜ë‹¤ë©´ ViTëŠ” `2-dimension` ì˜ imageë¥¼ inputìœ¼ë¡œ ë°›ëŠ”ë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ìœ„í•´ `H x W x C`ë¡œ ì´ë£¨ì–´ì§„   $\mathbf{x}\in \mathbb{ R }^{ H\times W\times C }$ì˜ ì´ë¯¸ì§€ë¥¼ flattenëœ 2D patches $x_{p}\in \mathbb{R}^{N \times (P^{2}\cdot C)}$ë¡œ ë³€í™˜í•œë‹¤.

- $(P,P)$ëŠ” image patchì˜ resolutionì´ë©°, $N=HW/P^2$ì˜ ê´€ê³„ë¥¼ ë§Œì¡±í•œë‹¤.

**Patch Embedding**

- ê¸°ì¡´ì˜ TransformerëŠ” ëª¨ë“  layerì—ì„œ $D_{model}$ì˜ constant latent vectorë¥¼ ì‚¬ìš©í–ˆë‹¤. ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ë°©ì‹ì„ ViTì—ì„œë„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì•„ë˜ì˜ ì‹ì„ ì‚¬ìš©í•˜ì—¬ 2Dì˜ flatten paddingì„ `D-dimension` ì˜ linear projectionì— íˆ¬ì˜í•˜ì˜€ë‹¤.

    $\mathbf{z}_{0}=\left[\mathbf{x}_{\text {class }} ; \mathbf{x}_{p}^{1}  \mathbf{E} ; \mathbf{x}_{p}^{2} \mathbf{E} ; \cdots ; \mathbf{x}_{p}^{N} \mathbf{E}\right]+\mathbf{E}_{p o s}, \quad \mathbf{E} \in \mathbb{R}^{\left(P^{2} \cdot C\right) \times D}, \mathbf{E}_{p o s} \in \mathbb{R}^{(N+1) \times D}$

    - $x_p$ì™€ embedding vector $E$ ë¥¼ ë‚´ì í•˜ë©´ : $x_{p}\mathbf{E}\in \mathbb{R}^{N \times D}$

- BERTì˜ CLS token?

**Position embedding**

- ìœ„ì¹˜ ì •ë³´ë¥¼ ì£¼ê¸° ìœ„í•´ patch embeddingì— positional embeddingê°’ì„ ë”í•´ì¤€ë‹¤.
- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” 2-Dì˜ positional embeddingì„ ì‚¬ìš©í–ˆì„ ë•Œì™€ í•™ìŠµê°€ëŠ¥í•œ 1-D positional embeddingì„ ì‚¬ìš©í–ˆì„ ë•Œì˜ ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ì—¬ **standard learnable 1-D position embedding** ì„ ì‚¬ìš©í•œë‹¤ê³  í•œë‹¤. ****
- Appendix D.3

**ViT Block**

Transformerì˜ encoderëŠ” ëª¨ë“  blockë§ˆë‹¤ Residual connectionê³¼ Layernorm(LN)ì„ ì ìš©í•˜ì˜€ë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ì€ Multi-Layer Perceptron(MLP)ê°€ ëœë‹¤.

MLPëŠ” 2ê°œì˜ [GELU(Gaussain Error Linear Unit)](https://arxiv.org/pdf/1606.08415.pdf) non-linearity layerë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤ê³  í•œë‹¤. 

$\begin{aligned}\mathbf{z}_{\ell}^{\prime} &=\operatorname{MSA}\left(\operatorname{LN}\left(\mathbf{z}_{\ell-1}\right)\right)+\mathbf{z}_{\ell-1}, & & \ell=1 \ldots L \\\mathbf{z}_{\ell} &=\operatorname{MLP}\left(\operatorname{LN}\left(\mathbf{z}_{\ell}^{\prime}\right)\right)+\mathbf{z}_{\ell}^{\prime}, & \ell &=1 \ldots L \\\mathbf{y} &=\operatorname{LN}\left(\mathbf{z}_{L}^{0}\right) & &\end{aligned}$

- **MSA (Multi-Head Self-Attention)**

**Hybrid Architecture**

ì´ë¯¸ì§€ë¥¼ patchë¡œ splití•˜ëŠ” ëŒ€ì‹ , CNNì˜ feature mapì—ì„œ input sequenceë¥¼ ìƒì„±í•  ìˆ˜ë„ ìˆë‹¤. ì´ ëª¨ë¸ì„ **í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸**ì´ë¼ê³  ë¶€ë¥´ëŠ”ë°, ì´ë•ŒëŠ”  ì•„ë˜ì˜ ë°©ì •ì‹ì˜ patch embedding projection **E** ê°€ CNN feature mapì—ì„œ ì¶”ì¶œëœ patchë¡œ ë³€ê²½ëœë‹¤.

$\mathbf{z}_{0}=\left[\mathbf{x}_{\text {class }} ; \mathbf{x}_{p}^{1}  \mathbf{E} ; \mathbf{x}_{p}^{2} \mathbf{E} ; \cdots ; \mathbf{x}_{p}^{N} \mathbf{E}\right]+\mathbf{E}_{p o s}, \quad \mathbf{E} \in \mathbb{R}^{\left(P^{2} \cdot C\right) \times D}, \mathbf{E}_{p o s} \in \mathbb{R}^{(N+1) \times D}$

ì´ ê²½ìš°ì—ëŠ” patchë¡œ `1x1` ì˜ spatial sizeë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— input sequenceëŠ” ì‰½ê²Œ flattenë  ìˆ˜ ìˆë‹¤.

### 3.2 Fine-Tuning and Higher Resolution

ì¼ë°˜ì ìœ¼ë¡œëŠ” **ViTë¥¼ large datasetì—ì„œ pre-trainingí•œ í›„, ë” ì‘ì€ downstream taskì— ëŒ€í•´ fine-tuningí•œë‹¤.** 

pre-traingì„ í•˜ëŠ” ê²ƒë³´ë‹¤ ë†’ì€ resolutionìœ¼ë¡œ fine-tuningì„ í•˜ëŠ”ê²Œ ì„±ëŠ¥ì— ë„ì›€ì´ ë ë•Œê°€ ë§ë‹¤. ë”°ë¼ì„œ pre-trained prediction headë¥¼ ì œê±°í•˜ê³  0ìœ¼ë¡œ ì´ˆê¸°í™”ëœ `D x K` ì˜ feedforward layerë¥¼ ë¶™ì˜€ë‹¤. (`K` ëŠ” downstream class)

ë§Œì•½ imageì˜ í™”ì§ˆì´ ë†’ë‹¤ë©´, sequence lengthëŠ” ë” ê¸¸ì–´ì§ˆ ê²ƒì´ë‹¤. ê·¸ëŸ°ë° ViTëŠ” ì œí•œëœ ê¸¸ì´ì˜ sequenceë§Œì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ pre-trained position embeddingì´ ë”ì´ìƒ ì˜ë¯¸ê°€ ì—†ì–´ì§€ê²Œ ëœë‹¤.

ë”°ë¼ì„œ ì´ ê²½ìš°ì—ëŠ” pre-trained position embeddingì´ ì›ë³¸ ì´ë¯¸ì§€ì˜ locationì— ë”°ë¼ 2D interpolationì„ ìˆ˜í–‰í•˜ë„ë¡ ì¡°ì •í•´ì¤€ë‹¤.

## 4. Experiments

- Resnet, ViT, hybrid modelì„ í‰ê°€
- ë‹¤ì–‘í•œ sizeì˜ dataì™€ ë§ì€ benchmark taskë¥¼ ì‚¬ìš©

ì´ ë…¼ë¬¸ì˜ ì €ìë“¤ì€ ViTê°€ ë” ë‚®ì€ pre-training costë¡œ ëŒ€ë¶€ë¶„ì˜ recognition benchmarkì—ì„œ SoTAë¥¼ ë‹¬ì„±í–ˆë‹¤ê³  ì£¼ì¥í•œë‹¤.

### 4.1 Setup

**Datasets**

Pre-train dataset

- **ILSVRC-2012 ImageNet dataset(ImageNet) -** 1k classes / 1.3M images
- **ImageNet-21k** - 21k classes / 14M images
- **JFT** - 18k classes / 303M images

Transfer Learning dataset

- ImageNet, cleaned up ReaL labels
- CIFAR 10/100
- Oxford-IIIT Pets
- Oxford Flowers-102

Evaluate

- 19-task VTAB classification suite

**Model Variants**

![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit6.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit6.PNG?raw=1)

- base ViT configurationìœ¼ë¡œëŠ”  BERTì™€ ìœ ì‚¬í•˜ê²Œ ì„¤ì •(HugeëŠ” ìƒˆë¡œ ì¶”ê°€)
- Vit-L/16 : Lì€ large, 16ì€ 16 x 16 patch

CNNì˜ baselineìœ¼ë¡œëŠ” ResNetì„ ì‚¬ìš©í–ˆê³ , Batch Normalization layerëŒ€ì‹  Group Normalizationì„ ì‚¬ìš©í–ˆë‹¤.

**Training & Fine-tuning**

**Training**

- Adam optimizer
    - ë…¼ë¬¸ì˜ ì‹¤í—˜ì—ì„œëŠ” Adamì´ SGDë³´ë‹¤ ë¹¨ëìŒ
- *Î²*1=0.9, *Î²*2=0.999, *batch_size*=4096
- weight decay: 0.1

**Fine-tuning**

- SGD with momentum,
    - Fine-tuningì€ SGDì‚¬ìš©
- *batch_size*=512
- using linear learning rate warmup and decay
- Higher resolution: 512 for ViT-L/16 and 518 for ViT-H/14

**Metrics**

downstream datasetëŠ” few-shotì´ë‚˜ fine-tuning accuracyë¥¼ í†µí•´ í‰ê°€í•œë‹¤.

ë³´í†µ fine-tuning performanceë¥¼ ë³´ì§€ë§Œ ë¹ ë¥´ê²Œ ê²°ê³¼ë¥¼ ë³´ê³  ì‹¶ì€ ê²½ìš°ì—ëŠ” linear few-shot accuracyë¥¼ ë³´ê¸°ë„ í•œë‹¤.

### 4.2 Comparison to State of the Art

![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit7.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit7.PNG?raw=1)

í•™ìŠµ ì‹œê°„ê³¼ ê²°ê³¼ì´ë‹¤.

- Big Transfer(BiT)ëŠ” large ResNetìœ¼ë¡œ supervised transfer learningì„ ìˆ˜í–‰í–ˆë‹¤.
- Noisy StudentëŠ” large EfficientNetìœ¼ë¡œ í•™ìŠµí–ˆë‹¤.

ë§Œê°œê°€ ë„˜ëŠ” TPUë¥¼ ì¨ì„œ í•™ìŠµí•œ ê±¸ ë³´ë©´, ì—­ì‹œ êµ¬ê¸€ì€ êµ¬ê¸€ì´ë‹¤...
ìµœê·¼ NLPìª½ ì—°êµ¬ëŠ” í° ê·œëª¨ì˜ íšŒì‚¬ê°€ ì•„ë‹ˆë©´ ëª»í•œë‹¤ëŠ” ë§ì´ ë§ëŠ” ê²ƒ ê°™ë‹¤,, 

ìœ„ì˜ ê²°ê³¼ëŠ” BiTì™€ ViTë¥¼ ë¹„êµí•œ ë‚´ìš©ì¸ë° ViTê°€ BiTë³´ë‹¤ ë” ì ì€ resourceë¥¼ ì‚¬ìš©í•¨ì—ë„ í›¨ì”¬ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ê³  ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤.

(ê°œì¸ì ì¸ ìƒê°ìœ¼ë¡œ) JFT datasetì€ êµ¬ê¸€ì—ì„œë§Œ ì‚¬ìš©ë˜ëŠ” ê³µê°œë˜ì§€ ì•Šì€ ë°ì´í„°ì…‹ì¸ë°, ì´ë¥¼ ì‚¬ìš©í•œ ê²°ê³¼ë¥¼ ë…¼ë¬¸ì— ì“°ëŠ”ê±´ ì•½ê°„ ì• ë§¤í•œ ê²ƒ ê°™ë‹¤ ğŸ¤”

 

![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit2.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit2.PNG?raw=1)](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit2.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit2.PNG?raw=1))

í° ì‚¬ì´ì¦ˆì˜ pre-trained data(ImageNet-21kì™€ JFT-300M)ë¥¼ ì‚¬ìš©í•´ì„œ ì‹¤í—˜ì„ í–ˆì„ ë•Œ, ì´ì „ ì—°êµ¬ë“¤ê³¼ ë¹„êµí•´ë³´ë©´ ViTëŠ” State of the Art ì˜ ê²°ê³¼ë¥¼ ë‚¸ë‹¤ ! ğŸ˜

### 4.3 Pre-training Data Requirements

![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit8.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit8.PNG?raw=1)

- small datasetì— ëŒ€í•´ì„œë„ ì¢‹ì€ performanceë¥¼ ë‚¼ ìˆ˜ ìˆë„ë¡ weight decay, dropout, label smoothingì˜ ì„¸ê°€ì§€ regularization parameterë¥¼ optimizeí–ˆë‹¤.
- ImageNetê³¼ ê°™ì€ ì‘ì€ pre-trained datasetì„ ì‚¬ìš©í•˜ë©´, ì •ê·œí™”ë¥¼ ë§ì´ í•´ë„ ViTì˜ ì„±ëŠ¥ì´ BiTë³´ë‹¤ ë–¨ì–´ì§„ë‹¤. (Figure3)
- ì´ë¥¼ ë³´ë©´, ViT ëª¨ë¸ì€ í° ë°ì´í„°ì…‹ì—ì„œë§Œ ì¢‹ì€ ì„±ëŠ¥ì„ ëƒ„ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- pre-training sampleì˜ ìˆ˜ê°€ ì ì„ ë•Œì—ëŠ” ViTì˜ ì„±ëŠ¥ì´ ResNetë³´ë‹¤ ë–¨ì–´ì§„ë‹¤.(Figure4)
    - TransformerëŠ” CNNì— ë¹„í•´ [inductive biases](https://coding-chobo.tistory.com/97)ê°€ ë¶€ì¡±í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµë°ì´í„°ê°€ ì ì€ ê²½ìš°ì—ëŠ” í•™ìŠµì´ ì˜ ì•ˆëœë‹¤. ì‰½ê²Œ ë§í•˜ìë©´ transformerëŠ” CNNì— ë¹„í•´ ì´ë¯¸ì§€ì˜ íŒ¨í„´ì„ í•™ìŠµí•˜ê¸° ê°€ ì‰½ì§€ ì•Šë‹¤. (translation equivarianceë‚˜ localityë¥¼ í•™ìŠµí•˜ê¸°ê°€ ì–´ë ¤ì›€)

        ë”°ë¼ì„œ ìœ„ì˜ ê²°ê³¼ì²˜ëŸ¼ transformerëŠ” ë°ì´í„°ì…‹ì˜ í¬ê¸°ê°€ í´ ë•Œì— ë” í•™ìŠµì´ ì˜ëœë‹¤

 

### 4.4 Scaling Study

![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit3.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit3.PNG?raw=1)

![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit11.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit11.PNG?raw=1)

JFT-300M datasetì„ í†µí•´ ë‹¤ì–‘í•œ ëª¨ë¸ì— ëŒ€í•´ scalingì—°êµ¬ë¥¼ í•´ë³´ì•˜ë‹¤.

ì´ë•Œ, data sizeëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì— ë³‘ëª©í˜„ìƒì´ ë˜ì§€ ì•Šìœ¼ë©° ê° modelì˜ pre-training cost ë‹¹ ì„±ëŠ¥ì„ ë¹„êµí•´ë³¸ë‹¤.

- ViTëŠ” ì „ë°˜ì ìœ¼ë¡œ ResNetë³´ë‹¤ íš¨ìœ¨ì  : performance/compute trade-offì—ì„œ 2-4ë°° ë” ì¢‹ì€ ì„±ëŠ¥ì„ ëƒ„
- datasetì˜ í¬ê¸°ê°€ ì‘ì€ ê²½ìš°ì—ëŠ” Hybridê°€ ViTë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ëƒ„

### 4.5 Inspecting Vision Transformer

![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit12.gif?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit12.gif?raw=1)

ViTëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìˆœì„œëŒ€ë¡œ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•œë‹¤.

1. ì²«ë²ˆì§¸ layerëŠ” flattened patchë¥¼ lower Dimension spaceìœ¼ë¡œ ì„ í˜• íˆ¬ì˜í•œë‹¤.

    ![https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit13.PNG?raw=1](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/vit13.PNG?raw=1)

    Figure7ì˜ ì™¼ìª½ ê·¸ë¦¼ì€ í•™ìŠµëœ embedding filterì˜ êµ¬ì„±ìš”ì†Œë¥¼ ë³´ì—¬ì¤€ë‹¤. 

2. í•™ìŠµëœ position embeddingì„ patch representationì— ì¶”ê°€í•œë‹¤.

    Figure7ì˜ ì¤‘ê°„ ì‚¬ì§„ì„ ë³´ë©´ ê°€ê¹Œì´ì— ìˆëŠ” patchë“¤ì˜ position embeddingì´ ë¹„ìŠ·í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

3. Self-attentionì„ í†µí•´ í•˜ë‹¨ì˜ layerë¶€í„° ìƒë‹¨ì˜ layerê¹Œì§€ ì „ì²´ imageì— ëŒ€í•œ ì •ë³´ë¥¼ í†µí•©í•œë‹¤. ê° ì •ë³´ë“¤ì„ ëª¨ìœ¼ë©´ image spaceì—ì„œ ì´ë¯¸ì§€ê°„ í‰ê·  ê±°ë¦¬ì¸ attention distanceë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.  

## 5. Conclusion

ì´ ë…¼ë¬¸ì€ Transformerë¥¼ image recognitionì— ì§ì ‘ì ìœ¼ë¡œ ì ìš©í–ˆë‹¤. 
Computer Visionì— self-attentionì„ ì ìš©í•œ ì´ì „ì˜ ì—°êµ¬ë“¤ê³¼ëŠ” ë‹¬ë¦¬, ViTì—ì„œëŠ” architectureì— image-specific inductive biasesë¥¼ ì ìš©í•˜ì§€ ì•Šì•˜ë‹¤. ëŒ€ì‹  imageë¥¼ patchë‹¨ìœ„ë¡œ ìª¼ê°œ Transformer encoderì—ì„œ ì²˜ë¦¬í•´ì¤¬ë‹¤.
ì´ì™€ ê°™ì€ ê°„ë‹¨í•˜ë©´ì„œë„ í™•ì¥ê°€ëŠ¥í•œ modelì€ pre-trainingëœ í° ë°ì´í„°ì…‹ê³¼ ê²°í•©í–ˆì„ ë•Œ ì•„ì£¼ ì¢‹ì€ ì„±ëŠ¥ì„ ëƒˆìœ¼ë©°, ë§ì€ image classification datasetì—ì„œ SoTAì˜ ê²°ê³¼ë¥¼ ëƒˆë‹¤.

ë‚¨ì•„ìˆëŠ” Challenge

- ViTë¥¼ detectionì´ë‚˜ segmentationì— ì ìš©í•˜ëŠ” ì—°êµ¬
- pre-training methods
  