---
title: "[Paper Review] Reformer: The Efficient Transformer ë…¼ë¬¸ ë¶„ì„"
excerpt: " "

categories: 
  - nlp
tags: 
  - deeplearning
  - ai
  - nlp
  - transformer
layout: jupyter
search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 

---


> ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” Transformerì˜ architetureë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“  Reformer(2020)ì— ëŒ€í•´ ì‚´í´ë³¸ë‹¤.

- Paper : [REFORMER: THE EFFICIENT TRANSFORMER(2020)](https://arxiv.org/abs/2001.04451)
          Nikita Kitaev, Åukasz Kaiser, Anselm Levskaya

## 1. Introduction

[Transformer model(Vaswani et al, 2017)](https://arxiv.org/abs/1706.03762)ì€ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼(NLP)ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” architectureì´ë‹¤. ([Transformer ë…¼ë¬¸ ë° ì½”ë“œ](https://happy-jihye.github.io/nlp/nlp-8/#attention))

Transformerì˜ ê°€ì¥ í° ì¥ì ì€ í™•ì¥ì„±ì´ë‹¤. ì´ ëª¨ë¸ì—ì„œëŠ” ì•„ì£¼ í° ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œë„ í•™ìŠµì´ ì˜ë˜ê¸° ë•Œë¬¸ì— BERTë¥¼ ë¹„ë¡¯í•œ ë‹¤ì–‘í•œ ëª¨ë¸ì— ì°¨ìš©ì´ ë˜ì—ˆê³  ê³„ì†í•´ì„œ SoTAì˜ ê²°ê³¼ë¥¼ ë‚´ê³  ìˆë‹¤.

> ğŸ¤” ëŒ€ê·œëª¨ì˜ Researchì—ì„œë§Œ í•  ìˆ˜ ìˆëŠ” ì—°êµ¬ê°€ ì•„ë‹Œê°€?

transformerì˜ ëª¨ë¸ì´ ì ì  ì»¤ì§€ë©´ì„œ ì´ ëª¨ë¸ì„ í–¥í•œ ë¹„íŒì˜ ëª©ì†Œë¦¬ë„ ì»¤ì§€ê³  ìˆë‹¤. Large-scale long-sequence modelì„ ì‚¬ìš©í•˜ë©´ ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆê¸°ëŠ” í•˜ì§€ë§Œ, í•™ìŠµì„ ìœ„í•´ì„œëŠ” ë„ˆë¬´ë‚˜ë„ ë§ì€ resourceê°€ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ì ì  GPUì˜ ì‹¸ì›€ì´ ë˜ê³  ìˆëŠ” ê²ƒì´ë‹¤.

- **Ex) 64Kì˜ tokenì„ í•™ìŠµì‹œí‚¤ëŠ” ê²½ìš° (embedding size : 1024, batch size : 8)**
    - Parameters : $64K * 1K * 8 = 0.5B$
    - $2GB$ì˜ ë©”ëª¨ë¦¬ê°€ í•„ìš”
- BERTëŠ” ë” ë§ì€ corpusë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— $17GB$ì˜ memoryë¥¼ ì‚¬ìš©í•œë‹¤,,

ë”°ë¼ì„œ <u>Reformer modelì—ì„œëŠ” Transformer architectureë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ìˆ˜ì •í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì—°ì‚° ì†ë„</u> ë‘ë§ˆë¦¬ì˜ í† ë¼ë¥¼ ì¡ê³ ì í•œë‹¤.

### âœğŸ» Problem & Solution

Reformerì—ì„œëŠ” Transformerì—ì„œì˜ ë¹„íš¨ìœ¨ì„±ì„ ì´ì™€ ê°™ì€ ë°©ì‹ë“¤ë¡œ ê°œì„ í•˜ê³ ì í•œë‹¤.

> (1) Nê°œì˜ layerë¥¼ ì‚¬ìš©í•˜ë©´ single-layerë¥¼ ì‚¬ìš©í–ˆì„ ë•Œë³´ë‹¤ memoryë¥¼ Në°° ë” ì‚¬ìš©í•œë‹¤. (back-propagationê³¼ì •ì—ì„œ intermediatesë“¤ì„ ì €ì¥í•´ì•¼í•˜ê¸° ë•Œë¬¸)
> 
> **â‡’ Reversible Layersë¥¼ ì‚¬ìš©í•˜ì !**

> (2) Attentionì˜ depthëŠ” $d_{model}$ì´ì§€ë§Œ, Feed-Forward Layerì˜ depthëŠ” $d_{ff}$ /
> Feed-Forward Layerë¥¼ ì‚¬ìš©í•˜ë©´ ë„ˆë¬´ë‚˜ë„ ë§ì€ memoryê°€ í•„ìš”
>
> **â‡’ Feed-Forward layerì˜ $d_{ff}$ë¥¼ chunckingí•˜ê³  activation í•¨ìˆ˜ë¥¼ splití•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ ì•„ë¼ì !**

> (3) ê¸°ì¡´ì˜ Transformer : ê¸¸ì´ê°€ $L$ì¸ ë¬¸ì¥ì„ attentioní•˜ë©´ computational & memory complexityê°€ $O(L^2)$
>
> **â‡’ Locality-sensitive hashingì„ ì‚¬ìš©í•˜ì ! = complexity = $O(L logL)$** 

ìœ„ì˜ ë°©ì‹ì—ì„œ

(1) ì¼ë°˜ ëª¨ë¸ ëŒ€ì‹  **Reversible residuals** ë¥¼ ì ìš©í•´ë„ ì‹¤í—˜ê²°ê³¼ëŠ” ê±°ì˜ ë³€í•˜ì§€ ì•Šìœ¼ë©°

(2) **Splitting activation**ì€ ì˜¤ì§ êµ¬í˜„ë¶€ì—ë§Œ ì˜í–¥ì„ ì¤„ ë¿, ì‚¬ì‹¤ìƒ transformerì˜ layerëŠ” ë™ì¼í•˜ë‹¤.

(3) ë§ˆì§€ë§‰ìœ¼ë¡œ **Attentionì—ì„œ Locality-sensitive hashing** ë¥¼ ì‚¬ìš©í•˜ë©´ training ë°©ì‹ì—ëŠ” ì˜í–¥ì„ ì£¼ì§€ë§Œ, ê²°ê³¼ëŠ” full attentionê³¼ì˜ ë¹„ìŠ·í•˜ë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Reformer ëª¨ë¸ë¡œ 64Kì˜ text task(`enwiki8` )ì™€ 12Kì˜ image generation task(`imagenet-64` ) ì‹¤í—˜ì„ í–ˆë‹¤. ë˜í•œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì‹¤í—˜ì„ í†µí•´ Reformerê°€ Full Transformerì™€ ë¹„ìŠ·í•œ ê²°ê³¼ë¥¼ ë‚´ë©´ì„œë„ ë” ë¹ ë¥´ê³  memory-efficientí•˜ë‹¤ê³  ë§í•˜ê³  ìˆë‹¤.

## 2. Locality-Sensitive Hashing Attention

### Attention in Transformer

Transformerì˜ standard attention ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. Attention ê´€ë ¨í•œ ì„¤ëª…ì€ [ì´ ê¸€](https://happy-jihye.github.io/nlp/nlp-8/#attention) ì„ ì°¸ê³ 

$$\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V$$

ë˜í•œ, transformerì—ì„œëŠ” parallelí•œ ê³„ì‚°ì„ ìœ„í•´ **Multi-head attention**ì´ë¼ëŠ” mechanismì„ ì‚¬ìš©í•œë‹¤.

<p align="center"><img src="https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/transformer3.png?raw=1" width = "350" ></p>

> ì´ë•Œ Q K V ë²¡í„°ê°€ ì–´ë””ì„œ ì˜¤ëŠ”ì§€ ì‚´í´ë³´ì

multi-head attentionì—ì„œì˜ queries, keys, valuesëŠ” [batch size, length, dmodel]ì˜ $A$ ë¼ëŠ” single tensorì—ì„œ 3ê°€ì§€ì˜ linear layerë¥¼ projectí•¨ìœ¼ë¡œì¨ ì–»ì–´ì§„ë‹¤. 

- ê´€ë ¨ ë‚´ìš©ì€ [ì´ ë¶€ë¶„ ì½”ë“œ ì°¸ê³ ](https://happy-jihye.github.io/nlp/nlp-8/#multi-head-attention-layer)

### **Shared-QK Transformer**

LSH attentionì˜ modelì—ì„œëŠ” Queriesì™€ Keysê°€ ê·¼ë³¸ì ìœ¼ë¡œ ë™ì¼í•˜ë‹¤.  $A$ tensorì˜ ê°™ì€ linear layerì—ì„œ Qì™€ Kê°€ ë‚˜ì˜¤ê³ , $A$ì˜ ë˜ ë‹¤ë¥¸ layerì—ì„œ Vê°€ ì–»ì–´ì§€ëŠ” ê²ƒì´ë‹¤.

ì´ ë…¼ë¬¸ì—ì„œëŠ” sharing QKê°€ Transformerì˜ ì„±ëŠ¥ì— ì˜í–¥ì„ ì•ˆì£¼ë¯€ë¡œ ì´ ë°©ì‹ì„ ì‚¬ìš©í•˜ê³  ìˆë‹¤ê³  ë§í•œë‹¤.

---

### Hashing Attention

Transformerì˜ attention mechanismì˜ memoryì‚¬ìš©ëŸ‰ì„ ê³„ì‚°í•´ë³´ì.

- Query, Key, Value : [batch size, seq length, d model]
- $QK^T$ : [batch size, length, length]
- batch sizeê°€ `1`ì´ê³ , sequence length ê°€ `64K`, 32-bit floatë¥¼ ì‚¬ìš©í–ˆë‹¤ë©´, <u>$16GB$ì˜ ë©”ëª¨ë¦¬</u>ë¥¼ ì‚¬ìš©í•œë‹¤ !!

> ğŸ¤” $QK^T$ì˜ `64K x 64K` í–‰ë ¬ì´ memoryì— ë‹¤ ì €ì¥ë  í•„ìš”ê°€ ìˆì„ê¹Œ?

$$\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V$$

ìš°ë¦¬ëŠ” ìœ„ì˜ ì‹ì„ í†µí•´ attentionì„ ê³„ì‚°í•œë‹¤. ê° query ($q_i)$ì˜ attention ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$\operatorname{softmax}\left(\frac{q_{i} K^{I}}{\sqrt{d_{k}}}\right) V$$

ê·¸ëŸ°ë° ì‚¬ì‹¤ $QK^T$ì˜ ê°’ì„ `softmax` í•  ë•Œ ê²°ê³¼ê°’ì€ largest elementì—ë§Œ ì˜í–¥ì„ ë°›ëŠ”ë‹¤. 

ì¦‰, ëª¨ë“  queryì— ëŒ€í•´ keyì™€ ë‚´ì í•´ì£¼ì§€ ì•Šê³  queryì™€ ë¹„ìŠ·í•œ (queryì™€ ê°€ê¹Œìš´) keyì— ëŒ€í•´ì„œë§Œ ë‚´ì ì„ í•´ì¤˜ë„ ëœë‹¤.

- 64Kì˜ `key`ê°€ ìˆì„ ë•Œ, ê° `query` ëŠ” ëª¨ë“  `key`ì— ëŒ€í•´ attentionì„ í•˜ì§€ ì•Šê³ , ê°€ê¹Œìš´ 32 or 64ì˜ keyì— ëŒ€í•´ì„œë§Œ attentionì„ í•´ì¤˜ë„ ê´œì°®ìŒ ğŸ˜‰

### Locality sensitive hashing

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/reformer11.gif?raw=1" width = "700" ></p>

> hashing ($x$ â†’ $h(x)$ )ì—ì„œ ê°€ê¹Œì´ì— ìˆëŠ” vectorë¥¼ ë†’ì€ í™•ë¥ ë¡œ ê°™ì€ hashì— ë„£ê³ , ë¨¼ vectorë“¤ì€ ë‹¤ë¥¸ hashì— ë„£ëŠ” ê²ƒì„ **locality-sensitive**ë¼ê³  ë¶€ë¥¸ë‹¤.

ìœ„ì—ì„œ ê°€ê¹Œìš´ ì´ì›ƒì— ëŒ€í•´ì„œë§Œ attentionì„ í•´ì¤˜ë„ ë¨ì„ í™•ì¸í–ˆë‹¤.  ë”°ë¼ì„œ ìš°ë¦¬ëŠ” LSHë¼ëŠ” ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ nearest neighborë¥¼ ì°¾ëŠ”ë‹¤. 

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/reformer1.PNG?raw=1" width = "800" ></p>

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/reformer10.gif?raw=1" width = "500" ></p>

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” random projection ë°©ì‹ìœ¼ë¡œ **angular locality sensitive hash**ë¥¼ ì‚¬ìš©í–ˆë‹¤. ([LSH scheme](https://arxiv.org/abs/1509.02897) ë…¼ë¬¸ ì°¸ê³ ) randomìœ¼ë¡œ  $\theta$ë¥¼ êµ¬í•œ í›„, ì›ì˜ ì–´ëŠ ì§€ì ì— ìœ„ì¹˜í•˜ëŠ” ì§€ë¥¼ í™•ì¸í•˜ê³  ê·¸ êµ¬ê°„ì˜ indexë¥¼ ë¶™ì´ëŠ” ë°©ì‹ì´ë‹¤.

- We achieve this by employing random projections as follows (see Figure 1). To get b hashes, we
first fix a random matrix R of size $[dk, b/2].$ We then define $h(x) = arg max([xR; âˆ’xR])$ where $[u; v]$Â denotes the concatenation of two vectors. This method is a known LSH scheme (Andoni et al., 2015) and is easy to implement and apply to batches of vectors.

### LSH Attention

LSH Attentionì„ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.(ì´ ì‹ì—ì„œëŠ” $\sqrt{d_{k}}$ ì˜ scalingì€ ìƒëµë˜ì–´ìˆìŒ)

$$o_{i}=\sum_{j \in \mathcal{P}_{i}} \exp \left(q_{i} \cdot k_{j}-z\left(i, \mathcal{P}_{i}\right)\right) v_{j} \quad \text { where } \mathcal{P}_{i}=\{j: i \geq j\}$$

- $\mathcal{P}_{i}$ = $i$  ìœ„ì¹˜ì— ìˆëŠ” queryê°€ attendí•  ìˆ˜ ìˆëŠ” set
- $z$ = partition function (ex. softmaxì—ì„œì˜ normalizing)

Maskingê¹Œì§€ í¬í•¨í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$o_{i}=\sum_{j \in \widetilde{\mathcal{P}}_{i}} \exp \left(q_{i} \cdot k_{j}-m\left(j, \mathcal{P}_{i}\right)-z\left(i, \mathcal{P}_{i}\right)\right) v_{j} \quad \text { where } m\left(j, \mathcal{P}_{i}\right)=\left\{\begin{array}{ll}\infty & \text { if } j \notin \mathcal{P}_{i} \\0 & \text { otherwise }\end{array}\right.$$

---

> ì´ì œë¶€í„° LSH Attention ê³¼ì •ì— ëŒ€í•´ ì‚´í´ë³´ìğŸ˜Š

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/reformer2.PNG?raw=1" width = "800" ></p>

1. **ê°  tokenì˜ (Query, Key), Valueë¥¼ ìƒì„±í•œë‹¤.**
    - shared-QK Transformer : LSH attentionì—ì„œëŠ” Queriesì™€ keysê°€ ê°™ì€ linear layerì— ìˆì–´ì•¼í•¨

        <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/reformer3.png?raw=1" width = "400" ></p>

**2. Locality-Sensitive Hashing**

- Angular locality sensitive hash -> ë¹„ìŠ·í•œ tokenë“¤ì€ ê°™ì€ bucketì— ìˆì„ í™•ë¥ ì´ í¼
- ê°™ì€ bucketë‚´ì— queriesì™€ keysê°€ ë¶ˆê· í˜•í•˜ê²Œ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ìŒì˜ ì‹ì„ ë§Œì¡±í•˜ë„ë¡ Q, Kë¥¼ ë½‘ëŠ”ë‹¤. 
  $$\mathcal{P}_{i}=\left\{j: h\left(q_{i}\right)=h\left(k_{j}\right)\right\}$$

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/reformer4.png?raw=1" width = "400" ></p>

**3. Sort by LSH bucket**

- ë¬¸ì¥ ìˆœì„œëŒ€ë¡œ bucketì„ sortingí•œë‹¤.

**4. Chunk sorted sequence to parallelize**

- ê³ ì •ëœ í¬ê¸°ë¡œ sequenceë¥¼ split

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/reformer5.png?raw=1" width = "500" ></p>

**5. Attention ì ìš©**

- ê°™ì€ bucket ë‚´ì—ì„œ attention
- ì´ì „ chunkì— ëŒ€í•´ì„œ attention
<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/reformer6.png?raw=1" width = "500" ></p>


> Chunking Attentionì„ í•˜ëŠ” ì´ìœ  
> - ë™ì¼í•œ bucketì— ìˆëŠ” tokenë“¤ì´ ë§ì•„ë„ íŠ¹ì • ê¸¸ì´ ì´ìƒ attentionì„ í•  ìˆ˜ ì—†ë„ë¡ í•˜ê¸° ìœ„í•´

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/reformer7.png?raw=1" width = "500" ></p>

---

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/reformer9.gif?raw=1" width = "700" ></p>

---

### Multi-round LSH Attention

ìœ„ì™€ ê°™ì´ LSH attentionì„ í•˜ë©´ ëª‡ê°€ì§€ ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆë‹¤.

- Hash bucketì˜ sizeê°€ ê³ ë¥´ì§€ ì•Šì„ ìˆ˜ë„ ìˆê³ 
- bucket ë‚´ì˜ queriesì™€ keysì˜ ìˆ˜ê°€ ì œê°ê°ì¼ ìˆ˜ë„ ìˆë‹¤.

ì¦‰, ì´ë ‡ê²Œ ë˜ë©´ ë¹„ìŠ·í•œ itemë“¤ì´ ë‹¤ë¥¸ bucketì— ë–¨ì–´ì§ˆ ìˆ˜ ìˆê²Œ ëœë‹¤.

ë”°ë¼ì„œ ì´ ê²½ìš°ì—ëŠ” LSH Attention ì—¬ëŸ¬ë²ˆ í•˜ëŠ” ë°©ì‹ì¸ Multi-round LSH Attention ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.  

$$\mathcal{P}_{i}=\bigcup_{r=1}^{n_{\text {round }}} \mathcal{P}_{i}^{(r)} \quad \text { where } \mathcal{P}_{i}^{(r)}=\left\{j: h^{(r)}\left(q_{i}\right)=h^{(r)}\left(q_{j}\right)\right\}$$

### Causal masking for shared-QK attention

Transformerì˜ attentionì—ì„œëŠ” ìê¸° ìì‹ ì„ í¬í•¨í•˜ì—¬ attentionì„ í•  ìˆ˜ ìˆì—ˆë‹¤. ë‹¤ë§Œ, shared-QKë¥¼ ì‚¬ìš©í•˜ë©´, ìê¸°ìì‹ ì— ëŒ€í•´ ë‚´ì ì„ í–ˆì„ ê²½ìš° ì´ ê°’ì´ ë„ˆë¬´ë‚˜ë„ ì»¤ì ¸ ì „ì²´ attentionê°’ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤.

ë”°ë¼ì„œ ìê¸° ìì‹ ì— ëŒ€í•´ì„œëŠ” attentionì„ í•˜ì§€ ëª»í•˜ë„ë¡ ì¡°ì •ì„ í•´ì¤€ ê²ƒì´ **Causal masking for shared-QK attention**ì´ë‹¤.

## 3. Reversible Transformer

### RevNets

Transformerì— ì‚¬ìš©ë˜ëŠ” Feed-Forward NNì—ì„œ Back-Propagationì„ í•˜ë ¤ë©´ ì¤‘ê°„ê²°ê³¼ë¬¼ì„ ë©”ëª¨ë¦¬ì— ì €ì¥í•´ì•¼ í•œë‹¤. 

- Memory use =  $b \times l \times d_{ff} \times n_l$

    = batch size * dim of layer * dim of feed-forward * numbers of layer

    ex) 1 * 64000(dim of layer) * 16(# of layer) * **4000 (dim of feed-forward)** * 4 (float32) = **16GB**

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/reformer12.png?raw=1" width = "800" ></p>

ë”°ë¼ì„œ [Reversible Residual Network(2017)](https://arxiv.org/pdf/1707.04585.pdf) Architectureë¥¼ ì‚¬ìš©í•´ì„œ intermediateë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ì§€ ì•Šê³ , outputìœ¼ë¡œ inputì„ ê³„ì‚°í•˜ë„ë¡ í•˜ì˜€ë‹¤.

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/reformer13.PNG?raw=1" width = "800" ></p>

$$Y_{1}=X_{1}+\text { Attention }\left(X_{2}\right) \quad Y_{2}=X_{2}+\text { FeedForward }\left(Y_{1}\right)$$

> ì´ ë°©ì‹ì„ ì‚¬ìš©í•˜ë©´, layerì˜ ìˆ˜ë§Œí¼ ë©”ëª¨ë¦¬ë¥¼ ì €ì¥í•˜ì§€ ì•Šì•„ë„ ë˜ë¯€ë¡œ Table3ì˜ $n_l$ termì´ ì‚¬ë¼ì§„ë‹¤.

---

### Chunking

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/reformer19.png?raw=1" width = "800" ></p>

$n_l$ termì„ ì§€ì›Œë„ ì—¬ì „íˆ ë©”ëª¨ë¦¬ëŠ” ë„ˆë¬´ë‚˜ë„ í¬ë‹¤ ($d_{ff} = 4K$)

ë”°ë¼ì„œ Feed Forward NNì—ì„œì˜ ì—°ì‚°ì„ ë³‘ë ¬ì ìœ¼ë¡œ í•  ìˆ˜ ìˆë„ë¡ chunkingì„ í•œë‹¤.

$$ Y_{2}=\left[Y_{2}^{(1)} ; \ldots ; Y_{2}^{(c)}\right]=\left[X_{2}^{(1)}+\text { FeedForward }\left(Y_{1}^{(1)}\right) ; \ldots ; X_{2}^{(c)}+\text { FeedForward }\left(Y_{1}^{(c)}\right)\right]$$

---

## Memory and Time Complexity

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/reformer14.PNG?raw=1" width = "850" ></p>

---

## 4. Experiments

- `Imagenet64` ë‘ `enwik8-64k` ë¡œ ì‹¤í—˜
    - $d_{model}=1024, d_{ff}=4096, n_{heads}=8$
    - Adafactor optimizer
- evaluation : `WMT 2014` Eng-Germ translation task

### Effect of sharing QK / reversible layers

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/reformer15.png?raw=1" width = "800" ></p>
---
<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/reformer18.png?raw=1" width = "800" ></p>

### LSH attention in Transformer

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/reformer16.png?raw=1" width = "800" ></p>

### Large Reformer Models

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/reformer17.png?raw=1" width = "800" ></p>

## 6. Opinions

> ì¬ë¯¸ë‚œ ë°©ì‹ìœ¼ë¡œ Transformerì˜ mechanismì„ í’€ì–´ê°„ ì¬ë¯¸ë‚œ modelì´ë¼ëŠ” ìƒê°ì´ ë“ ë‹¤. 
> 
> ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•œ ì‹¤í—˜ì˜ ì§€í‘œë¡œ bpdë¥¼ ì“´ê²Œ ì•½ê°„ì€ ì•„ì‰½ë‹¤. ë˜í•œ, Reformer ìì²´ë¥¼ transformerì™€ ë¹„êµí•˜ëŠ” ì‹¤í—˜ì´ ì•½ê°„ ë¶€ì¡±í•˜ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆë‹¤.
> 
> ê·¸ë˜ë„ ì•„ì´ë””ì–´ ìì²´ê°€ ê´œì°®ê³  ë…¼ë¬¸ë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì“°ì—¬ìˆì–´ì„œ ì¬ë¯¸ë‚˜ê²Œ ì½ì—ˆë˜ ë…¼ë¬¸ì´ë‹¤. ê´€ë ¨ ë…¼ë¬¸ìœ¼ë¡œ Longformer, Performer, Sparse attention, Big Bird ë“±ì˜ ë¹„ìŠ·í•œ ë…¼ë¬¸ë“¤ë„ ì½ì–´ë³´ë©´ ë„ì›€ì´ ë  ê²ƒ ê°™ë‹¤.

## Reference

[reformer transformer gif - Google Search](https://www.google.com/search?q=reformer+transformer+gif&tbm=isch&ved=2ahUKEwjC6fzthuDvAhWXBaYKHfArCPkQ2-cCegQIABAA&oq=reformer+transformer+gif&gs_lcp=CgNpbWcQA1CEGFiNJGCdJWgAcAB4AIABpgGIAccKkgEEMS4xMZgBAKABAaoBC2d3cy13aXotaW1nwAEB&sclient=img&ei=FVBnYIKLK5eLmAXw16DIDw&bih=575&biw=1230#imgrc=_5oR5COFaoumcM)

[Machine learning ìŠ¤í„°ë”” (18) Neural Network Introduction - README](http://sanghyukchun.github.io/74/)