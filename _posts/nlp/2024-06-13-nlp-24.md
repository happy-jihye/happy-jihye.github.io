---
title: "[4] RAG (Retriever Augumented Generation)"
excerpt: " "

categories: 
  - nlp
tags: 
  - deeplearning
  - ai
  - nlp
  - LLM

search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/24-rag/0.png?raw=1" width = "700" ></p>

## [1] Data Creation

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/24-rag/1.png?raw=1" width = "500" ></p>

### Query Expansion

- Query decompose
    - Visconde: Multi-document QA with GPT-3 and Neural Reranking
- HyDE: Hypothetical Document Embeddings
    
    <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/24-rag/2.png?raw=1" width = "700" ></p>
    
    - GPT 모델을 활용하여 inference 만으로 데이터를 수집 → 주어진 query에 대한 hypothetical document를 생성 (이 document는 생성된 것이므로 틀릴수도 있음)
    - contriver 모델같은 unsupervised learned encoder를 통해 문서를 encoding → 이후 실제 문서에서 비슷한 vector를 search (corpus embedding space에서의 similarity를 계산)
        - 주어진 query에 대해 잘못된 생성된 정보를 사용하지 않고, 실제 corpus의 문서를 사용할 수 있음
- Multi Query Expansion
    - [langchain MultiQueryRetriever](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/MultiQueryRetriever/)


## [2] Retrieval

### **Sparse retrieval**

- BoW를 구성하는 n-gram 방식
- TF-IDF
    - TF (Term-Frequency)
        - document에 term이 등장하는 빈도. (빈도수가 높을수록 문서의 검색 점수는 높아짐.)
        - 문서 내에 같은 단어가 여러번 등장하면 그 단어에 가중치를 줌
    - IDF (Inverse Document Frequency)
        - 전체 문서에서 등장하는 term의 빈도수(DF)의 역수(Inverse)
        - term이 여러 문서에 등장할수록 낮은 가중치를 줌
    - TF-IDF: 단어가 얼마나 반복되는지(TF)와 얼마나 자주 사용되는 단어인지 (IDF)
- BM25
    - 주어진 query에 대해 문서와의 연관성을 평가하는 ranking 함수. (TF-IDF 의 변형)
    - BM25는 TF-IDF보다 좋음. WHY?
        - TF의 영향이 줄어듦. TF에서는 문서 내 단어빈도가 높아질수록 검색 점수도 좋아지지만, BM25에서는 특정값으로 수렴
        - IDF의 영향이 커짐
            - BM25에서는 DF가 높아지면 검색점수가 0으로 수렴하므로, [불용어](https://wikidocs.net/22530)가 검색점수에 영향을 덜 줌
        - 문서 길이의 영향이 줄어듦.
            - BM25는 TF-IDF 와 다르게 문서 길이에 대한 정규화를 해서 문서 길이가 점수에 영향을 미치지 않음
    - elasticsearch의 기본 유사도 알고리즘
    - 특정 도메인을 사용하는 경우 BM25가 vectorDB보다 좋음
    - 형태소 분석을 잘해야 (tokenizer가 좋아야) BM25의 성능이 좋음 [(참고)](https://medium.com/@autorag/%ED%95%9C%EA%B5%AD%EC%96%B4-%EB%AC%B8%EC%84%9C%EC%97%90%EC%84%9C-bm25-%EC%82%AC%EC%9A%A9-%EC%8B%9C-%EA%BC%AD-%ED%99%95%EC%9D%B8%ED%95%B4%EC%95%BC-%ED%95%A0-%EA%B2%83-1edef9daddfd)

### **Dense retrieval - VectorDB**

- [embedding model (참고)](https://medium.com/@lars.chr.wiik/best-embedding-model-openai-cohere-google-e5-bge-931bfa1962dc)
    - OpenAI) text-embedding-3-large
        - 24.01.25, multilingual, 256/1024/3072 dim 지원
    - Cohear) embed-multilingual-v3.0
        - 23.11.02, multilingual, 1024 dim
    - Google) text-multilingual-embedding-preview-0409
        - 24.04.02, multilingual, 768 dim
    - Microsoft) E5
        - 2024 초, multilingual, 384/758/1024 dim
    - BGE-M3
        - 24.01.30, multilingual, 1024 dim
        - M3:  Multi-linguality (100+ languages), Multi-granularities (input length up to 8192), Multi-Functionality (unification of dense, lexical, multi-vec (colbert) retrieval

### **Hybrid**

- [RRF (Reciprocal Rank Fusion)](https://velog.io/@acdongpgm/NLP.-Reciprocal-rank-fusion-RRF-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0)
    - 서로 다른 관련성 지표 (relevance indicators)를 가진 여러개의 result sets을 하나의 결과 집합으로 결합하는 방법 (ex. BM25와 vectorDB의 리트리벌 결과를 모두 사용)
    - 모든 검색 엔진의 역수 순위 점수를 합산하여 최종 순위를 결정
- CC (Convex Combination)
    - RRF랑 비슷. multiple source의 리트리벌 정보를 사용
    - Convex Combination 알고리즘을 활용하여 각 리트리벌 간의 가중치가 다른 점수 계산이 가능
    - 검색엔진의 중요도에 따라 가중치를 조절하여 리트리벌 결과를 뽑음
- [RSF (Relative Score Fusion)](https://marker-inc-korea.github.io/AutoRAG/nodes/retrieval/hybrid_rsf.html)
    - 각 검색 엔진 마다 다른 스코어 범위를 갖고 있으므로, 이를 상대 스코어로 정규화한 후 (검색 엔진간 스코어 스케일 차이 극복) 변환된 상대 스코어를 합산하여 최종 스코어 계산
- DBSF (Distributtion-based Score Fusion)
    - RSF와 유사. RSF는 min-max normalization을 사용했다면, DBSF는 distribution을 고려한 (mean, std 고려) min-max score를 적용
    - 검색엔진 간 스코어 분포의 차이를 보다 정교하게 고려 가능

## [3] ReRanking

- [LLM Reranking](https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6)
- [UPR](https://velog.io/@vkehfdl1/UPR-Reranker-%ED%95%B5%EC%8B%AC%EB%A7%8C-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-%EC%BD%94%EB%93%9C%EA%B9%8C%EC%A7%80) ([Improving Passage Retrieval with Zero-shot Question Generation](https://arxiv.org/abs/2204.07496))
- [TART](https://velog.io/@vkehfdl1/TART-%ED%95%B5%EC%8B%AC%EB%A7%8C-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-%EC%BD%94%EB%93%9C%EA%B9%8C%EC%A7%80) ([Task-aware Retrieval with Instructions](https://arxiv.org/pdf/2211.09260))

---
**Reference**

[Elasticsearch 유사도 알고리즘 (TF/IDF, BM25 비교)](https://velog.io/@mayhan/Elasticsearch-유사도-알고리즘)