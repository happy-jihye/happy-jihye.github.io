---
title: "[3-1] A Survey of Large Language Models - ë‹¤ì–‘í•œ LLMsë¶€í„°, Data, Architecture, Training ê¹Œì§€"
excerpt: " "

categories: 
  - nlp
tags: 
  - deeplearning
  - ai
  - nlp
  - LLM

search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 

use_math: true
---

## Language Model

- Statistical LM (SLM)
- Neural LM (NLM)
- Pretrained LM (PLM)
- Large LM (LLM)

## [1] Resources of LLM

### LLM

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/0.png?raw=1" width = "900" ></p>

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/1.png?raw=1" width = "900" ></p>

### GPT series

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/2.png?raw=1" width = "800" ></p>

- (1) training decoder-only Transfer (2) scaling up the size of language models
- early explorations
    - decoder-only transformer êµ¬ì¡° ë•ë¶„ì— unsupervised learning ê°€ëŠ¥ & ë‹¤ì–‘í•œ taskë¥¼ í•œë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë¨
- capacity leap
    - GPT1,2 â†’ GPT3 ë¡œ ë„˜ì–´ì˜¤ë©´ì„œ model capacityë¥¼ í™• í‚¤ì› ìŒ
    - ì´ë•Œë¶€í„° in-context learning (ICL)ì´ ì˜ë¨
        - â†’ LLMì„ teach (instruct)í•˜ê²Œ ê°€ëŠ¥í•´ì§„ë‹¤ëŠ” ê²ƒ
- capacity enhancement
    - training on code data
        - â†’ ëª¨ë¸ì˜ reasoning ì„±ëŠ¥ì„ ë†’ì„. CoT ë„ ë” ì˜ë¨
    - human alignment
        - InstructGPT: GPT-3 ëª¨ë¸ì„ human alignmentì— ë§ê²Œ finetuning (3 stage RLHF)

### LLaMA series

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/3.png?raw=1" width = "800" ></p>

- LLaMA ëŠ” ì„±ëŠ¥ë„ ì¢‹ê³  opensoure ì„
    - Llamaë¥¼ instuction tuning + continual pretraining í•˜ë ¤ëŠ” ì›€ì§ì„ì´ ë§ìŒ
        - íŠ¹íˆ LLaMA â†’ non-englishë¡œ adaptationí•˜ë ¤ê³  original vocabì„ extend í•˜ê±°ë‚˜ target language dataë¡œ fine-tuning í•˜ë ¤ëŠ” ì›€ì§ì„ë„ ë§ìŒ
    - [architecture](https://happy-jihye.github.io/nlp/nlp-12/#22-architecture)
        - (1) ReLU ëŒ€ì‹  SwiGLU (2) PE ëŒ€ì‹  RoPE (3) LN ëŒ€ì‹  RMSNorm
- Vicuna: LLaMAë¥¼ sharedGPTë¡œ ë¶€í„° ìˆ˜ì§í•œ user ë°ì´í„°ë¡œ fine-tuningí•˜ëŠ” ëª¨ë¸. ì´í›„ multimodal ëª¨ë¸ë¡œ ë°œì „
    - â†’ LLaVA, miniGPT4, InstructBLIP,

### Others

- Mistral
- **PaLM Family**
- **FLAN**
    
<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/4.png?raw=1" width = "550" ></p>
    
- Gopher

## [2] Data

. | | |
--|--|--
![](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/5.png?raw=1)|![](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/6.png?raw=1) | ![](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/7.png?raw=1) 


### 2.1 Data Collection

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/8.png?raw=1" width = "800" ></p>

### 2.2 Data Preprocessing

- **Quality Filtering**
    - (1) classifier-based
        - binary classifierë¥¼ í›ˆë ¨ì‹œí‚¨ í›„ ë¶„ë¥˜
        - but ê³ í€„ì˜ ë°ì´í„°ë¥¼ ì‹¤ìˆ˜ë¡œ ì§€ìš°ëŠ” ê²½ìš°ë„ ë§ìŒ (â†’ pretraining corpusì˜ diversityë¥¼ í›¼ì†)
    - (2) heuristic-based
        - BLOOM, Gopher ê°™ì€ ëª¨ë¸ì€ heuristicí•˜ê²Œ ì œê±° (well-designed rulesì— ë”°ë¼ low qual textë¥¼ ë¶„ë¥˜)
            - language based filtering: LLMìœ¼ë¡œ í•„í„°ë§
            - metric based filtering: eval metricìœ¼ë¡œ ì´ìƒí•œ ë¬¸ì¥ í•„í„°ë§
            - statistic based filtering
            - keyword based filtering: HTML tags, hyperlink ì™€ ê°™ì€ noisyí•œ ë°ì´í„° ì œê±°
- **De-duplication**
    - sentence level: repetitive patternì„ ë§Œë“¤ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë°˜ë³µëœ ë‹¨ì–´ë‚˜ phraseëŠ” ì œê±°í•´ì•¼í•¨
    - document level
    - dataset level
    - https://tech.scatterlab.co.kr/deduplication/
- Privacy reduction
    - personally identifiable information (PII)ë¥¼ ì œê±°í•´ì•¼í•¨
- tokenization
    - word-based tokenization â†’ **subword tokenizers** (transformer ê³„ì—´ë¶€í„° ì´ë¥¼ ë§ì´ ì‚¬ìš©)
    - [Byte-Pair Encoding (BPE) tokenization](https://wikidocs.net/22592)
        - subword segmentation ì•Œê³ ë¦¬ì¦˜ (ê¸°ì¡´ì— ìˆë˜ ë‹¨ì–´ë¥¼ ë¶„ë¦¬í•œë‹¤.)
        - character ë‹¨ìœ„ì—ì„œ ìì£¼ ë³´ëŠ” ë¬¸ìë“¤ë¼ë¦¬ merge í•˜ì—¬ vocabì„ ì„œì„œíˆ ë§Œë“¤ì–´ë‚´ëŠ” bottom-up ë°©ì‹. ë¯¸ë¦¬ ì •ì˜ëœ vocab sizeì— ë„ë‹¬í•  ë•Œê¹Œì§€ ê»˜ì† ì´ ê³¼ì • ë°˜ë³µ
        - ë¯¸ë“±ë¡ ë‹¨ì–´(out-of-vocabulary, OOV)ì— ëŒ€í•´ robustnessë¥¼ ì œê³µí•¨
            - ë‹¨ì–´ë¥¼ ë” ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„ì¸ subword ë‹¨ìœ„ë¡œ ë¶„í•´ (í˜•íƒœí•™ì  ë¶„í•´) í•˜ê¸° ë•Œë¬¸ì— OOV ë‹¨ì–´ë„ ìœ ì‚¬í•œ íŒ¨í„´ì˜ subwordë¡œ í† í°í™” ê°€ëŠ¥
        - multilingual corpusì„ ì˜ ì²˜ë¦¬
            - ì–¸ì–´ ë…ë¦½ì ì¸ í† í°í™” - ì¼ê´€ëœ ë°©ì‹ìœ¼ë¡œ tokení™”ë¥¼ ì˜í•¨
            - ë‹¤ì–‘í•œ ì–¸ì–´ëŠ” subwordë¥¼ ê³µìœ í•˜ëŠ” ê²½ìš°ë„ ë§ì€ë°, BPEëŠ” ì´ë¥¼ ì˜ ì²˜ë¦¬
        - GPT-2, BART, LLaMA ë“±ì—ì„œ ì‚¬ìš©
    - **WordPiece tokenization**
        - BPEë‘ ìœ ì‚¬í•˜ë‚˜, merge í•  ë•Œ likelihood ê¸°ë°˜ ë°©ì‹ì„ ì‚¬ìš©
        - merge) language modelì„ í›ˆë ¨ì‹œí‚¨ í›„, training dataì˜ likelihoodë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ merge
        - BERT, DistilBERT
    - **Unigram tokenization**
        - í° vocabì—ì„œ í•„ìš”ì—†ëŠ” tokenë“¤ì„ í•˜ë‚˜ì”© ì œê±°í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜
        - trained unigram language modelì„ í™œìš©í•´ì„œ ìµœì ì˜ ë‹¨ì–´ë¥¼ tokenization â†’ ì´í›„ expectation-maximization (EM) ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ unigramì˜ í™•ë¥  ì¶”ì • ë° language model update

### 2.3 Data scheduling

**Data Mixture**

- increasing the diversity of data sources
- optimizing data mixtures
    - (1) target downstream taskê°€ ìˆì„ ë•Œ, feature spaceì— ì˜ ê·¼ì‚¬í•˜ë„ë¡ pretraining dataì„ selecting (2) downstream task ì— ê¸ì •ì ì¸ ì˜í–¥ì„ ì£¼ëŠ” data ì±„íƒ (3) â€¦
    - ex) [DoReMI](https://github.com/sangmichaelxie/doremi)

**Data Curriculum**

- Coding
    - CodeLLaMA: LLaMA2 â†’ ì¶”ê°€ í•™ìŠµ (2T general tokens â†’ 500B code-heavy tokens)
    - CodeLLaMA-Python (2T general tokens â†’ 500B code-heavy tokens â†’ 100B Python-heavy tokens)
- Mathematics
    - Llemma: CodeLLaMA-Python (2T general tokens â†’ 500B code-heavy tokens â†’ 10(2T general tokens â†’ 500B code-heavy tokens â†’ 50âˆ¼200B math-heavy tokens).
        - Note that the pre-training data of Llemma also contains 5% general domain data as a form of regularization.
- Long context
    - RoPE-based LLMì˜ position embeddings ì±„íƒ
    - CodeLLaMA further extends the context window of LLaMA 2 (2.5T tokens with 4K context window â†’ 20B tokens with 16K context window)
    - LongLLaMA also achieves longer context window with the help of external memory and a unique training objective (1T tokens with 2K context window â†’ 10B tokens with 8K context window).

## [3] Architecture

### 3.1 architecture

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/9.png?raw=1" width = "700" ></p>

**Encoder-decoder Architecture**

- vanilla Transformer
    - encoder: multi-head self-attention (input seq â†’ encoding â†’ latent representations)
    - decoder: ì´ representationë“¤ì— ëŒ€í•´ cross-attention. auto-regressive í•˜ê²Œ ìƒì„±

**Casual Decoder Architecture**

- ìµœê·¼ LLMì—ì„œ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” êµ¬ì¡°. inputê³¼ target ì‹œí€€ìŠ¤ë¥¼ êµ¬ë¶„ ì—†ì´ ì²˜ë¦¬í•˜ë©°, causal masking patternìœ¼ë¡œ ì¸í•´ ì´ì „ í† í°ì—ë§Œ conditionë¨
- unidirectional attention mask
    - ê°ê°ì˜ input tokenì´ past tokensë‘ ìì‹ ë§Œ attend ê°€ëŠ¥
    - ì´í›„ input & output tokenì€ decoderë¡œ ë“¤ì–´ê°€ì„œ ì²˜ë¦¬
- GPT series
    - íŠ¹íˆ GPT-3ëŠ” ì´ architectureê°€ in-context learningì— ë§¤ìš° íš¨ê³¼ì ì„ì„ ë³´ì„

**Prefix Decoder Architecture (non-casual decoder only)**

- decoder-only ëª¨ë¸ì´ ì…ë ¥/ì»¨ë””ì…”ë‹ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ë” í’ë¶€í•œ non-causal representationì„ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ attention maskë¥¼ ìˆ˜ì •í•œ êµ¬ì¡°
    - â†’ prefix tokenì— ëŒ€í•´ì„œëŠ” bidirectional attentionì„ ìˆ˜í–‰ (encoder-decoder ëª¨ë¸ì²˜ëŸ¼)
    - â†’ generated tokenì— ëŒ€í•´ì„œëŠ” unidirectional attentionì„ ìˆ˜í–‰
- ìµœê·¼ì—ëŠ” scratch ë¶€í„° pretrainingì„ í•˜ì§€ ì•Šê³ , causual decoderë¥¼ í•™ìŠµì‹œí‚¤ë‹¤ê°€ training ê°€ì†í™”ë¥¼ ìœ„í•´ prefix decoderë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ì—°êµ¬ë„ ì§„í–‰ë¨
    - ex) PaLM â†’ U-PaLM

**Mixture-of-Experts (MoE)**

- [LLM ì•„í‚¤í…ì²˜ì— Mixture of Experts(MoE)ë¥¼ í™œìš©í•˜ê¸°](https://developer.nvidia.com/ko-kr/blog/applying-mixture-of-experts-in-llm-architectures/) ê¸€ ì°¸ê³ 
    - í•˜ë‚˜ì˜ ë ˆì´ì–´ ë˜ëŠ” ì—°ì‚°(ì˜ˆ: ì„ í˜• ë ˆì´ì–´, MLP ë˜ëŠ” attention projection)ì˜ ê³„ì‚°ì„ ì—¬ëŸ¬ ê°œì˜ â€œì „ë¬¸ê°€(expert)â€ í•˜ìœ„ ë„¤íŠ¸ì›Œí¬ë¡œ ë¶„í• í•˜ëŠ” ì‹ ê²½ë§ì˜ ì•„í‚¤í…ì²˜ íŒ¨í„´
    - (1) expertì˜ í•˜ìœ„ ë„¤íŠ¸ì›Œí¬
    - (2) routing ì•Œê³ ë¦¬ì¦˜
- computational costë¥¼ ìœ ì§€í•˜ë©´ì„œ model parameterë¥¼ scale upí•  ìˆ˜ ìˆìŒ. experts ìˆ˜ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ total paramì„ ëŠ˜ë¦¬ë©´ ì„±ëŠ¥ì´ í–¥ìƒë¨
- ex) Mixtral 8x7B, GPT-4 (ì•„ë§ˆ 110B x 16 way MoE, top-k routhing k=2)
    
<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/10.png?raw=1" width = "400" ></p>
    
- ë¬¸ì œ) instability (routing operationì´ complex + hard-switching)
- MoE-based LM ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ routing moduleì— high-precision tensorsë¥¼ ì“´ë‹¤ë˜ê°€, smaller rangeì˜ ëª¨ë¸ë¡œ initializing í•œë‹¤ë˜ê°€ í•˜ëŠ” ë°©ë²•ë¡ ì´ ìƒê¸°ê³  ìˆìŒ

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/11.png?raw=1" width = "800" ></p>

### 3.2 Transformerì˜ ì£¼ìš” configuration

(1) normalization (2) positional embeddings (3) activation functions (4) attention & bias

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/12.png?raw=1" width = "800" ></p>

ì¼ë°˜í™” ì„±ëŠ¥ê³¼ í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•´ pre RMSNorm, SwiGLU/GeGLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê³ , ì„ë² ë”© ë ˆì´ì–´ ì§í›„ì—ëŠ” LNì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¢‹ìŒ. ë˜í•œ, ìœ„ì¹˜ ì„ë² ë”©ìœ¼ë¡œëŠ” RoPEë‚˜ ALiBiê°€ ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ë” ì¢‹ìŒ.

**(1) Normalization** 

- LayerNorm
    - BatchNormì€ ë‹¤ì–‘í•œ ê¸¸ì´ë¥¼ ê°€ì§„ sequence dataë¥¼ ë‹¤ë£¨ê¸° ì–´ë ¤ì› ìŒ â†’ layernorm ë“±ì¥: sequenceì— ë”°ë¥¸ ê³ ì •ê¸¸ì´ë¥¼ ì •ê·œí™”.
- RMSNorm
    - LNì˜ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ RMSNorm ë“±ì¥
        - activation ê°’ë“¤ì˜ re-scalingë§Œ ì§„í–‰: mean & variance ê³„ì‚° ëŒ€ì‹  activationsì˜ í•©ì— ëŒ€í•´ RMS
    - Gopher, Chinchilla, LLaMAê°€ RMSNorm ì‚¬ìš©
- DeepNorm
    - deep transformerë¥¼ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•´ Microsoftì—ì„œ ì œì•ˆ
    - DeepNormì„ residual connection ì²˜ëŸ¼ ì‚¬ìš© â†’ Transformerë¥¼ 1000 layer ë¡œ í™•ì¥ ê°€ëŠ¥. + ì„±ëŠ¥, stability ì¢‹ìŒ
    - GLM-130Bì—ì„œ ì‚¬ìš©

**(2) Normalization Position**

- post-LN
    - vanilla Transformerì—ì„œ ì‚¬ìš©. residual blocks ì‚¬ì´ì— ìœ„ì¹˜
    - But post-LNì€ output layer ê·¼ì²˜ì˜ í° gradient ë•Œë¬¸ì— ì•ˆì •ì ì´ê²Œ í•™ìŠµì´ ì•ˆë¨
    - â†’ ìµœê·¼ì—” ì˜ ì‚¬ìš© ì•ˆí•¨
- Pre-LN
    - ê°ê°ì˜ sub-layer ì „ì— ìœ„ì¹˜í•¨. ì¶”ê°€ LNì€ final prediction ì „ì— ìœ„ì¹˜
        - â†’ post-LNì— ë¹„í•´ í•™ìŠµ ë•Œ stableí•¨
    - GLM ê°™ì€ í° ëª¨ë¸ì—ì„œ unstable
- Sandwich-LN
    - pre-LN ê¸°ë°˜. residual connection ì „ì— extra LNì„ ì¶”ê°€ â†’ Transformer layer outputì˜ value explosion ë°©ì§€
    - Sandwich-LNì„ ì‚¬ìš©í•˜ë©´ ê°€ë” í•™ìŠµì˜ stabilityê°€ ë§ê°€ì§€ê¸°ë„ í•¨ (â†’ training collapse)

**(3) activation functions** 

- in existing LLMs, GeLU activationsì´ ë§ì´ ì‚¬ìš©
- ìµœê·¼ì—” GLU activationì´ ë§ì´ ì‚¬ìš©
    - SwiGLU, GeGLU â€¦ â†’ ì„±ëŠ¥ì€ ë§¤ìš° ì¢‹ì§€ë§Œ, GeLUë³´ë‹¤ 50% ë” paramì´ ë§ìŒ

**(2) positional embeddings** 

- Absolute position embedding
    - vanilla Transformerì—ì„œ ì‚¬ìš©
    - sinusoidal & learned position embedding
    - ê° ìœ„ì¹˜ë§ˆë‹¤ ê³ ì •ëœ embedding vectorë¥¼ ì‚¬ìš©
    - ê° ìœ„ì¹˜ì˜ ì„ë² ë”©ì„ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµ. (ìœ„ì¹˜ ê°„ì˜ ìƒëŒ€ì  ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ì§€ëŠ” ì•ŠìŒ)
- Relative position embedding
    - ì…ë ¥ í† í° ì‚¬ì´ì˜ ìƒëŒ€ì  ê±°ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ„ì¹˜ ì •ë³´ë¥¼ encoding (ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë¬´ê´€í•¨) â†’ í† í° ê°„ì˜ ê´€ê³„ë¥¼ ë” ì˜ í¬ì°©í•  ìˆ˜ ìˆìŒ
    - keyì™€ query ë²¡í„°ì— ìƒëŒ€ ìœ„ì¹˜ë¥¼ embedding í•œ í›„, ë‚´ì í•˜ì—¬ attention score ê³„ì‚°. (sequenceì˜ keyì™€ query ì‚¬ì´ì˜ ê±°ë¦¬ì— ë”°ë¼ ìƒëŒ€ì  ìœ„ì¹˜ë¡œ encoding)
- Rotary positional embedding
    - ìœ„ì¹˜ ì •ë³´ë¥¼ ì„ë² ë”© ë²¡í„°ì— ì§ì ‘ ë”í•˜ëŠ” rotation ë³€í™˜ì„ ì ìš©.
    - ê° ìœ„ì¹˜ë§ˆë‹¤ ê³ ìœ í•œ íšŒì „ ê°ë„ë¥¼ í• ë‹¹í•˜ê³ , ì´ë¥¼ ì…ë ¥ ì„ë² ë”©ì— ì ìš©í•˜ì—¬ ìœ„ì¹˜ ì •ë³´ë¥¼ ì£¼ì… (rotation í–‰ë ¬ì„ ì´ìš©í•´ì„œ ì ˆëŒ€ ìœ„ì¹˜ë¥¼ encoding í•˜ê³ , self-attention ì‹ì—ì„œ relative position dependency (ìƒëŒ€ ìœ„ì¹˜ ì˜ì¡´ì„±) ì •ë³´ë¥¼ ë”í•´ì¤Œ)
    - ìµœê·¼ LLMì—ì„œ ë§ì´ ì‚¬ìš©
- ALiBi

**(4) attention & bias**

- Full attention
    - vanilla Transformerì—ì„œ ì‚¬ìš©
- sparse attention
    - full attentionì˜ ê³„ì‚° ë³µì¡ë„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆë¨
    - ê° queryê°€ ìœ„ì¹˜ì— ë”°ë¼ tokenì˜ ë¶€ë¶„ì§‘í•©ì—ë§Œ attentionì„ ìˆ˜í–‰
    - GPT-3, Factorized Attentionì´ ì ìš©ë¨
- Multi-query / grouped-query attention
    - multi-query attention (MQA)
        - ì„œë¡œ ë‹¤ë¥¸ headê°€ keyì™€ valueì— ëŒ€í•´ ë™ì¼í•œ ì„ í˜• ë³€í™˜ í–‰ë ¬ì„ ê³µìœ  (ë‹¨ì¼ key-value headë¥¼ ì‚¬ìš©) â†’ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ì¡°ê¸ˆ ë–¨ì–´ì§€ì§€ë§Œ, inference ì†ë„ê°€ ë§¤ìš° ë¹¨ë¼ì§
        - PaLM, StarCoder ë“±ì— ì‚¬ìš©
    - [group-query attention (GQA)](https://happy-jihye.github.io/nlp/nlp-13/)
        - MQAì™€ MHAì˜ ì ˆì¶©ì•ˆ
        - ì—¬ëŸ¬ê°œì˜ headì— ëŒ€í•´ groupì„ ë‚˜ëˆˆ í›„, groupì— ëŒ€í•´ key, value transformation matrices ì ìš©
        - LLaMA2, 3 ë“±ì— ì‚¬ìš©
    
    <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/13.png?raw=1" width = "700" ></p>
    
- FlashAttention
    - ë‹¤ì–‘í•œ attention methods: mode qualityì™€ computing efficiencyê°„ì˜ trade-offê°€ ìˆìŒ
    - GPUë‚´ attention ëª¨ë“ˆì˜ ì†ë„ì™€ ë©”ëª¨ë¦¬ ì†Œëª¨ëŸ‰ì„ ìµœì í™”í•œ ë°©ë²•
        - transformer self attention ì—°ì‚°ì‹œì— ì£¼ë¡œ memory throughputì´ ë‚®ì•„ tensor coreì˜ ì—°ì‚° íš¨ìœ¨ì´ ë–¨ì–´ì§. â†’ ì—°ì‚°ì„ ì¤‘ë³µí•´ì„œ ì‹¤í–‰í•˜ë”ë¼ë„ HBMê³¼ SRAM ì‚¬ì´ì˜ IOë¥¼ ìµœì†Œí™”í•˜ì—¬ ê¸°ì¡´ë³´ë‹¤ ì•½ 2ë°° ì´ìƒ latencyë¥¼ ì¤„ì„ [(ì¶œì²˜)](https://tech.scatterlab.co.kr/serving-architecture-3/)
        - ëª¨ë¸ì˜ ì„±ëŠ¥ì— ì „í˜€ ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ. ë‹¨ì§€ HW levelì˜ bottleneckì„ SW ê¸°ë²•ìœ¼ë¡œ í•´ê²°
    - Pytorch, DeepSpeed, Megatron-LM ë“±ì— í†µí•©ë¨
- [PagedAttention](https://pangyoalto.com/pagedattetion-review/)
    - ì„œë²„ì— ë°°í¬ëœ LLMì—ì„œ GPU ë©”ëª¨ë¦¬ ì‚¬ìš© íš¨ìœ¨ì„ ë†’ì´ê¸° ìœ„í•´ ì œì•ˆë¨ (GPUë¥¼ KV cacheê°€ ì—„ì²­ ì¡ì•„ë¨¹ìŒ)
        - KV cacheëŠ” ë°˜ë³µ ê³„ì‚°ë˜ëŠ” ê°’ì„ ì €ì¥í•´ ë‘” ê²ƒ
    - ë©”ëª¨ë¦¬ë¥¼ ì¡ì•„ë¨¹ëŠ” ì£¼ëœ ì›ì¸ì€ ë‹¤ì–‘í•œ ê¸¸ì´ì˜ input length â†’ ë”°ë¼ì„œ PagedAttention: ê° sequenceë¥¼ subsequenceë¡œ ë‚˜ëˆˆ í›„, í•˜ìœ„ ì‹œí€€ìŠ¤ì˜ KV ìºì‹œë¥¼ non-contiguous ë¸”ë¡ì— í• ë‹¹í•¨

### 3.3 Long Context Modeling

1. **Scaling Position Embedding**
    - LLMì˜ context lengthë¥¼ í›ˆë ¨ë•Œë³´ë‹¤ ëŠ˜ë¦´ë ¤ë©´ position indicsë¥¼ í™•ì¥í•´ì•¼í•¨
    - í›ˆë ¨ëœ lengthë³´ë‹¤ ê¸´ textë¥¼ ì˜ ì¼ë°˜í™”í•˜ì—¬ position embeddingí•˜ëŠ” ëŠ¥ë ¥ì„ `extrapolation capacity` ë¼ í•¨
    - ğŸ‘‡ğŸ»Â RoPEë¥¼ ë” ê¸´ ë¬¸ì¥ìœ¼ë¡œ scaling í•˜ëŠ” ë°©ë²•ë¡ ë“¤
    - [a] Direct model fine-tuning
        - modelì„ ë” ê¸´ textë¡œ fine-tuning í•˜ëŠ” ë°©ì‹
        - multi-stage approach (e.g., 2K â†’ 8K â†’ 32K).
        - ì¢‹ì€ë° ëŠë¦¼
        - ex) CodeLLaMA, LongLLaMA
            - CodeLLaMA further extends the context window of LLaMA 2 (2.5T tokens with 4K context window â†’ 20B tokens with 16K context window)
            - LongLLaMA also achieves longer context window with the help of external memory and a unique training objective (1T tokens with 2K context window â†’ 10B tokens with 8K context window).
    - [b] Position interpolation
        - ì£¼ì–´ì§„ ë¬¸ì¥ì„ original context window ì•ˆìœ¼ë¡œ downscaling â†’ pretraining ë™ì•ˆì˜ rotation angleì„ ë²—ì–´ë‚˜ì§€ ì•ŠìŒ
        - [a] ë°©ë²•ë¡ ë³´ë‹¤ context windowë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í‚¤ìš°ì§€ë§Œ, ì§§ì€ text ì²˜ë¦¬ë¥¼ ì˜ ëª»í•¨ (side-effect)
    - [c] Position truncation
        - context lengthë³´ë‹¤ ê¸´ ë¶€ë¶„ì€ truncation í•˜ëŠ” ë°©ì‹
        - ReRoPE, LeakyReRoPE ê°™ì€ ëª¨ë¸ì€ maximum-training length ë³´ë‹¤ window lengthë¥¼ ì‘ê²Œ ë¯¸ë¦¬ ì„¤ì • â†’ pre-defined window ë‚´ì˜ ë¬¸ì¥ì€ ìœ ì§€, ê·¸ ì´ìƒì˜ ë¬¸ì¥ì€ maximum training lengthì— ë§ê²Œ interpolate (í˜¹ì€ ìœ ì§€)
        - local position relationshipì„ ì˜ ìœ ì§€í•˜ê³  extrapolation capacityë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì§€ë§Œ, attention ì„ ë‘ë²ˆ ê³„ì‚°í•´ì•¼í•¨ (â†’ computing resource ë§ì´ ë“¦)
    - [d] Base modification
        - RoPE ì—°ì‚°ì˜ formulaë¥¼ ì•½ê°„ ìˆ˜ì •í•˜ì—¬ (baseë¥¼ ì‘ê²Œ) ëª¨ë“  dimensionì˜ wavelengthë¥¼ train length ì•„ë˜ë¡œ rescale í•˜ëŠ” ë°©ì‹. ì´ ë°©ì‹ì´ extrapolation capacity ì¸¡ë©´ì— ë” ì¢‹ë‹¤ í•¨
    - [e] Base truncation
        - Base modificationê³¼ ìœ ì‚¬í•˜ê²Œ, basis truncationë„ training lengthë¥¼ ì´ˆê³¼í•˜ëŠ” wavelengthë¥¼ ê°€ì§„ dimensionì„ ì²˜ë¦¬
        - ë” í° position indicesì— ëŒ€í•´ out-of-distribution rotation angleì„ í”¼í•  ìˆ˜ ìˆìŒ. BUT long context taskì—ì„œ ì„±ëŠ¥ ë³„ë¡œ
2. **Adapting Context Window**
    - [[a] Parallel context window (PCW)](https://arxiv.org/pdf/2212.10947)
        - [fusion-in-decoder (FiD)](https://gbdai.tistory.com/68)ì²˜ëŸ¼ divide-and-conquer ì „ëµì„ ì‚¬ìš©í•˜ì—¬ input textë¥¼ ì²˜ë¦¬
        - ë°©ë²•ë¡ 
            - (1) input textë¥¼ ì—¬ëŸ¬ segmentsë¡œ ë‚˜ëˆˆ í›„ ê°ê°ì„ shared position embeddingsìœ¼ë¡œ encoding
                - context window ë¼ë¦¬ PEë¥¼ ê³µìœ  â†’ ê° windowì—ì„œ ìƒì„±ëœ í† í°ì´ ê°™ì€ ê±°ë¦¬ì— ìˆìŒ
            - (2) ì´í›„ generation stageì—ì„œ ê° segment ë³„ë¡œ attention ê³„ì‚°.
                - masked attention â†’ window ë‚´ì˜ ì´ì „ í† í°ë“¤ì— ëŒ€í•´ì„œë§Œ attention
        - ë¬¸ì œ: segments ì˜ ìˆœì„œë¥¼ ê³ ë ¤ ì•ˆí•¨ + window ê°„ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§ í•˜ì§€ ì•ŠìŒ â†’  multi-hop reasoningê³¼ ê°™ì´ ì‹œí€€ì…œí•œ ì¶”ë¡ ì´ í•„ìš”í•œ taskì—ì„œëŠ” ì„±ëŠ¥ ì €í•˜
    - [b] A-shaped context window
        - [lost in the middle](https://arxiv.org/pdf/2307.03172)
            - ê¸´ input sequenceë¥¼ ì²˜ë¦¬í•  ë•Œ, LLMì€ ì´ˆë°˜ë¶€ì™€ ë§ˆì§€ë§‰ ë¶€ë¶„ë§Œ ê¸°ì–µí•˜ê³  (ë” í° attention weight) ì¤‘ë°˜ë¶€ë¥¼ ê¹Œë¨¹ëŠ” ê²½í–¥ì´ ìˆë‹¤.
        - â†’ LM-infinite, StreamLLMì€ â€œA-shapedâ€ attention maskë¥¼ ì œì•ˆ
            - initial tokenê³¼ nearest tokenì—ì„œëŠ” queryì— ëŒ€í•´ attentionì´ í° tokenë“¤ë§Œ ì„ íƒì ìœ¼ë¡œ ëƒ…ë‘ê³  ë‚˜ë¨¸ì§„ ë²„ë¦¼
            - ê³ ì •ëœ memoryì— ëŒ€í•´ extra-long text generationì€ ì˜ í•˜ì§€ë§Œ.. ë²„ë ¤ì§„ í† í°ì— ì¤‘ìš”í•œ ì •ë³´ê°€ ìˆë‹¤ë©´, ì´ìš©ì´ ì–´ë µë‹¤ëŠ” ë‹¨ì ì´ ìˆìŒ
    - [c] External memory
        - documentì˜ ì‚¬ì´ì¦ˆê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ ë¬¸ì œ í•´ê²°ì— í•„ìš”í•œ ì¤‘ìš”í•œ tokensì€ ì¼ë¶€ì„ (ì‹¤ì œë¡œ top-k attention keyë“¤ì´ original full attentionë¥¼ ê·¼ì‚¬í•  ìˆ˜ ìˆìŒ)
        - â†’ ì™¸ë¶€ ë©”ëª¨ë¦¬ì— ê³¼ê±°ì˜ ì¤‘ìš” keyë“¤ì„ ì €ì¥í•´ë†¨ë‹¤ê°€ ìƒì„±í•  ë•Œ k-NN search methodë¥¼ í†µí•´ ê´€ë ¨ì„± ë†’ì€ kê°œì˜ ì£¼ìš” ê´€ë ¨ í† í°ë“¤ë§Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë¡  ì œì•ˆ (Memory Transformer, LongLLaMA..)

### 3.4 Decoding Strategy

- Greedy search
    - ê° ë‹¨ê³„ì—ì„œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í† í°ì„ ì„ íƒí•˜ëŠ” ë°©ì‹ (ì´ì „ í† í°ë“¤ì— ëŒ€í•´ most likely token ì˜ˆì¸¡). ì¢€ ë” ì¼ê´€ë˜ê³  ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ê²½í–¥ì´ ìˆìŒ
    - Beam search
        - decoding í•˜ëŠ”ë™ì•ˆ ê° ë‹¨ê³„ì—ì„œ n (beam size) highest probabilitiesì˜ ë¬¸ì¥ì„ ìœ ì§€í•œ í›„, ìµœì¢…ì ìœ¼ë¡œ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ë¬¸ì¥ì„ ì„ íƒí•˜ëŠ” ë°©ì‹
        - Length penalty (length normalization)
            - beam searchì€ ì§§ì€ ë¬¸ì¥ì„ ì„ í˜¸í•˜ê¸° ë•Œë¬¸ì— length penalty ë¥¼ í†µí•´ ì´ë¥¼ ê·¹ë³µ
- Sampling-based methods
    - í™•ë¥  ë¶„í¬ì— ë”°ë¼ ë¬´ì‘ìœ„ë¡œ í† í°ì„ ì„ íƒí•˜ëŠ” ë°©ì‹ (diversityí•˜ê²Œ ìƒì„±ì„ ì˜ í•˜ë„ë¡ probability distributionë¥¼ baseë¡œ í•˜ì—¬ next tokenì„ random sampling). ë” ë‹¤ì–‘í•˜ê³  ì°½ì˜ì ì´ì§€ë§Œ ë¬¸ë²• ì˜¤ë¥˜ë‚˜ ë¹„ë¬¸ì´ ë§ì´ ë°œìƒ
    - Temperature sampling
        - ì–¸ì–´ ëª¨ë¸ì˜ í™•ë¥  ë¶„í¬ë¥¼ ì¡°ì •í•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°. Temperatureê°€ ë†’ì„ìˆ˜ë¡ í™•ë¥  ë¶„í¬ê°€ í‰í‰í•´ì ¸ì„œ ë‹¤ì–‘í•œ í† í°ì´ ì„ íƒë  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§€ê³ , ë‚®ì„ìˆ˜ë¡ í™•ë¥ ì´ ë†’ì€ í† í°ì— ì§‘ì¤‘ë˜ì–´ ê²°ì •ë¡ ì ì¸ ì¶œë ¥ì„ ìƒì„±.
            - Temperatureê°€ 1ì¼ ë•ŒëŠ” ì›ë˜ì˜ í™•ë¥  ë¶„í¬ë¥¼ ìœ ì§€
            - 1ë³´ë‹¤ ì‘ìœ¼ë©´ ë†’ì€ í™•ë¥ ì˜ í† í°ì´ ë” ìì£¼ ì„ íƒë¨
            - 1ë³´ë‹¤ í¬ë©´ í™•ë¥  ë¶„í¬ê°€ í‰í‰í•´ì ¸ ë‹¤ì–‘í•œ í† í°ì´ ì„ íƒë  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§
    - Top-k sampling
        - ê° ë‹¨ê³„ì—ì„œ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ kê°œì˜ í† í°ì„ ì„ íƒí•œ í›„, ê·¸ ì¤‘ì—ì„œ í™•ë¥ ì— ë¹„ë¡€í•˜ì—¬ ë¬´ì‘ìœ„ë¡œ í† í°ì„ ì„ íƒí•˜ëŠ” ë°©ì‹
            - k ê°’ì´ í´ìˆ˜ë¡ ë” ë§ì€ ì˜µì…˜ ì¤‘ì—ì„œ ì„ íƒí•˜ê²Œ ë˜ë¯€ë¡œ ë‹¤ì–‘ì„±ì´ ì¦ê°€
            - ì‘ì„ìˆ˜ë¡ ë†’ì€ í™•ë¥ ì˜ í† í°ì— ì§‘ì¤‘í•˜ê²Œ ë¨
        - í™•ë¥ ì´ ë‚®ì€ í† í°ì„ ì œê±°í•¨ìœ¼ë¡œì¨ ì™„ì „í•œ random samplingë³´ë‹¤ëŠ” ê´€ë ¨ì„± ë†’ì€ í† í°ì„ ì„ íƒí•  ê°€ëŠ¥ì„±ì„ ë†’ì´ë©´ì„œë„, ê·¸ ì¤‘ì—ì„œ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ì—¬ ì–´ëŠ ì •ë„ì˜ ë‹¤ì–‘ì„±ì„ í™•ë³´í•  ìˆ˜ ìˆìŒ
    - Top-p sampling
        - ëˆ„ì  í™•ë¥ ì´ íŠ¹ì • ì„ê³„ê°’ pë¥¼ ë„˜ëŠ” í† í°ë“¤ ì¤‘ì—ì„œ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ëŠ” ë°©ì‹
            - p ê°’ì´ í´ìˆ˜ë¡ ë” ë§ì€ í† í°ì„ ê³ ë ¤í•˜ê²Œ ë˜ë¯€ë¡œ ë‹¤ì–‘ì„±ì´ ì¦ê°€
            - ì‘ì„ìˆ˜ë¡ ë†’ì€ í™•ë¥ ì˜ í† í°ì— ì§‘ì¤‘
        - ì¼ë°˜ì ìœ¼ë¡œ p ê°’ì€ 0.9 ë‚´ì™¸ë¡œ ì„¤ì •ë¨ (ìƒìœ„ 90% í™•ë¥  ì§ˆëŸ‰ì— í•´ë‹¹í•˜ëŠ” í† í°ë“¤ ì¤‘ì—ì„œ ì„ íƒí•œë‹¤)
- Reducing data transfer â†’ GPU ë©”ëª¨ë¦¬ íš¨ìœ¨í™”
    - PagedAttention
        - KV cacheë¥¼ non-continous memory blockì— ì €ì¥í•´ì„œ ë©”ëª¨ë¦¬ íš¨ìœ¨í™”
    - Flash-Decoding
        - attention íš¨ìœ¨í™”
    - MQA, GQA
        - KV parameterë¥¼ ê³µìœ í•˜ëŠ” ë°©ì‹

---

## [4] Training

### 4.1 Optimization Setting

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/14.png?raw=1" width = "700" ></p>

**Batch Training**

- GPT-3, PaLM ê°™ì€ ëª¨ë¸: í›ˆë ¨ ì¤‘ì— dynamicí•˜ê²Œ batch sizeë¥¼ ì˜¬ë¦¼
    - GPT-3 (32K â†’ 3.2M tokens)
- dynamic batch schedulingì´ LLM ì„ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ”ë° ë„ì›€ì´ ëë‹¤ í•¨
    - ë°°ì¹˜ í¬ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ë©´ ëª¨ë¸ì˜ í•™ìŠµ ë‹¨ê³„ì™€ ì‚¬ìš© ê°€ëŠ¥í•œ ê³„ì‚° ë¦¬ì†ŒìŠ¤ì— ë§ì¶° ìƒ˜í”Œ ìˆ˜ë¥¼ ìµœì í™”í•  ìˆ˜ ìˆìŒ â†’ í•™ìŠµ íš¨ìœ¨ì„ ë†’ì´ê³  ê³„ì‚° ë¹„ìš©ì„ ì ˆê°í•˜ë©° ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒ [(ì°¸ê³ )](https://medium.com/the-modern-scientist/the-symphony-of-efficiency-dynamically-updating-the-batch-size-in-machine-learning-4c8ce45e98fa)

**Learning Rate**

- warm-up & decay strategies

**Optimizer**

- LLM: Adam & AdamW ë§ì´ ì‚¬ìš© (GPT-3)
- Adafactor - PALM, T5

### 4.2 Scalable Training Techniques

**GPU Memory**

FP16 & x parameters

- parameters: 2x (2 bytes for fp16)
- gradients: 2x
- optimizer state: 12x
    - parameter copy: 4x (4 bytes for fp32)
    - momentums: 4x
    - variance: 4x

### 4.2.1 3D Parallerism

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/15.png?raw=1" width = "800" ></p>

ë‹¤ìŒ ì„¸ë¯¸ë‚˜ ì°¸ê³ 
https://www.youtube.com/watch?v=JA1l96tjrs4&t=1256s

**4.2.1.1** **Data Parallelism**

- ì—¬ëŸ¬ GPUë“¤ì— model paramameters, model gradients, optimizer paramtersë¥¼ ë³µì œ â†’ ì „ì²´ training corpusë¥¼ ë‚˜ëˆ ì„œ (data parallelism) ì—¬ëŸ¬ GPUë¡œ í•™ìŠµ
- GPUëŠ” í• ë‹¹ëœ ë°ì´í„°ë§Œ ì²˜ë¦¬í•˜ê³ , forwardì™€ backward propagationì„ ìˆ˜í–‰í•˜ì—¬ gradientë¥¼ ì–»ìŒ
- ì„œë¡œ ë‹¤ë¥¸ GPUì—ì„œ ê³„ì‚°ëœ gradientëŠ” aggregationë˜ì–´ ì „ì²´ ë°°ì¹˜ì˜ gradientë¥¼ ì–»ê³ , ì „ì²´ GPUì˜ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸

<details><summary> Data Parallelism ì¶”ê°€ ì„¤ëª… </summary><p>
    
<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/16.png?raw=1" width = "700" ></p>

[ì¶œì²˜: soochel ë‹˜ ë¸”ë¡œê·¸](https://medium.com/tesser-team/%EB%8B%A4%EC%A4%91-gpu%EB%A5%BC-%ED%9A%A8%EC%9C%A8%EC%A0%81%EC%9C%BC%EB%A1%9C-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95-dp%EB%B6%80%ED%84%B0-fsdp%EA%B9%8C%EC%A7%80-3057d31150b6), [ì°¸ê³ : NCCL](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#reducescatter), [youtube](https://www.youtube.com/watch?v=By_O0k102PY)

1. ì…ë ¥ ë°ì´í„°ì™€ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê° GPUì— ì „ë‹¬í•´ì¤˜ì•¼ ëœë‹¤.
2. backward ê³¼ì •ì—ì„œëŠ” ê° GPUì— ì „ë‹¬ëœ ë°ì´í„°ì™€ ê´€ë ¨ëœ gradientë¥¼ ë‚˜ëˆ„ì–´ ì£¼ì–´ì•¼ í•œë‹¤.
3. ë§ˆì§€ë§‰ìœ¼ë¡œ ëª¨ë“  gradientë¥¼ ëª¨ìœ¼ê³  ì—…ë°ì´íŠ¸ë¥¼ í•´ì£¼ì–´ì•¼ í•œë‹¤.

Data Parallelismì€ ì²«ë²ˆì§¸ GPUê°€ í•˜ëŠ” ì¼ì´ ë„ˆë¬´ ë§ìŒ 

â†’ pytorch, multi-processingì„ ì´ìš©í•œ Distributed Data Parallelism 

- multiprocessingì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— multi-threadì˜ ë‹¨ì  í•´ê²° ê°€ëŠ¥
- ì—¬ëŸ¬ê°œì˜ processë¥¼ ì‚¬ìš© â†’ GPU-1 ì´ ëª¨ë“ ì¼ì„ ë‹¤ í•˜ì§€ ì•Šê³ , GPUë“¤ì´ ì¼ì„ ë‚˜ëˆ ì„œ í•¨. ë‹¤ë§Œ, GPU ê°„ í†µì‹ ì´ í•„ìš”í•´ì§

The key insight to unlock full parameter sharding is that we can decompose theÂ [all-reduce](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allreduce)Â operations in DDP into separateÂ [reduce-scatter](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#reducescatter)Â andÂ [all-gather](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allgather)Â operations [(ì¶œì²˜)](https://engineering.fb.com/2021/07/15/open-source/fsdp/)

</p></details>


<details><summary> DDP (`Distributed Data Parallelism`) </summary><p>

[DPë¶€í„° FSDPê¹Œì§€ ê¸€ ë°œì·Œ](https://medium.com/tesser-team/%EB%8B%A4%EC%A4%91-gpu%EB%A5%BC-%ED%9A%A8%EC%9C%A8%EC%A0%81%EC%9C%BC%EB%A1%9C-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95-dp%EB%B6%80%ED%84%B0-fsdp%EA%B9%8C%EC%A7%80-3057d31150b6)

DDPì—ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ samplerë¥¼ í†µí•´ ê° GPUì— ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ê°€ ì „ì†¡ë˜ë©°, ê° ë°ì´í„°ë¥¼ ì´ìš©í•´ì„œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ gradients A, B, C, Dë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´í›„ All Reduce ì—°ì‚°ì„ í†µí•´ gradients A, B, C, Dì— ëŒ€í•œ í‰ê· ì„ êµ¬í•œ ë’¤, ëª¨ë“  GPUì— ì „ë‹¬ë©ë‹ˆë‹¤. ì´í›„ optimizerì˜ stepì„ í†µí•´ ê° GPUì—ì„œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ì—…ë°ì´íŠ¸ ë˜ê³ , ë˜‘ê°™ì€ gradients ê°’ì„ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì—, ë˜‘ê°™ì€ ëª¨ë¸ ì •ë³´ê°€ ë³´ì¥ë©ë‹ˆë‹¤. 

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/17.png?raw=1" width = "500" ></p>

</p></details>

<details><summary> FSDP (`Fully Sharded Data Palleral`) </summary><p>

ë°˜ë©´ FSDPì—ì„œëŠ” ëª¨ë¸ì˜ ëª¨ë“  ì •ë³´ê°€ í•˜ë‚˜ì˜ GPUì— ìˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì—¬ëŸ¬ GPUì— ë¶„ì‚°ë˜ì–´(sharded) ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ forward ê³¼ì •ì—ì„œ ëª¨ë¸ì˜ ê° layerë¥¼ í†µê³¼í•  ë•Œë§ˆë‹¤ ë‹¤ë¥¸ GPUì— ì €ì¥ë˜ì–´ ìˆëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì ¸ì™€ ì‚¬ìš©í•˜ê³  ì œê±°í•©ë‹ˆë‹¤ (All Gather ì—°ì‚°). ì´í›„ backward ê³¼ì •ì—ì„œ ë‹¤ì‹œ gradientsë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ë‹¤ë¥¸ GPUì— ì €ì¥ë˜ì–´ ìˆëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•˜ê³  (All Gather ì—°ì‚°), ê° GPUì—ì„œ ê³„ì‚°ëœ gradientsë¥¼ ë‹¤ì‹œ ì›ë˜ ì†í•´ ìˆë˜ GPUì— ì „ë‹¬í•˜ê¸° ìœ„í•´ì„œ Reduce Scatter ì—°ì‚°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ ê° GPUì—ëŠ” ê° GPUê°€ ê°–ê³  ìˆë˜ ëª¨ë¸ì— ëŒ€í•œ gradientsë§Œ ë‚¨ê¸° ë•Œë¬¸ì—, ì´í›„ optimizerì˜ step ì—°ì‚°ì„ í†µí•´ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/18.png?raw=1" width = "700" ></p>|<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/19.png?raw=1" width = "700" ></p>
--|--

</p></details>

<details><summary> ZeRO </summary><p>

- DeepSpeedì—ì„œ ì œì•ˆë¨. data parallelismì—ì„œì˜ memory dedundancy ì´ìŠˆì— ì§‘ì¤‘
    - data parallelismì€ ê° GPUì— ëª¨ë¸ì„ copy (model paramameters, model gradients, optimizer paramters) â†’ memory redundancy ë¬¸ì œ ìƒê¹€
- memory redundancy ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê° GPUì—ì„œ dataì˜ ì¼ë¶€ë§Œ ìœ ì§€í•˜ê³ , ë‚˜ë¨¸ì§€ ë°ì´í„°ëŠ” í•„ìš”í•  ë•Œ ë‹¤ë¥¸ GPUì—ì„œ ê²€ìƒ‰ (retrieve)í•˜ëŠ” ë°©ë²•ë¡  (ì´ 3ê°€ì§€)
    - (1) optimizer state partitioning (2) gradient partitioning  â†’ communication overhead X
    - (3) parameter partitioning â†’ communication overhead ëŠ” ìˆì§€ë§Œ (50%) ë©”ëª¨ë¦¬ íš¨ìœ¨í™” ê°€ëŠ¥
- PytorchëŠ” ZeROì™€ ë¹„ìŠ·í•œ FSDP ì œê³µ

</p></details>

>
> 

**4.2.1.2** [**Tensor Parallelism**](https://pytorch.org/tutorials/intermediate/TP_tutorial.html)

- multi GPU ì—°ì‚°ì„ ìœ„í•´ LLMì˜ tensor(parameter í–‰ë ¬)ë¥¼ decompose
- matrix multiplicationì„ ì—¬ëŸ¬ GPUì—ì„œ ë³‘ë ¬ ì—°ì‚°
- Megatron-LM ë“±ì—ì„œ ì‚¬ìš© ê°€ëŠ¥

**4.2.1.3 [PipeLine Parallelism](https://siboehm.com/articles/22/pipeline-parallel-training)**

- LLMì˜ ì„œë¡œ ë‹¤ë¥¸ layerë¥¼ multi GPU ì— ë¶„ë°°
- ex) Transformer
    - ê°™ì€ GPUì— ì—°ì†ì ì¸ layerë¥¼ ë¡œë“œ â†’ GPUê°„ì˜ ì „ì†¡ ë¹„ìš©ì„ ì¤„ì„ (gradient, hidden state..)
- Pipeline Parallelismì€ ë‹¤ë¥¸ GPUì—ì„œ ì§„í–‰ëœ ì—°ì‚° ê²°ê³¼ë¥¼ ê¸°ë‹¤ë ¤ì•¼í•¨ â†’ bubble overhead ì¡´ì¬
    - ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ GPipe, PipeDream ì€ padding multiple batches of dataì™€ asynchronous gradient update ê¸°ìˆ ì„ ì œì•ˆí•˜ì—¬ pipeline íš¨ìœ¨ì„±ì„ ê°œì„ 

### 4.2.2 Mixed Precision Training

- ê³¼ê±° PLMs (ex. BERT)ë“¤ì€ ì£¼ë¡œ FP32 ì‚¬ìš©í–ˆì§€ë§Œ, ìµœê·¼ì—” ë©”ëª¨ë¦¬ íš¨ìœ¨í™” + communication overhead ê°ì†Œë¥¼ ìœ„í•´ FP16ë¥¼ ì‚¬ìš©
    - Overflow ë¬¸ì œ ìƒê¸¸ ìˆ˜ ìˆìŒ (ë’¤ìª½ layerë¡œ ê°ˆìˆ˜ë¡ GEMM `- matrix multiplication` ì˜ ì—°ì‚° ê²°ê³¼ê°’ì´ ì»¤ì§ˆ ìˆ˜ ìˆëŠ”ë°, ì´ë•Œ ì´ ê°’ì´ FP16ì˜ ìµœëŒ“ê°’ì¸ 66504ë³´ë‹¤ í´ ê²½ìš° overflow ë°œìƒ
- A100 ê°™ì€ NVIDIA GPUsëŠ” FP16 ì—°ì‚°ì„ 2ë²ˆ í•´ì„œ FP32 ì²˜ëŸ¼ ì‚¬ìš©
- BUT, FP16ì€ ê³„ì‚° ì •í™•ë„ë¥¼ ë–¨ì–´íŠ¸ë¦´ ìˆ˜ ìˆìŒ â†’ ëª¨ë¸ ì„±ëŠ¥ í•˜ë½
- ì´ë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ BF16 (Brain Floating Point) ê°€ ë“±ì¥
    - BF16ëŠ” FP16ë³´ë‹¤ ë” ë§ì€ exponent bitì™€ ë” ì ì€ significant bitë¥¼ í• ë‹¹
    - Pre-trainingì˜ ê²½ìš°, BF16ì´ ì¼ë°˜ì ìœ¼ë¡œ representation accuracy ë©´ì—ì„œ FP16ë³´ë‹¤ ìš°ìˆ˜

## [5] [LLM Serving] Inference

ì°¸ê³  ê¸€:

[1] https://tech.scatterlab.co.kr/serving-architecture-3/
[2] https://engineering.clova.ai/posts/2022/03/hyperclova-part-3

**multi batch**

- ì¶”ë¡  ê³¼ì •ì€ í•™ìŠµ ë•Œì™€ëŠ” ë‹¤ë¥´ê²Œ, throughputì„ ë†’ì´ëŠ” ê²ƒë³´ë‹¤ latencyë¥¼ ì¤„ì´ëŠ” ê²Œ ì¤‘ìš”í•¨ â†’ batch sizeë¥¼ ìµœëŒ€í™”í•˜ê¸° ë³´ë‹¤ëŠ” í•œë²ˆì— single batchë§Œ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ latencyë¥¼ ì¤„ì„

**dynamic batch**

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/20.png?raw=1" width = "500" ></p>

https://clear.ml/blog/increase-huggingface-triton-throughput-by-193/

- ì‹¤ì œ REST APIë¥¼ êµ¬ì„±í•˜ê³  ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°›ìœ¼ë©´, ì‹œê°„ ì°¨ì´ë¥¼ ë‘ê³  ê°ê¸° ë‹¤ë¥¸ ìš”ì²­ë“¤ì´ ë“¤ì–´ì˜¤ê²Œ ë¨. ì´ë•Œ ê° ìš”ì²­ë§ˆë‹¤ single inferenceë¥¼ í•˜ë©´ latencyê°€ ê¸¸ì–´ì§
    - â†’ dynamic batch ì‚¬ìš©: ì—¬ëŸ¬ê°œì˜ ì…ë ¥ì„ í•œê°œì˜ batchë¡œ ë¬¶ì–´ ìµœëŒ€í•œ tensor coreì˜ utilizationì„ ë†’ì„

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/21.png?raw=1" width = "500" ></p>

- ë‹¤ë§Œ, ì‹¤ì œë¡œ dynamic batch (multi batch)ë¥¼ í•´ë³´ë‹ˆ ëŒ€ì²´ì ìœ¼ë¡œëŠ” latencyì˜ í° ë³€ë™ ì—†ì´ ì²˜ë¦¬ëŸ‰ì´ ëŠ˜ì–´ë‚¬ì§€ë§Œ íŠ¹ì •ìƒí™©ì—ì„œëŠ” ì§€ì—°ì‹œê°„ì´ ë°°ë¡œ ì¦ê°€í•˜ëŠ” í˜„ìƒì´ ë°œìƒ.
- WHEN? í•˜ë‚˜ì˜ batch ë¡œ ë¬¶ì€ ìš”ì²­ë“¤ì˜ ì…ë ¥ ë¬¸ì¥ì˜ ê¸¸ì´ê°€ í¬ê²Œ ë‹¤ë¥¸ ê²½ìš°ì—ëŠ” ì§§ì€ ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ generationì´ ë˜ê¸° ë•Œë¬¸ì— ì²˜ë¦¬ ì‹œê°„ì´ ì§€ì²´ë¨
- í•´ê²°ë°©ë²•
    - [1] ë²„í‚·íŒ… ì „ëµ
        - ì…ë ¥ ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ë¹„ìŠ·í•œ ìš”ì²­ë¼ë¦¬ ë¶„ë¥˜í•˜ì—¬ bucketì„ ë§Œë“  í›„ multi batch ì—°ì‚°
        - ë‹¤ë§Œ bucketìœ¼ë¡œ ì˜ ë¬¶ëŠ”ë‹¤ í•´ë„ ê° ì…ë ¥ ë¬¸ì¥ì˜ ê¸¸ì´ì°¨ê°€ ì—¬ì „íˆ ì¡´ì¬ (â†’ ì§€ì—°ì‹œê°„ ì¦ê°€). ë˜ ë²„í‚·ìœ¼ë¡œ ë¬¶ìœ¼ë ¤ë‹¤ê°€ ì„œë¹„ìŠ¤ì—ì„œ ë©€í‹°ë°°ì¹˜ì—°ì‚°ì„ ëª»í•˜ê¸°ë„ í•¨
    - [2] GPTì˜ attentnion maskì˜ íŠ¹ì§•ì„ ì´ìš©
        - ì´ë ‡ê²Œ attention maskë¥¼ ìˆ˜ì •í•˜ëŠ” ë°©ì‹ì„ ì ìš© â†’ ì…ë ¥ ê¸¸ì´ê°€ ì„œë¡œ ë‹¤ë¥¸ ìš”ì²­ì´ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ ë¬¶ì¼ ë•Œ ì§€ì—°ì‹œê°„ì´ í¬ê²Œ ì¦ê°€í•˜ëŠ” í˜„ìƒì„ ë§‰ì„ ìˆ˜ ìˆì—ˆë‹¤ê³  í•¨
        
        <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/22-llm/22.png?raw=1" width = "500" ></p>
        

**Iteration Batching**

- FriendliAIì˜Â [**Iteration-level Scheduling**](https://www.usenix.org/conference/osdi22/presentation/yu)
- batchë¡œ ë¬¶ì¸ ìš”ì²­ë“¤ì—ì„œ output tokenì´ í•˜ë‚˜ì”© í˜•ì„±ë  ë•Œë§ˆë‹¤ ìƒˆë¡œìš´ ìš”ì²­ì„ batch í¬í•¨ ì‹œí‚´