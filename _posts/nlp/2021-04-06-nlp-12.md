---
title: "[Paper Review] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 논문 분석"

excerpt: " "

categories: 
  - nlp
tags: 
  - deeplearning
  - ai
  - nlp
  - transformer
  - bert

search: true

# 목차
toc: true  
toc_sticky: true 

---


> 이번 포스팅에서는 Transformer의 architeture를 효율적으로 만든 Reformer(2020)에 대해 살펴본다.

- Paper : [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
          (2019 / Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova)


## 1. Introduction