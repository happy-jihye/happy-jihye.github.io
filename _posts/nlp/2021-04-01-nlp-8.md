---
title: "[Paper Review] Transformer : Attention is All You Need ë…¼ë¬¸ ë¶„ì„ ë° ì½”ë“œ ì‹¤ìŠµ"
excerpt: " "

categories: 
  - nlp
tags: 
  - deeplearning
  - ai
  - nlp
  - pytorch
  - transformer
layout: jupyter
search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 

---

<p align="right">
  <a href="https://github.com/happy-jihye/Natural-Language-Processing/blob/main/code/6_Attention_is_All_You_Need.ipynb" role="button" target="_blank">
    <img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
  </a>
  <a href="https://mybinder.org/v2/gh/happy-jihye/Natural-Language-Processing/main?filepath=code/6_Attention_is_All_You_Need.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/assets/badges/binder.svg" alt="Open In Binder"/>
  </a>
  <a href="https://colab.research.google.com/github/happy-jihye/Natural-Language-Processing/blob/main/code/6_Attention_is_All_You_Need.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
  </a>
</p>


> 2021/04/01 Happy-jihye ğŸŒº
> 
> **Reference** : [pytorch-seq2seq/6 - Attention is All You Need](https://github.com/bentrevett/pytorch-seq2seq)
> 
> **paper** : [Attention is All You Need(2017)](https://arxiv.org/abs/1706.03762)

--- 

# 0. Introduction

- ì˜¤ëŠ˜ ì†Œê°œí•´ë“œë¦´ ëª¨ë¸ì€ ë“œë””ì–´ **Transformer**ì…ë‹ˆë‹¤. TransformerëŠ” Googleì—ì„œ 2017ë…„ì— ë°œí‘œí•œ ëª¨ë¸ë¡œ, ìµœê·¼ê¹Œì§€ NLPì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” architectureì´ë©° ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì—°êµ¬ë“¤ì´ ì§€ê¸ˆê¹Œì§€ë„ *state-of-the-art*í•œ ê²°ê³¼ë¥¼ ë‚´ê³  ìˆìŠµë‹ˆë‹¤. 

  í˜„ì¬ NLPì—ì„œ ê°€ì¥ ì¸ê¸°ìˆëŠ” ëª¨ë¸ì€ Googleì˜ [BERT](https://arxiv.org/abs/1810.04805)(Bidirectional Encoder Representations from Transformers)ë‚˜ OpenAIì˜ [GPT-3](https://arxiv.org/abs/2005.14165)ë¡œ, ì´ ì—­ì‹œ transformerì˜ architecureë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ë“¤ì— ëŒ€í•´ì„œëŠ” í–¥í›„ í¬ìŠ¤íŒ… í•˜ê² ìŠµë‹ˆë‹¤ :)

- ê¸°ì¡´ì˜ ë²ˆì—­ ëª¨ë¸ë“¤ì€ RNNì„ í™œìš©í•œ Encoder-Decoderì˜ êµ¬ì¡°ì˜€ë‹¤ë©´, Transformerë¶€í„°ëŠ” ë…¼ë¬¸ì˜ ì œëª©([Attention is All You Need(2017)](https://arxiv.org/abs/1706.03762)) ì²˜ëŸ¼ ì˜¤ì§ Attentionë§Œì„ ì‚¬ìš©í•˜ì—¬ ìì—°ì–´ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. 

---
  

RNN ëª¨ë¸ë“¤ì€ language modelingì´ë‚˜ machine translationê³¼ ê°™ì€ sequence modelingì—ì„œ ìì£¼ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. 

ë‹¤ë§Œ, Recurrent modelì€ ìˆœì°¨ì ìœ¼ë¡œ ì—°ì‚°ì„ í•˜ê¸° ë•Œë¬¸ì— parallelizationì´ ì–´ë ¤ì› ê³ , ë¬¸ì¥ì´ ê¸¸ì–´ì§ˆ ìˆ˜ë¡ ì„±ëŠ¥ ì—­ì‹œ ë–¨ì–´ì§€ë©°, memoryì˜ ì œì•½ìœ¼ë¡œ batchì—ë„ ì œí•œì´ ìƒê¸¸ ìˆ˜ ìˆë‹¤ëŠ” ë¬¸ì œì ì´ ìˆì—ˆìŠµë‹ˆë‹¤. (`factorization tricks`ì´ë‚˜ `conditional computation`ë“±ì˜ ë°©ë²•ì„ í†µí•´ ì–´ëŠì •ë„ ì—°ì‚°ì— ìˆì–´ì„œ efficiencyë¥¼ ë†’ì´ê¸´ í–ˆì§€ë§Œ, Sequential computationì˜ ê·¼ë³¸ì ì¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ì§€ëŠ” ëª»í–ˆìŠµë‹ˆë‹¤.)


ë”°ë¼ì„œ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‚˜ì˜¨ ëª¨ë¸ì´ ë°”ë¡œ Transformerì…ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” CNNì´ë‚˜ RNN ëª¨ë¸ ì—†ì´ ì˜¤ì§ Attention mechanismë§Œì„ ì‚¬ìš©í•˜ì—¬ machine translation taskë¥¼ ì§„í–‰í•˜ì˜€ê³ , ì´ëŠ” ë§¤ìš° ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ê²Œ ë©ë‹ˆë‹¤ !!




# 1. Paper Review

## Model Architecture

<p align="center"><img src="https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/transformer1.png?raw=1" width = "400" ></p>

TransformerëŠ” Encoder-Decoder ëª¨ë¸ì„ ë”°ë¥´ë©°, ì£¼ìš” architectureë¡œëŠ” **stacked self-attention, point-wise fully connected layers**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

### Encoder

- EncoderëŠ” ëª¨ë‘ Nê°œì˜ ë™ì¼í•œ layerê°€ ë°˜ë³µë˜ëŠ” í˜•íƒœì…ë‹ˆë‹¤.(ë…¼ë¬¸ì—ì„œì˜ N = 6)
- ìœ„ì˜ ê·¸ë¦¼ì—ì„œë„ ë³¼ ìˆ˜ ìˆë‹¤ì‹œí”¼ Encoder layerëŠ” **Multi-head Self-Attention mechanism**ê³¼ **Positional-wise fully connected Feed-Forward network**ì˜ 2ê°œì˜ layerë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.  
- ë˜í•œ, ë‘ê°œì˜ sub-layerì— **residual connection** ë°©ì‹ì„ ì ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ë†’ì˜€ìœ¼ë©°, ì´ ê³„ì‚°ì„ ì‰½ê²Œí•˜ê¸° ìœ„í•´ì„œ embedding layerì™€ 2ê°œì˜ sub-layerì˜ outputì˜ dimensionì„ 512ë¡œ ë§ì¶°ì£¼ì—ˆìŠµë‹ˆë‹¤.


### Decoder

- Decoderë„ Encoderì™€ ë§ˆì°¬ê°€ì§€ë¡œ N = 6ê°œì˜ layerë¥¼ ë°˜ë³µí•˜ì—¬ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
- DecoderëŠ” 3ê°œì˜ sub-layerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
  - ì²«ë²ˆì§¸ sub-layerë¡œëŠ” **masked self-attention layer**ì…ë‹ˆë‹¤. encoderì™€ ìœ ì‚¬í•˜ì§€ë§Œ, í˜„ì¬ì˜ positionì— ì´í›„ postionì˜ ì¶œë ¥ê°’ì´ ë‚˜ì˜¤ëŠ” ê±¸ ë°©ì§€í•˜ê¸° ìœ„í•´ maskingê¸°ë²•ì„ ì ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

    ì¦‰, self-attentionì‹œ í˜„ì¬ ìœ„ì¹˜ë³´ë‹¤ ë’¤ì— ìˆëŠ” ë‹¨ì–´ëŠ” attend ë¥¼ í•˜ì§€ ëª»í•˜ê²Œ ë©ë‹ˆë‹¤.
    
  - ë‚˜ë¨¸ì§€ layerëŠ” encoderì™€ ë¹„ìŠ·í•˜ì§€ë§Œ, ë‘ë²ˆì§¸ sub-layerì¸  **Multi-head Attention**ëŠ” encoderì˜ outputì„ inputìœ¼ë¡œ ë°›ëŠ”ë‹¤ëŠ” ì ì´ ë‹¤ë¦…ë‹ˆë‹¤.

## Attention

attention layerë¡œëŠ” 2ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤. í•˜ë‚˜ëŠ” **Scaled Dot-Product Attention**ì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ì—¬ëŸ¬ ê°œì˜ Scaled Dot-Product Attentionë¥¼ ì‚¬ìš©í•œ **Multi-Head Attention**ì…ë‹ˆë‹¤.

### (1) Scaled Dot-Product Attention

<p align="center"><img src="https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/transformer2.png?raw=1" width = "300" ></p>

- Scaled Dot-Product Attentionì˜ inputì€ 3ê°€ì§€ë¡œ, queriesì™€ keysëŠ” $d_k$ dimensionì„ ê°€ì§€ê³  valuesëŠ” $d_v$ dimensionì„ ê°€ì§‘ë‹ˆë‹¤. 
- ìš°ë¦¬ëŠ” Query vecì™€ Key vecë¥¼ ë‚´ì í•œ í›„ $\sqrt{d_k}$ë¡œ ë‚˜ëˆ„ì–´ì£¼ì—ˆìŠµë‹ˆë‹¤.
  - scalingì„ í•˜ì§€ ì•Šìœ¼ë©´, dot-productì˜ ê°’ì´ ë„ˆë¬´ë‚˜ë„ ì»¤ì ¸ softmaxë¥¼ ì·¨í–ˆì„ ë•Œì˜ gradientê°€ ë§¤ìš° ì‘ì•„ì§‘ë‹ˆë‹¤. ê·¸ë ‡ê²Œ ë˜ë©´, back-propagation ê³¼ì •ì—ì„œ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— scalingì˜ ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.

> attention functionìœ¼ë¡œëŠ” dot-product í•¨ìˆ˜ ì™¸ì—ë„ additive attention í•¨ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. 
> ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ë„ ì¢‹ì§€ë§Œ dot-product í•¨ìˆ˜ê°€ í–‰ë ¬ê³±ì— ì¡°ê¸ˆë” ìµœì í™”ë˜ì–´ìˆê¸° ë•Œë¬¸ì— ë” ë¹ ë¥´ê³  space-efficientí•©ë‹ˆë‹¤.


$$ \text{Attention}(Q, K, V) =\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V $$

### (2) Multi-Head Attention

<p align="center"><img src="https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/transformer3.png?raw=1" width = "300" ></p>

ì´ ë°©ì‹ì€ ë³‘ë ¬í™”ë¥¼ ìœ„í•´ ì „ì²´ dimensionì— ëŒ€í•´ í•œë²ˆì— attentionì„ í•˜ì§€ ì•Šê³ , $h$ë²ˆì— ê±¸ì³ attentionì„ í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

Q, K, V vectorë¥¼ headê°œ ë§Œí¼ ë‚˜ëˆˆ í›„ attentionì„ í•´ì£¼ê³ , ì´ë¥¼ ë‹¤ì‹œ ì´ì–´ë¶™ì´ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.

$$ \begin{matrix}
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1,...,\text{head}_h)W^O\\
\text{where}~\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K,VW_i^V)
\end{matrix} $$

- parameter

$$ W_i^Q,~W_i^K\in\mathbb{R}^{d_{\text{model}}\times d_k}, W_i^V \in\mathbb{R}^{d_{\text{model}}\times d_k}, W^O \in \mathbb{R}^{hd_v\times d_{\text{model}}} $$ 

- ì´ ë…¼ë¬¸ì—ì„œëŠ” 8ê°œì˜ parallelí•œ attention layer(head)ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

$$ d_k=d_v=d_{\text{model}}/h = 64 $$

### Applications of Attention in Transformer

Transformerì—ì„œëŠ” ì´ 3ê°€ì§€ì˜ multi-head attentionì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

1. **Encoder self-attentinon**
  - self-attentionì—ì„œì˜ Q, K, V vectorëŠ” ëª¨ë‘ ê°™ì€ layerì¸, ì´ì „ encoderì˜ outputì—ì„œ ì˜µë‹ˆë‹¤. 
  - Encoderì—ì„œëŠ” ì´ì „ encoder layerì˜ ëª¨ë“  ìœ„ì¹˜ì— attend í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

2. **Masked self-attention(Decoder)**
  - decoderì˜ self-attentionëŠ” ì´ì „ decoder layerì˜ ëª¨ë“  ìœ„ì¹˜ì— attend í•  ìˆ˜ëŠ” ìˆì§€ë§Œ, í˜„ì¬ ìœ„ì¹˜ ì „ê¹Œì§€ë§Œ attend í•˜ë„ë¡ <u>masking</u>ì´ë¼ëŠ” ê¸°ë²•ì„ ì ìš©í•©ë‹ˆë‹¤.
  - ì¦‰, "I love you" ë¼ëŠ” ë¬¸ì¥ì´ ìˆì„ ë•Œ `love`ëŠ” `I`ë§Œì„, `you`ëŠ” `I`ì™€ `love`ë¥¼ ì°¸ê³ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - ì´ëŠ” **scaled dot-product attention**ì—ì„œ í˜„ì¬ position ì´í›„ì— ìˆëŠ” sequenceì˜ ê°’ì„ ìŒì˜ ë¬´í•œëŒ€ê°’ê³¼ ê³±í•¨ìœ¼ë¡œì¨ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤. maskingëœ ê°’ì€ softmax functionì„ ì·¨í–ˆì„ ë•Œ ê²°ê³¼ê°’ì´ 0ì´ ë©ë‹ˆë‹¤.


## Position-wise Feed Forward Networks

attention layerì´ì™¸ì—ë„ **Fully connected Feed-Forward network**ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ layerëŠ” ë‘ ê°œì˜ linear layerì™€ ReLU activation functionìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

$$ \text{FFN}(x)=\max(0, xW_1+b_1)W_2+b_2 $$

inputê³¼ outputì˜ dimensionì€ $d_model = 512$ì´ë©°, inner-layerì˜ dimensionì€ $d_ff = 2048$ì…ë‹ˆë‹¤.
> Feed Forward NNì˜ inner layerì—ì„œ ë„ˆë¬´ë‚˜ë„ ë§ì€ memoryë¥¼ ì‚¬ìš©í•´ [Reformer](https://arxiv.org/abs/2001.04451)ì—ì„œëŠ” ì´ë¥¼ *residual connection*ê³¼ *chunking*ì„ ì´ìš©í•˜ì—¬ ê°œì„ í•˜ê¸°ë„ í•©ë‹ˆë‹¤.




## Embeddings and Softmax

- ë‹¤ë¥¸ sequence transduction modelì²˜ëŸ¼ input/outputì„ $d_model$ë¡œ **embedding** í•©ë‹ˆë‹¤. 
- **linear transformation**ê³¼ **softmax function**ì„ ì´ìš©í•´ì„œ decoderì˜ outputì„ *predicted next-token probabilities*ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤.

- íŠ¹ì´í•œ ì ì€ transformer modelì˜ ë‘ê°œì˜ embedding layerì™€ ì´ softmax functionì˜ weightê°€ ê°™ë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì¦‰, linear transformationì—ì„œ ëª¨ë‘ ê°™ì€ weightë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

## Positional Encoding 

transformerëŠ” recurrentë‚˜ convolutionì„ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ìœ„ì¹˜ì— ëŒ€í•œ ì •ë³´ë¥¼ ì¤˜ì•¼í•©ë‹ˆë‹¤. **positional encodding**ì€ embeddingê³¼ ì°¨ì›ì´ ë™ì¼í•˜ë©°($d_model$), embedding vectorì™€ ë”í•¨ìœ¼ë¡œì¨ ìœ„ì¹˜ì •ë³´ë¥¼ ë„£ì–´ì¤ë‹ˆë‹¤.

$$ \begin{matrix}
PE_{(pos,2i)}=\sin(pos/10000^{2i/d_{\text{model}}})\\
PE_{(pos,2i+1)}=\cos(pos/10000^{2i/d_{\text{model}}})
\end{matrix} $$

positional embeddingë„ í•™ìŠµì´ ê°€ëŠ¥í•˜ì§€ë§Œ, sinuidal í•¨ìˆ˜ë¥¼ ì´ìš©í•´ë„ ë¹„ìŠ·í•œ ì„±ëŠ¥ì´ ë‚˜ì™€ ìœ„ì˜ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í–ˆë‹¤ê³  ë…¼ë¬¸ì—ì„œëŠ” ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤.

# 2. Code Practice

## Preparing Data


```python
!apt install python3.7
!pip install -U torchtext==0.6.0
!python -m spacy download en
!python -m spacy download de
```


```python
import torch
import torch.nn as nn
import torch.optim as optim

from torchtext.datasets import Multi30k
from torchtext.data import Field, BucketIterator

import spacy
import numpy as np

import random
import math
import time

SEED = 1234

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
```

### **Tokenizers**
- tokenizersëŠ” ë¬¸ì¥ì„ ê°œë³„ tokenìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
  - e.g. "good morning!" becomes ["good", "morning", "!"]
- nlpë¥¼ ì‰½ê²Œ í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” python packageì¸ `spaCy`ë¥¼ ì´ìš©í•˜ì—¬, tokení™”ë¥¼ í•  ì˜ˆì •ì…ë‹ˆë‹¤.



```python
spacy_de = spacy.load('de')
spacy_en = spacy.load('en')
```


```python
def tokenize_de(text):
  return [tok.text for tok in spacy_de.tokenizer(text)]

def tokenize_en(text):
  return [tok.text for tok in spacy_en.tokenizer(text)]
```

ë‹¤ìŒìœ¼ë¡œëŠ” **Field** ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
 
`batch_first = True`ë¥¼ ì´ìš©í•˜ì—¬ ë¯¸ë‹ˆ batchì˜ ì°¨ì›ì„ ë§¨ ì•ìœ¼ë¡œ í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.


```python
SRC = Field(tokenize= tokenize_de,
            init_token = '<sos>',
            eos_token = '<eos>',
            lower = True,
            batch_first = True)

TRG = Field(tokenize= tokenize_en,
            init_token = '<sos>',
            eos_token = '<eos>',
            lower = True,
            batch_first = True)
```

- datasetìœ¼ë¡œëŠ” [Multi30k dataset](https://github.com/multi30k/dataset)ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” ì•½ 3ë§Œê°œì˜ ì˜ì–´, ë…ì¼ì–´, í”„ë‘ìŠ¤ì–´ ë¬¸ì¥ì´ ìˆëŠ” ë°ì´í„°ì´ë©° ê° ë¬¸ì¥ ë‹¹ 12ê°œì˜ ë‹¨ì–´ê°€ ìˆìŠµë‹ˆë‹¤.
- `exts`ëŠ” sourceì™€ targetìœ¼ë¡œ ì‚¬ìš©í•  ì–¸ì–´ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.


```python
train_data, valid_data, test_data = Multi30k.splits(exts= ('.de', '.en'),
                                                    fields = (SRC, TRG))
```

{:.output_stream}

```
training.tar.gz:   0%|          | 0.00/1.21M [00:00<?, ?B/s]
```

{:.output_stream}

```
downloading training.tar.gz

```

{:.output_stream}

```
training.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.21M/1.21M [00:00<00:00, 6.05MB/s]
validation.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46.3k/46.3k [00:00<00:00, 1.68MB/s]
```

{:.output_stream}

```
downloading validation.tar.gz
downloading mmt_task1_test2016.tar.gz

```

{:.output_stream}

```

mmt_task1_test2016.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66.2k/66.2k [00:00<00:00, 1.72MB/s]

```


```python
print(f"Number of training examples: {len(train_data.examples)}")
print(f"Number of validation examples: {len(valid_data.examples)}")
print(f"Number of testing examples: {len(test_data.examples)}")
```

{:.output_stream}

```
Number of training examples: 29000
Number of validation examples: 1014
Number of testing examples: 1000

```


```python
print(len(vars(train_data.examples[0])['src']))
print(len(vars(train_data.examples[1])['src']))

print(vars(train_data.examples[0]))
print(vars(train_data.examples[1]))
```

{:.output_stream}

```
13
8
{'src': ['zwei', 'junge', 'weiÃŸe', 'mÃ¤nner', 'sind', 'im', 'freien', 'in', 'der', 'nÃ¤he', 'vieler', 'bÃ¼sche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}
{'src': ['mehrere', 'mÃ¤nner', 'mit', 'schutzhelmen', 'bedienen', 'ein', 'antriebsradsystem', '.'], 'trg': ['several', 'men', 'in', 'hard', 'hats', 'are', 'operating', 'a', 'giant', 'pulley', 'system', '.']}

```

### Build Vocabulary
- `build_vocab`í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ê° tokenì„ indexingí•´ì¤ë‹ˆë‹¤. ì´ë•Œ, sourceì™€ targetì˜ vocabularyëŠ” ë‹¤ë¦…ë‹ˆë‹¤.
- `min_freq`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì†Œ 2ë²ˆ ì´ìƒ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ë§Œ vocabularyì— ë„£ì–´ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ë•Œ, í•œë²ˆë§Œ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ëŠ” `<unk>` tokenìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.
- ì´ë•Œ, vocabularyëŠ” **training set**ì—ì„œë§Œ ë§Œë“¤ì–´ì ¸ì•¼í•©ë‹ˆë‹¤. *(validation/test setì— ëŒ€í•´ì„œëŠ” ë§Œë“¤ì–´ì§€ë©´ ì•ˆë¨!!)* 


```python
SRC.build_vocab(train_data, min_freq = 2)
TRG.build_vocab(train_data, min_freq = 2)
```


```python
print(f"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}")
print(f"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}")
```

{:.output_stream}

```
Unique tokens in source (de) vocabulary: 7855
Unique tokens in target (en) vocabulary: 5893

```

### Create the iterators
- `BucketIterator`ë¥¼ ì´ìš©í•˜ì—¬ batch sizeë³„ë¡œ tokenë“¤ì„ ë¬¶ê³ , ì–´íœ˜ë¥¼ ì½ì„ ìˆ˜ ìˆëŠ” tokenì—ì„œ indexë¡œ ë³€í™˜í•´ì¤ë‹ˆë‹¤.


```python
# for using GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```


```python
BATCH_SIZE = 128

train_iterator, valid_iterator, test_iterator = BucketIterator.splits(
    (train_data, valid_data, test_data),
    batch_size = BATCH_SIZE,
    device = device
)
```

## Building the Model

### Encoder

<p align="center"><img src="https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/transformer4.png?raw=1" width = "200" ></p>


**Positional Encoding**

TransformerëŠ” recurrentê°€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ìœ„ì¹˜ ì •ë³´ë¥¼ ì£¼ê¸° ìœ„í•´ì„œëŠ” *positional encoding*ì´ í•„ìš”í•©ë‹ˆë‹¤. 

ì´ë²ˆì— êµ¬í˜„í•œ notebookì—ì„œëŠ” Transformer ë…¼ë¬¸ì˜ ê³ ì •ëœ ì •ì  ì„ë² ë”©(fixed static embedding)ì¸  positional encodingì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ , BERTì—ì„œ ì‚¬ìš©ëœ **postional embedding**ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. (ë…¼ë¬¸ì˜ siní•¨ìˆ˜ë¡œ ëœ positional encoding ê´€ë ¨ ì½”ë“œëŠ” [ì´ ê¸€](http://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding) ì°¸ê³ )



```python
class Encoder(nn.Module):
    def __init__(self, 
                 input_dim, 
                 hid_dim, 
                 n_layers, 
                 n_heads, 
                 pf_dim,
                 dropout, 
                 device,
                 max_length = 100):
        super().__init__()

        self.device = device
        
        ''' Input Embedding '''
        self.tok_embedding = nn.Embedding(input_dim, hid_dim)
        # <sos> tokenë¶€í„° ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ë¡œ ì„¤ì •í•œ 100ê¹Œì§€ positional embeddingì„ í•¨.
        self.pos_embedding = nn.Embedding(max_length, hid_dim)
        
        ''' Multiple Encoder '''
        # ë…¼ë¬¸ì—ì„œëŠ” N=6ê°œì˜ encoder layerë¥¼ ì‚¬ìš©í•˜ì˜€ìŒ
        self.layers = nn.ModuleList([EncoderLayer(hid_dim, 
                                                  n_heads, 
                                                  pf_dim,
                                                  dropout, 
                                                  device) 
                                     for _ in range(n_layers)])
        
        self.dropout = nn.Dropout(dropout)
        
        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)
        
    def forward(self, src, src_mask):
        
        #src = [batch size, src len]
        #src_mask = [batch size, 1, 1, src len]
        
        batch_size = src.shape[0]
        src_len = src.shape[1]
        
        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)

        #pos = [batch size, src len]
        
        # src ë¬¸ì¥ì˜ embedding vectorì™€ positional vectorë¥¼ ë”í•´ì¤Œ
        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))
        
        #src = [batch size, src len, hid dim]
        
        for layer in self.layers:
            src = layer(src, src_mask)
            
        #src = [batch size, src len, hid dim]
            
        return src
```


```python
# pos example
torch.arange(0, 7).unsqueeze(0).repeat(2, 1)
```




{:.output_data_text}

```
tensor([[0, 1, 2, 3, 4, 5, 6],
        [0, 1, 2, 3, 4, 5, 6]])
```



#### Encoder layer

ë‘ ê°œì˜ sub layer ì™¸ì—ë„ [Layer Normalization](https://arxiv.org/abs/1607.06450) layer ë¥¼ ì¶”ê°€í•˜ì—¬ ì„±ëŠ¥ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.



```python
class EncoderLayer(nn.Module):
    def __init__(self, 
                 hid_dim, 
                 n_heads, 
                 pf_dim,  
                 dropout, 
                 device):
        super().__init__()
        
        ''' Multi Head self-Attention '''        
        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)
        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)

        ''' Positional FeedForward Layer'''
        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, 
                                                                     pf_dim, 
                                                                     dropout)
        self.ff_layer_norm = nn.LayerNorm(hid_dim)

        self.dropout = nn.Dropout(dropout)
        
    def forward(self, src, src_mask):
        
        #src = [batch size, src len, hid dim]
        #src_mask = [batch size, 1, 1, src len] 
                
        #self attention
        _src, _ = self.self_attention(src, src, src, src_mask)
        
        #dropout, residual connection and layer norm
        src = self.self_attn_layer_norm(src + self.dropout(_src))
        
        #src = [batch size, src len, hid dim]
        
        #positionwise feedforward
        _src = self.positionwise_feedforward(src)
        
        #dropout, residual and layer norm
        src = self.ff_layer_norm(src + self.dropout(_src))
        
        #src = [batch size, src len, hid dim]
        
        return src
```

### Multi Head Attention Layer


```python
class MultiHeadAttentionLayer(nn.Module):
    def __init__(self, hid_dim, n_heads, dropout, device):
        super().__init__()
        
        assert hid_dim % n_heads == 0
        
        self.hid_dim = hid_dim
        self.n_heads = n_heads
        self.head_dim = hid_dim // n_heads
        
        # encoderì˜ Q, K, VëŠ” ëª¨ë‘ ê°™ì€ spaceì—ì„œ ì˜µë‹ˆë‹¤.
        
        self.fc_q = nn.Linear(hid_dim, hid_dim)
        self.fc_k = nn.Linear(hid_dim, hid_dim)
        self.fc_v = nn.Linear(hid_dim, hid_dim)
        
        self.fc_o = nn.Linear(hid_dim, hid_dim)
        
        self.dropout = nn.Dropout(dropout)
        
        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)
        
    def forward(self, query, key, value, mask = None):
        
        batch_size = query.shape[0]
        
        #query = [batch size, query len, hid dim]
        #key = [batch size, key len, hid dim]
        #value = [batch size, value len, hid dim]
                
        Q = self.fc_q(query)
        K = self.fc_k(key)
        V = self.fc_v(value)
        
        #Q = [batch size, query len, hid dim]
        #K = [batch size, key len, hid dim]
        #V = [batch size, value len, hid dim]
                
        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)
        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)
        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)
        
        #Q = [batch size, n heads, query len, head dim]
        #K = [batch size, n heads, key len, head dim]
        #V = [batch size, n heads, value len, head dim]
                
        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale
        
        #energy = [batch size, n heads, query len, key len]
        
        if mask is not None:
            energy = energy.masked_fill(mask == 0, -1e10)
        
        attention = torch.softmax(energy, dim = -1)
                
        #attention = [batch size, n heads, query len, key len]
                
        x = torch.matmul(self.dropout(attention), V)
        
        #x = [batch size, n heads, query len, head dim]
        
        x = x.permute(0, 2, 1, 3).contiguous()
        
        #x = [batch size, query len, n heads, head dim]
        
        x = x.view(batch_size, -1, self.hid_dim)
        
        #x = [batch size, query len, hid dim]
        
        x = self.fc_o(x)
        
        #x = [batch size, query len, hid dim]
        
        return x, attention
```

### Position-wise Feedforward Layer



```python
class PositionwiseFeedforwardLayer(nn.Module):
    def __init__(self, hid_dim, pf_dim, dropout):
        super().__init__()
        
        self.fc_1 = nn.Linear(hid_dim, pf_dim)
        self.fc_2 = nn.Linear(pf_dim, hid_dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        
        #x = [batch size, seq len, hid dim]
        
        x = self.dropout(torch.relu(self.fc_1(x)))
        
        #x = [batch size, seq len, pf dim]
        
        x = self.fc_2(x)
        
        #x = [batch size, seq len, hid dim]
        
        return x
```

### Decoder
<p align="center"><img src="https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/transformer5.png?raw=1" width = "200" ></p>


```python
class Decoder(nn.Module):
    def __init__(self, 
                 output_dim, 
                 hid_dim, 
                 n_layers, 
                 n_heads, 
                 pf_dim, 
                 dropout, 
                 device,
                 max_length = 100):
        super().__init__()
        
        self.device = device
        
        self.tok_embedding = nn.Embedding(output_dim, hid_dim)
        
        self.pos_embedding = nn.Embedding(max_length, hid_dim)
        
        self.layers = nn.ModuleList([DecoderLayer(hid_dim, 
                                                  n_heads, 
                                                  pf_dim, 
                                                  dropout, 
                                                  device)
                                     for _ in range(n_layers)])
        
        self.fc_out = nn.Linear(hid_dim, output_dim)
        
        self.dropout = nn.Dropout(dropout)
        
        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)
        
    def forward(self, trg, enc_src, trg_mask, src_mask):
        
        #trg = [batch size, trg len]
        #enc_src = [batch size, src len, hid dim]
        #trg_mask = [batch size, 1, trg len, trg len]
        #src_mask = [batch size, 1, 1, src len]
                
        batch_size = trg.shape[0]
        trg_len = trg.shape[1]
        
        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)
                            
        #pos = [batch size, trg len]
            
        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))
                
        #trg = [batch size, trg len, hid dim]
        
        for layer in self.layers:
            trg, attention = layer(trg, enc_src, trg_mask, src_mask)
        
        #trg = [batch size, trg len, hid dim]
        #attention = [batch size, n heads, trg len, src len]
        
        output = self.fc_out(trg)
        
        #output = [batch size, trg len, output dim]
            
        return output, attention
```

#### Decoder Layer


```python
class DecoderLayer(nn.Module):
    def __init__(self, 
                 hid_dim, 
                 n_heads, 
                 pf_dim, 
                 dropout, 
                 device):
        super().__init__()
        
        ''' Multi Head self Attention'''
        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)
        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)

        ''' Encoder-decoder attention'''
        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)
        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)

        ''' Positionwise FeedForward Layer'''
        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, 
                                                                     pf_dim, 
                                                                     dropout)
        self.ff_layer_norm = nn.LayerNorm(hid_dim)

        self.dropout = nn.Dropout(dropout)
        
    def forward(self, trg, enc_src, trg_mask, src_mask):
        
        #trg = [batch size, trg len, hid dim]
        #enc_src = [batch size, src len, hid dim]
        #trg_mask = [batch size, 1, trg len, trg len]
        #src_mask = [batch size, 1, 1, src len]
        
        #self attention
        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)
        
        #dropout, residual connection and layer norm
        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))
            
        #trg = [batch size, trg len, hid dim]
            
        #encoder attention
        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)
        
        #dropout, residual connection and layer norm
        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))
                    
        #trg = [batch size, trg len, hid dim]
        
        #positionwise feedforward
        _trg = self.positionwise_feedforward(trg)
        
        #dropout, residual and layer norm
        trg = self.ff_layer_norm(trg + self.dropout(_trg))
        
        #trg = [batch size, trg len, hid dim]
        #attention = [batch size, n heads, trg len, src len]
        
        return trg, attention
```

### Seq2Seq



```python
class Seq2Seq(nn.Module):
    def __init__(self, 
                 encoder, 
                 decoder, 
                 src_pad_idx, 
                 trg_pad_idx, 
                 device):
        super().__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.src_pad_idx = src_pad_idx
        self.trg_pad_idx = trg_pad_idx
        self.device = device
        
    def make_src_mask(self, src):
        
        #src = [batch size, src len]
        
        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)

        #src_mask = [batch size, 1, 1, src len]

        return src_mask
    
    def make_trg_mask(self, trg):
        
        #trg = [batch size, trg len]
        
        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)
        
        #trg_pad_mask = [batch size, 1, 1, trg len]
        
        trg_len = trg.shape[1]
        
        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()
        
        #trg_sub_mask = [trg len, trg len]
            
        trg_mask = trg_pad_mask & trg_sub_mask
        
        #trg_mask = [batch size, 1, trg len, trg len]
        
        return trg_mask

    def forward(self, src, trg):
        
        #src = [batch size, src len]
        #trg = [batch size, trg len]
                
        src_mask = self.make_src_mask(src)
        trg_mask = self.make_trg_mask(trg)
        
        #src_mask = [batch size, 1, 1, src len]
        #trg_mask = [batch size, 1, trg len, trg len]
        
        enc_src = self.encoder(src, src_mask)
        
        #enc_src = [batch size, src len, hid dim]
                
        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)
        
        #output = [batch size, trg len, output dim]
        #attention = [batch size, n heads, trg len, src len]
        
        return output, attention
```


## Training the Seq2Seq Model


```python
INPUT_DIM = len(SRC.vocab)
OUTPUT_DIM = len(TRG.vocab)
HID_DIM = 256
ENC_LAYERS = 3
DEC_LAYERS = 3
ENC_HEADS = 8
DEC_HEADS = 8
ENC_PF_DIM = 512
DEC_PF_DIM = 512
ENC_DROPOUT = 0.1
DEC_DROPOUT = 0.1

enc = Encoder(INPUT_DIM, 
              HID_DIM, 
              ENC_LAYERS, 
              ENC_HEADS, 
              ENC_PF_DIM, 
              ENC_DROPOUT, 
              device)

dec = Decoder(OUTPUT_DIM, 
              HID_DIM, 
              DEC_LAYERS, 
              DEC_HEADS, 
              DEC_PF_DIM, 
              DEC_DROPOUT, 
              device)
```


```python
SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]
TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]

model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)
```


```python
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'The model has {count_parameters(model):,} trainable parameters')
```

{:.output_stream}

```
The model has 9,038,853 trainable parameters

```

- ì´ˆê¸° ê°€ì¤‘ì¹˜ê°’ì€ $\mathcal{N}(0, 0.01)$ì˜ ì •ê·œë¶„í¬ë¡œë¶€í„° ì–»ì—ˆìŠµë‹ˆë‹¤.


```python
def initialize_weights(m):
    if hasattr(m, 'weight') and m.weight.dim() > 1:
        nn.init.xavier_uniform_(m.weight.data)

model.apply(initialize_weights);
```

- optimizerí•¨ìˆ˜ë¡œëŠ” `Adam`ì„ ì‚¬ìš©í•˜ì˜€ê³ , loss functionìœ¼ë¡œëŠ” `CrossEntropyLoss`ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, `<pad>` tokenì— ëŒ€í•´ì„œëŠ” loss ê³„ì‚°ì„ í•˜ì§€ ì•Šë„ë¡ ì¡°ê±´ì„ ë¶€ì—¬í–ˆìŠµë‹ˆë‹¤.


```python
LEARNING_RATE = 0.0005

optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)
```


```python
criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)
```

### Training


```python
def train(model, iterator, optimizer, criterion, clip):
    
    model.train()
    
    epoch_loss = 0
    
    for i, batch in enumerate(iterator):
        
        src = batch.src
        trg = batch.trg
        
        optimizer.zero_grad()
        
        output, _ = model(src, trg[:,:-1])
                
        #output = [batch size, trg len - 1, output dim]
        #trg = [batch size, trg len]
            
        output_dim = output.shape[-1]
            
        output = output.contiguous().view(-1, output_dim)
        trg = trg[:,1:].contiguous().view(-1)
                
        #output = [batch size * trg len - 1, output dim]
        #trg = [batch size * trg len - 1]
            
        loss = criterion(output, trg)
        
        loss.backward()
        
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        
        optimizer.step()
        
        epoch_loss += loss.item()
        
    return epoch_loss / len(iterator)
```

### Evaluation


```python
def evaluate(model, iterator, criterion):
    
    model.eval()
    
    epoch_loss = 0
    
    with torch.no_grad():
    
        for i, batch in enumerate(iterator):

            src = batch.src
            trg = batch.trg

            output, _ = model(src, trg[:,:-1])
            
            #output = [batch size, trg len - 1, output dim]
            #trg = [batch size, trg len]
            
            output_dim = output.shape[-1]
            
            output = output.contiguous().view(-1, output_dim)
            trg = trg[:,1:].contiguous().view(-1)
            
            #output = [batch size * trg len - 1, output dim]
            #trg = [batch size * trg len - 1]
            
            loss = criterion(output, trg)

            epoch_loss += loss.item()
        
    return epoch_loss / len(iterator)
```


```python
def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs
```

### Train the model through multiple epochs


```python
N_EPOCHS = 10
CLIP = 1

best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):
    
    start_time = time.time()
    
    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)
    valid_loss = evaluate(model, valid_iterator, criterion)
    
    end_time = time.time()
    
    epoch_mins, epoch_secs = epoch_time(start_time, end_time)
    
    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'tut6-model.pt')
    
    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')
```

{:.output_stream}

```
Epoch: 01 | Time: 0m 40s
	Train Loss: 2.018 | Train PPL:   7.523
	 Val. Loss: 1.864 |  Val. PPL:   6.452
Epoch: 02 | Time: 0m 40s
	Train Loss: 1.734 | Train PPL:   5.665
	 Val. Loss: 1.755 |  Val. PPL:   5.783
Epoch: 03 | Time: 0m 40s
	Train Loss: 1.525 | Train PPL:   4.596
	 Val. Loss: 1.673 |  Val. PPL:   5.327
Epoch: 04 | Time: 0m 39s
	Train Loss: 1.362 | Train PPL:   3.903
	 Val. Loss: 1.640 |  Val. PPL:   5.156
Epoch: 05 | Time: 0m 40s
	Train Loss: 1.224 | Train PPL:   3.400
	 Val. Loss: 1.610 |  Val. PPL:   5.002
Epoch: 06 | Time: 0m 40s
	Train Loss: 1.109 | Train PPL:   3.030
	 Val. Loss: 1.621 |  Val. PPL:   5.060
Epoch: 07 | Time: 0m 40s
	Train Loss: 1.008 | Train PPL:   2.739
	 Val. Loss: 1.635 |  Val. PPL:   5.130
Epoch: 08 | Time: 0m 40s
	Train Loss: 0.922 | Train PPL:   2.515
	 Val. Loss: 1.652 |  Val. PPL:   5.216
Epoch: 09 | Time: 0m 40s
	Train Loss: 0.843 | Train PPL:   2.324
	 Val. Loss: 1.675 |  Val. PPL:   5.340
Epoch: 10 | Time: 0m 39s
	Train Loss: 0.774 | Train PPL:   2.169
	 Val. Loss: 1.702 |  Val. PPL:   5.485

```


```python
model.load_state_dict(torch.load('tut6-model.pt'))

test_loss = evaluate(model, test_iterator, criterion)

print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')
```

{:.output_stream}

```
| Test Loss: 1.679 | Test PPL:   5.359 |

```

## Inference



```python
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
```


```python
def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):
    
    model.eval()
        
    if isinstance(sentence, str):
        nlp = spacy.load('de_core_news_sm')
        tokens = [token.text.lower() for token in nlp(sentence)]
    else:
        tokens = [token.lower() for token in sentence]

    tokens = [src_field.init_token] + tokens + [src_field.eos_token]
        
    src_indexes = [src_field.vocab.stoi[token] for token in tokens]

    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)
    
    src_mask = model.make_src_mask(src_tensor)
    
    with torch.no_grad():
        enc_src = model.encoder(src_tensor, src_mask)

    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]

    for i in range(max_len):

        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)

        trg_mask = model.make_trg_mask(trg_tensor)
        
        with torch.no_grad():
            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)
        
        pred_token = output.argmax(2)[:,-1].item()
        
        trg_indexes.append(pred_token)

        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:
            break
    
    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]
    
    return trg_tokens[1:], attention
```


```python
def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):
    
    assert n_rows * n_cols == n_heads
    
    fig = plt.figure(figsize=(15,25))
    
    for i in range(n_heads):
        
        ax = fig.add_subplot(n_rows, n_cols, i+1)
        
        _attention = attention.squeeze(0)[i].cpu().detach().numpy()

        cax = ax.matshow(_attention, cmap='bone')

        ax.tick_params(labelsize=12)
        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], 
                           rotation=45)
        ax.set_yticklabels(['']+translation)

        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

    plt.show()
    plt.close()
```


```python
example_idx = 8

src = vars(train_data.examples[example_idx])['src']
trg = vars(train_data.examples[example_idx])['trg']

print(f'src = {src}')
print(f'trg = {trg}')
```

{:.output_stream}

```
src = ['eine', 'frau', 'mit', 'einer', 'groÃŸen', 'geldbÃ¶rse', 'geht', 'an', 'einem', 'tor', 'vorbei', '.']
trg = ['a', 'woman', 'with', 'a', 'large', 'purse', 'is', 'walking', 'by', 'a', 'gate', '.']

```


```python
translation, attention = translate_sentence(src, SRC, TRG, model, device)

print(f'predicted trg = {translation}')
```

{:.output_stream}

```
predicted trg = ['a', 'woman', 'with', 'a', 'large', 'purse', 'walks', 'past', 'a', 'gate', '.', '<eos>']

```


```python
display_attention(src, translation, attention)
```


<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/6_Attention_is_All_You_Need_68_0.png?raw=1" width = "500" ></p>



```python
example_idx = 6

src = vars(valid_data.examples[example_idx])['src']
trg = vars(valid_data.examples[example_idx])['trg']

print(f'src = {src}')
print(f'trg = {trg}')
```

{:.output_stream}

```
src = ['ein', 'brauner', 'hund', 'rennt', 'dem', 'schwarzen', 'hund', 'hinterher', '.']
trg = ['a', 'brown', 'dog', 'is', 'running', 'after', 'the', 'black', 'dog', '.']

```


```python
translation, attention = translate_sentence(src, SRC, TRG, model, device)

print(f'predicted trg = {translation}')
```

{:.output_stream}

```
predicted trg = ['a', 'brown', 'dog', 'running', 'after', 'a', 'black', 'dog', '.', '<eos>']

```


```python
display_attention(src, translation, attention)
```


<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/nlp/6_Attention_is_All_You_Need_71_0.png?raw=1" width = "500" ></p>


## BLEU


```python
from torchtext.data.metrics import bleu_score

def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):
    
    trgs = []
    pred_trgs = []
    
    for datum in data:
        
        src = vars(datum)['src']
        trg = vars(datum)['trg']
        
        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)
        
        #cut off <eos> token
        pred_trg = pred_trg[:-1]
        
        pred_trgs.append(pred_trg)
        trgs.append([trg])
        
    return bleu_score(pred_trgs, trgs)
```


```python
bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)

print(f'BLEU score = {bleu_score*100:.2f}')
```

{:.output_stream}

```
BLEU score = 35.38

```


```python

```
