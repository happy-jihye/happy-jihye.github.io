---
title: "[1] DeepLearning 이론"
excerpt: " "

categories: 
  - nlp
tags: 
  - deeplearning
  - ai
  - nlp
  - LLM

search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---

## [1] Machine Learning

1. **Linear Regression (선형 회귀)**
    - 선형 회귀는 연속적인 값 예측, MSE 비용함수 사용
    - 입력 변수와 출력 변수 사이의 선형 관계를 모델링하는 알고리즘
    - 사용 예시: 집 가격 예측, 매출 예측 등 연속적인 값을 예측하는 문제에 많이 사용
    - Cost Function: 선형 회귀에서는 일반적으로 Mean Squared Error (MSE)를 비용 함수로 사용.
2. **Logistic Regression (로지스틱 회귀)**
    - 로지스틱 회귀는 이진 분류, BCE 비용함수 사용
    - 개념: 로지스틱 회귀는 이진 분류 문제를 해결하기 위한 알고리즘. 선형 회귀와 유사하지만, 출력 함수로 시그모이드 함수를 사용하여 0과 1 사이의 확률 값을 반환함. 이 확률 값을 기준으로 이진 분류를 수행
    - 사용 예시: 스팸 메일 분류, 질병 진단 등 이진 분류 문제에 많이 사용
    - Cost Function: 로지스틱 회귀에서는 일반적으로 Binary Cross-Entropy (BCE)를 비용 함수로 사용. BCE는 실제 클래스와 예측 확률 사이의 차이를 측정
3. **Softmax Regression (소프트맥스 회귀)**
    - 소프트맥스 회귀는 다중 클래스 분류, 크로스 엔트로피 손실 사용
    - 개념: 소프트맥스 회귀는 다중 클래스 분류 문제를 해결하기 위한 알고리즘. 입력 변수의 선형 조합에 소프트맥스 함수를 적용하여 각 클래스에 대한 확률을 계산. 가장 높은 확률을 가진 클래스를 예측 결과로 선택.
    - 사용 예시: 손글씨 숫자 인식, 이미지 분류 등 다중 클래스 분류 문제에 많이 사용
    - Cost Function: 소프트맥스 회귀에서는 일반적으로 Cross-Entropy Loss를 비용 함수로 사용. 크로스 엔트로피 손실은 실제 클래스와 예측 확률 분포 사이의 차이를 측정.

## [2] [DeepLearning과 Perceptron, Artificial Neural Network](https://www.cognex.com/ko-kr/blogs/deep-learning/research/what-is-deep-learning-1)

### activation function

- perceptron (activation func = step func)
- logistic regression
- softmax regression

<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/20-dl/7.jpeg?raw=1" width = "600" ></p>

- GeLU (Gaussian Error Linear Unit)
    - dropout, zoneout, ReLU의 특성을 조합하여 유도
    - 입력값을 가우시안 함수에 적용 (ReLU의 smoothing 버전)
    - GeLU는 음수 입력값도 작은 값으로 매핑하여 약간의 정보 보존 가능 + 음수영역에서도 작은 gradient를 가지기 때문에 gradient vanishing 문제 완화됨
    - LLM에 GeLU 많이 사용됨
        - LLM은 매우 깊은 신경망을 가지므로 gradient vanishing 문제가 덜 생기는 GeLU가 더 적합함
        - 언어 데이터는 연속적이고 유연한 특성을 가지므로 smoothing 한 GeLU가 더 적합함
        - 음수 입력값의 정보를 일부 보존하기 때문에 미묘한 뉘앙스를 포착하는데 더 좋음
    
    <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/20-dl/0.png?raw=1" width = "300" ></p>
    
- GLU (Gated Linear Unit)
    - GLU는 입력 벡터를 두 부분으로 나누어 한 부분은 선형 변환을, 다른 부분은 게이트로 사용
    - GLU(x) = (x * W + b) ⊙ σ(x * V + c), ⊙는 element-wise multiplication, σ는 시그모이드 함수
    - 게이트를 통해 정보의 흐름을 조절하여 중요한 정보는 통과시키고 불필요한 정보는 차단할 수 있음
- SwiGLU
    - 수식: SwiGLU(x) = Swish(x * W1 + b1) ⊙ (x * W2 + b2)
    - SwiGLU는 Swish 함수의 장점과 GLU의 게이팅 메커니즘을 결합
        - Swish 함수는 무한 차분 가능하며, 음수 영역에서도 작은 기울기를 가지므로 기울기 소실 문제를 완화시켜줌
        - GLU의 게이팅 메커니즘은 중요한 정보는 통과시키고 불필요한 정보는 차단하여 정보 흐름을 조절합니다.
- GEGLU(GELU with GLU):
    - GEGLU는 GELU와 GLU를 결합한 활성화 함수
    - 수식: GEGLU(x) = GELU(x * W1 + b1) ⊙ (x * W2 + b2)
        - GELU로 입력을 변환한 후, GLU처럼 게이트를 적용하여 정보 흐름을 조절합니다.


### Underfitting과 Overfitting

**Overfitting을 방지하는 방법**

- (1) data 양 늘리기 (2) batch normalization (3) 모델의 complexity를 줄이기 (4) drop out [(5) weight regularization](https://seongyun-dev.tistory.com/52)
- **dropout** 의 효과
    - (1) overfitting 방지 (모델이 일부 뉴런에 의존하는걸 방지, robust 강화) (2) ensemble (학습할 때마다 랜덤한 뉴런을 선택 → 여러 모델을 합친 효과와 유사하게 동작) (3) 효율적 계산 가능
- **Regularization**
    - norm: 유한 차원의 벡터공간에서 벡터의 절대적 크기 (magnitude) 혹은 벡터 간 distance, Lp-norm (르베그 공간)
    - L1 norm (manhattan distance), L2 norm (Euclidean distance)
    
    <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/20-dl/1.png?raw=1" width = "500" ></p>
        
### Loss function

- Cross entropy Loss
    - KL divergence 와 유사
    
    <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/20-dl/2.png?raw=1" width = "800" ></p>
    
### Optimization

경사하강법을 기반으로 objective function에 대한 최적의 파라미터를 찾는 알고리즘
    
<p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/20-dl/8.jpg?raw=1" width = "800" ></p>

- **[1] Gradient Descent**
    - 모델의 parameter를 최적화하는데 사용되는 알고리즘. loss function의 gradient를 계산한 후 loss가 최소화 되는 방향으로 weight를 업데이트함
    - 한계: (1) local minimum에 빠지기 쉽다 (2) 안장점 (saddle point)를 벗어나지 못한다
- **[2] momentum**
    - GD에서 loss를 업데이트할때 이전 step에서의 update 방향을 고려하여 (관성을 고려하여) 업데이트
    - 한계: optimum point를 관성에 의해 지나칠 수 있음
        - → Nesterov Accelerated Gradient (NAD) 같은 optimizer도 나옴
- **[3] AdaGrad**
    - feature별로 adaptive learning rate를 적용 (이전까지의 gradient의 제곱값을 누적하여 lr 결정) gradient가 클수록 모델을 조금 업데이트하고, 작을수록 모델을 많이 업데이트
        - → 학습 초기에는 빠르게 수렴하다가 후반에는 안정적으로 수렴
    - 학습이 진행될수록 lr가 급격하게 작아져서 local optima + saddle point에 빠질 수 있음
- **[4] RMSProp**
    - Adagrad의 단점을 보완 → gradient의 제곱값의 지수이동평균을 사용 → 학습이 진행될수록 lr이 급격하게 작아지는 문제 보완
- **[5] Adam**
    - momentum + RMSProp
        - momentum 처럼 과거의 gradient 를 고려 (방향)
        - RMSProp 처럼 adaptive lr 사용
    - gradient의 1, 2차 momentum (평균과 분산)을 추정하여 lr 을 조정. 학습속도가 빠르고 hyperparameter에 대한 민감도가 낮음

[Loss Function과 Optimization](https://happy-jihye.github.io/cs231n/cs231n-3/) 참고

### Weight Initialization

- **[1] zero initialization**
    - 모든 parameter의 값을 0으로 초기화 하는 것 → backprop 때 모두 같은 값으로 변하게 되어 의미 X (학습해도 weight가 안변함)
- **[2] random initialization**
    - ex) gaussian distribution에서 샘플링
        - layer가 깊어질수록 output cost가 0에 수렴 → zero ini랑 비슷
        - activation function을 넣은 DNN에서는 문제가 됨 (layer가 깊어질수록 activation 값이 0으로 수렴) → gradient vanishing 문제 발생 가능
    - weight를 큰 난수로 초기화 → 뉴런의 출력이 커져 -1, 1로 포화 & gradient는 0으로 수렴 → gradient vanishing
- **[3] Lecun Initialization**
    - weight를 적절한 범위 내에서 무작위로 초기화. 입력 유닛의 개수에 맞게 분산을 조절
        - 각 출력 분산을 일정하게 유지하게 해서 네트워크의 수렴속도를 높임
    - Lecun Initialization은 주로 시그모이드나 하이퍼볼릭 탄젠트 활성화 함수를 사용할 때 효과적
- **[4] Xavier initialization**
    - 입출력의 분산을 맞춰주는 게 목표
        - Lecun Initialization은 입력 유닛의 개수만을 고려하는 반면, Xavier Initialization은 입력과 출력 유닛의 개수를 모두 고려
        - → gradient vanishing / exploding 문제 완화 (출력의 분산이 일정하게 유지되어 gradient가 사라지지 않음)
        - → gradient를 적절하게 학습시켜 학습속도가 빨라짐
    - 시그모이드나 하이퍼볼릭 탄젠트와 같은 활성화 함수를 사용할 때 효과적
- **[5] He initialization**
    - ReLU 계열의 활성화 함수를 사용할 때 최적화
        - → relu 계열은 음수값을 0으로 만들기 때문에 분산이 입력보다 작아지는 경향 → he는 이를 고려하여 xavier 보다 큰 분산을 가지도록 설계. relu 의 특성을 반영하여 입력과 출력의 분산을 맞춰줌

### Normalization

- [**[1] batch normalization**](https://gaussian37.github.io/dl-concept-batchnorm/)
    - mini-batch 단위로 데이터를 normalize하는 방식. 각 feature map의 channel 별로 평균과 분산을 구한 후 normalize
    - 사용) CNN, MLP에서 많이 사용. 특히 image classification, object detection 등에 사용
        - BUT, RNN model 에는 적용이 힘듦 (rnn은 time step마다 각기 다른 데이터가 연속적으로 나오는 sequence data를 다룸 → batchnorm 적용이 힘듦)
    - SGD에서 batch 단위로 학습하게 되면, `internal covariant shift` 문제가 생김
        - 각 계층에서 feature가 입력되고 난 후 → conv, fc, … → activation 를 거치는데 그럼 연산 전후의 data distribution이 달라짐. (layer마다 분포가 다 달라짐) 이를 맞추기 위해 batchnorm
        
        <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/20-dl/3.png?raw=1" width = "400" ></p>
        
        <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/20-dl/4.png?raw=1" width = "700" ></p>
        
    - batchnorm은 학습과 추론때 다르게 적용되어야함
        - 학습 단계의 batchnorm
            - 학습단계에서 모든 feature에 정규화를 해주면, gradient descent에 따른 weight의 반응이 같아져 학습에 유리해짐
            - batchnorm은 γ, β의 역할이 중요함
                
                $$
                BN(X) = \gamma \left( \frac{X - \mu_{batch}}{\sigma_{batch}} \right) + \beta 
                $$
                
                - 왜냐하면 batchnorm 후에 activation function이 적용되는데, 이때 relu를 사용하면 음수에 해당하는 부분이 날라감. 따라서 relu 같은 activation function을 적용하더라도 이 값이 사라지지 않도록 정규화값에 γ, β 를 곱하고 더해줌
        - 추론 단계의 batchnorm
            - 학습 때와 달리 batch 단위로 평균과 분산을 구하기 어려움 → 학습 때 저장된 평균/분산을 사용
                - 평균 - 이동평균 (moving average): 학습시 최근 N개의 평균값
                - 분산 - 지수평균 (exponential average)
- **[2] layer normalization**
    - layer를 기반으로 하여 data sample 단위로 normalize (각 sample의 feature map 전체에 대해 normalize)
    - sequence에 따른 고정길이 정규화. 일반화를 잘함
    - 사용) NLP 모델에 많이 사용 (RNN, Transformer 구조 등)
    
    <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/20-dl/5.png?raw=1" width = "600" ></p>
    
- **[3] Instance Norm**
    - data sample의 channel 별로 normalize
    - 사용) 이미지 스타일 변환 계열에 많이 사용됨 (style-transfer, image generation)
        - [stylegan 에서도 adain 사용](https://happy-jihye.github.io/gan/gan-6/#2-style-based-generator)
        - batchnorm은 batch 별 normalize를 한다면, instance norm은 mini-batch에서 이미지 한장씩만을 normalize 한 후 개별 이미지 분포를 사용
    
    <p align="center"><img src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/2024/nlp/20-dl/6.png?raw=1" width = "500" ></p>
    
- **[4] Group Normalization**
    - BN과 LN의 중간 형태, channel을 그룹별로 나눠서 normalize
    - batch가 작거나 메모리 제약이 있는 상태에서 BN의 대용으로 사용
    - 사용) image segmentation, mask r-cnn

### **Forward 와 Backward**

---

**Reference**
[자연어천재만재](https://heygeronimo.tistory.com/)
[06-02 머신 러닝 훑어보기](https://wikidocs.net/32012)
