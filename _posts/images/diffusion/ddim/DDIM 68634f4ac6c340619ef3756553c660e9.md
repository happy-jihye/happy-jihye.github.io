# DDIM

<aside>
ğŸ’¡ DDPMì€ adversarial training ì—†ì´ë„ image generationì´ ì˜ë¨ì„ ì¦ëª…í•˜ì˜€ë‹¤. ê·¸ëŸ¬ë‚˜ markov chainì„ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì¶”ë¡ í•˜ê¸° ë•Œë¬¸ì— sampleì„ ìƒì„±í•˜ë ¤ë©´ ë§ì€ step(ê±°ì˜ ìˆ˜ì²œ step)ì„ ê±°ì³ì•¼í•œë‹¤ëŠ” ë¬¸ì œê°€ ìˆë‹¤.

DDIMì—ì„œëŠ” ì¢€ ë” ë¹ ë¥´ê²Œ sampleì„ ìƒì„±í•˜ê¸° ìœ„í•´ non-markovian diffusion processë¡œ DDPMì„ ì¼ë°˜í™”í•œë‹¤. non-Markovian processë¥¼ í†µí•´ ì¢€ë” deterministicí•œ generative processë¥¼ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìœ¼ë©°, high qualityì˜ sampleì„ ë³´ë‹¤ ë¹ ë¥´ê²Œ ìƒì„±í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.

</aside>

DDPMì€ forward *diffusion process* (from data to noise)ì˜ ì—­ê³¼ì •, generative process (from noise to data)ë¥¼ í•™ìŠµí•œë‹¤. ì´ ìƒì„± ê³¼ì •ì€ markov chainì„ í†µí•´ ì´ë¤„ì§€ê¸° ë•Œë¬¸ì— single sampleì„ ë§Œë“œëŠ”ë°ì—ëŠ” ìˆ˜ì²œ stepì„ ê±°ì³ì•¼í•˜ë©°, GANê³¼ ë¹„êµí•˜ë©´ ì†ë„ê°€ ë§¤ìš° ëŠë¦¬ë‹¤.

DDIMì—ì„œëŠ” diffusion modelì˜ ì†ë„ë¥¼ ë¹ ë¥´ê²Œ í•˜ê²Œ ìœ„í•´ implicit probabilistic modelì„ ì œì•ˆí•˜ì˜€ë‹¤. (DDPMê³¼ objective functionì€ ë™ì¼)

---

## 1. Background

**DDPM**

![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled.png)

![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%201.png)

 

**Simplified Loss function of DDPM**

$$
L_{\gamma}\left(\epsilon_{\theta}\right):=\sum_{t=1}^{T} \gamma_{t} \mathbb{E}_{\boldsymbol{x}_{0} \sim q\left(\boldsymbol{x}_{0}\right), \epsilon_{t} \sim \mathcal{N}(\mathbf{0}, I)}\left[\left\|\epsilon_{\theta}^{(t)}\left(\sqrt{\alpha_{t}} \boldsymbol{x}_{0}+\sqrt{1-\alpha_{t}} \epsilon_{t}\right)-\epsilon_{t}\right\|_{2}^{2}\right]
$$

---

## 2. Variational Inference For Non-Markovian Forward Processes

### 2.1 Non-Markovian Forward Process

DDIMì—ì„œëŠ” DDPMê³¼ ë™ì¼í•˜ê²Œ marginal distributionì„ êµ¬ì„±í•˜ì§€ë§Œ, inference ê³¼ì •ì— ì‚¬ìš©ë˜ëŠ” joint distributionì˜ ê²½ìš°ëŠ” ì•½ê°„ ë‹¤ë¥´ê²Œ distributionì„ êµ¬ì„±í•˜ì˜€ë‹¤.

![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%202.png)

- DDPMì€ $x_t$ ê°€ ë°”ë¡œ ì´ì „ step ê°’ $x_{t-1}$ ì— ì˜í•´ ê²°ì •ë˜ëŠ” `markovian chain`ì´ì§€ë§Œ
- DDIMì€ $x_t$ ê°€ ë°”ë¡œ ì´ì „ step ê°’ $x_{t-1}$ ê³¼ $x_0$ ì— ì˜í•´ ê²°ì •ë˜ëŠ” `non-markovian chain`ì´ë‹¤.
    
    

ë˜í•œ, DDPMì— ë”°ë¥´ë©´ posterior distributionì€ ì•„ë˜ì˜ ë‘ ì‹ì„ ë§Œì¡±í•˜ëŠ”ë°, (ì• ì´ˆì— defineì„ ì´ë ‡ê²Œ í•˜ì˜€ìŒ)

$$
q_{\sigma}\left(\boldsymbol{x}_{T} \mid \boldsymbol{x}_{0}\right)=\mathcal{N}\left(\sqrt{\alpha_{T}} \boldsymbol{x}_{0},\left(1-\alpha_{T}\right) \boldsymbol{I}\right)
$$

$$
q_{\sigma}\left(\boldsymbol{x}_{t} \mid \boldsymbol{x}_{0}\right)=\mathcal{N}\left(\sqrt{\alpha_{t}} \boldsymbol{x}_{0},\left(1-\alpha_{t}\right) \boldsymbol{I}\right)
$$

ì´ ì‹ë“¤ì„ ë§Œì¡±í•˜ë ¤ë©´ reverse conditional distributionì˜ meanê°’ì´ ë‹¤ìŒê³¼ ê°™ì•„ì•¼ ëœë‹¤ê³  í•œë‹¤.

$$
q_{\sigma}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{x}_{0}\right)=\mathcal{N}\left(\sqrt{\alpha_{t-1}} \boldsymbol{x}_{0}+\sqrt{1-\alpha_{t-1}-\sigma_{t}^{2}} \cdot \frac{\boldsymbol{x}_{t}-\sqrt{\alpha_{t}} \boldsymbol{x}_{0}}{\sqrt{1-\alpha_{t}}}, \sigma_{t}^{2} \boldsymbol{I}\right)
$$

- ì´ ì‹ ìœ ë„ê°€ ì•ˆë˜ëŠ”ë°.. ì–´ë–»ê²Œ ë‚˜ì˜¨ ê±´ì§€ ì˜ ëª¨ë¥´ê² ë‹¤ (DDPMì˜ ì‹ì´ë‘ì€ ì™„ì „ ëŒ€ì‘ë˜ì§€ëŠ” ì•ŠëŠ” ë“¯?)
- ì•„ë§ˆ `reverse conditional distribution`ì‹ì„ ìœ„ì™€ ê°™ì´ ì •ì˜í•˜ë©´.. ëŒ€ì¶© unit varianceê°€ 1ë¡œ ìœ ì§€ë˜ë©´ì„œ, ì°¨í›„ì— ì „ê°œë˜ëŠ” ì‹ë“¤ì´ DDPMì˜ ì‹ë“¤ê³¼ ì˜ ëŒ€ì‘ì´ ë¼ì„œ ì´ë ‡ê²Œ ì •ì˜í•œ ê²ƒ ê°™ë‹¤.

**Forward Process**

bayesâ€™ ruleì— ë”°ë¼ forward process ì‹ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìœ¼ë©°, ì´ ì—­ì‹œ gaussian distributionì„ ë”°ë¥¸ë‹¤.

$$
q_{\sigma}\left(\boldsymbol{x}_{t} \mid \boldsymbol{x}_{t-1}, \boldsymbol{x}_{0}\right)=\frac{q_{\sigma}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{x}_{0}\right) q_{\sigma}\left(\boldsymbol{x}_{t} \mid \boldsymbol{x}_{0}\right)}{q_{\sigma}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{0}\right)}
$$

- DDIMì˜ Forward ProcessëŠ” DDPMê³¼ëŠ” ë‹¤ë¥´ê²Œ ë”ì´ìƒ markovianì´ ì•„ë‹ˆë‹¤. â­ï¸
- $\sigma$ : forward processê°€ ì–¼ë§ˆë‚˜ stochastic í•œì§€ë¥¼ ê²°ì •í•˜ë©°, ì´ ê°’ì´ 0ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ deterministic í•´ì§
    - $x_{t-1}$ ê°€ $x_0, x_t$ ì— ì˜í•´ ê²°ì •ë¨

---

### 2.2 Generative Process and Unified Variational Inference Objective

![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%203.png)

**Variational Inference Objective**

![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%204.png)

- ì¤‘ìš”í•œê±´ ì•„ë‹ˆì§€ë§Œ.. ë…¼ë¬¸ ìˆ˜ì‹ (11)ë²ˆì—ì„œ log ê°€ ë¹ ì ¸ìˆëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.. ì˜¤íƒ€ì¸ ë“¯ í•´ìš© ğŸ˜‚
- $\sigma$ ë¥¼ ì–´ë–¤ ê°’ìœ¼ë¡œ ì„ íƒí•˜ëƒì— ë”°ë¼ ë‹¤ë¥¸ ëª¨ë¸ì´ ëœë‹¤.
    - ë§Œì•½ $\sigma=0$ ìœ¼ë¡œ ë‘”ë‹¤ë©´, ì´ í•¨ìˆ˜ëŠ” DDIMì˜ objective functionì´ ë˜ë©°
    - $\sigma=$ ë¡œ ë‘”ë‹¤ë©´, ì´ í•¨ìˆ˜ëŠ” DDPMì˜ objective functionì´ ëœë‹¤.

**DDPMì˜ Variational objective, $L_r$**

$$
L_{\gamma}\left(\epsilon_{\theta}\right):=\sum_{t=1}^{T} \gamma_{t} \mathbb{E}_{\boldsymbol{x}_{0} \sim q\left(\boldsymbol{x}_{0}\right), \epsilon_{t} \sim \mathcal{N}(\mathbf{0}, I)}\left[\left\|\epsilon_{\theta}^{(t)}\left(\sqrt{\alpha_{t}} \boldsymbol{x}_{0}+\sqrt{1-\alpha_{t}} \epsilon_{t}\right)-\epsilon_{t}\right\|_{2}^{2}\right]
$$

- $\epsilon_{\theta}^t$ ëŠ” ì„œë¡œ ë‹¤ë¥¸ tì— ëŒ€í•´ì„œ parameter $\theta$ ë¥¼ ê³µìœ í•˜ì§€ ì•ŠëŠ”ë‹¤. â†’ $\epsilon_{\theta}$ ì€ ì˜¤ì§ weight $\gamma$ ì— ì˜í•´ ìµœì í™”ë¨
- ë§Œì•½  
$\gamma=1$ ì´ë¼ë©´, objective functionì€ DDPMì˜ variational lower boundì™€ ê°™ë‹¤.
    - $L_1 = J_{\sigma}$
- ë˜í•œ Theorem 1 ì— ë”°ë¥´ë©´ $J_{\sigma}$ ì€ $L_{\gamma}$ ì˜ ì¼ì¢…ì¸ë°, $J_{\sigma}$ ëŠ”  
$\gamma=1$ ì¼ë•Œ ($L_1$) ìµœì ì˜ ê°’ì„ ê°€ì§„ë‹¤.

$$
\text { For all } \sigma>\mathbf{0} \text {, there exists } \gamma \in \mathbb{R}_{>0}^{T} \text { and } C \in \mathbb{R} \text {, such that } J_{\sigma}=L_{\gamma}+C \text {. }
$$

---

## 3. Sampling From Generalized Generative Processes

Theorem 1 ì— ë”°ë¥´ë©´,  
$\gamma=1$ ë¡œ ë‘ì—ˆì„ ë•Œ optimal solutionì„ êµ¬í•  ìˆ˜ ìˆì—ˆë‹¤. ë”°ë¼ì„œ $L_1$ì„ objective functionìœ¼ë¡œì¨ ì‚¬ìš©í•œë‹¤ê³  í•´ë³´ì. 

ìš°ë¦¬ëŠ” $\sigma$ ì„ ì–´ë–»ê²Œ ì„¤ì •í•˜ëƒì— ë”°ë¼ì„œ forward processë¥¼ markovian processë¡œ í•™ìŠµì‹œí‚¬ ìˆ˜ë„ ìˆê³ , non-markovian processë¡œ í•™ìŠµì‹œí‚¬ ìˆ˜ë„ ìˆë‹¤. ì´ë•Œ ì£¼ì˜í•  ì ì€  $\sigma$ ë¥¼ ì–´ë–¤ ê°’ìœ¼ë¡œ ë‘ëƒì™€ ìƒê´€ì—†ì´ ìš°ë¦¬ê°€ í•™ìŠµí•´ì•¼í•˜ëŠ” parameterëŠ” $\theta$ ë¼ëŠ” ì ì´ë‹¤.

<aside>
ğŸ’¡ - ì¦‰, markovian processë¡œ í•™ìŠµì‹œí‚¨ pretrained DDPM modelì˜ parameter $\theta$ ë¥¼ DDIMì˜ generative processì—ë„ ì´ìš©í•  ìˆ˜ ìˆê²Œ ëœë‹¤ëŠ” ê²ƒì´ë‹¤.
- ì—¬ê¸°ì„œ ì§šê³  ë„˜ì–´ê°ˆ ê²Œ ìˆëŠ”ë°, DDIMì€ ìƒˆë¡œìš´ í›ˆë ¨ ë°©ë²•ì„ ì œì‹œí–ˆë‹¤ê¸° ë³´ë‹¤ëŠ” diffusion processì˜ objective functionì„ non-markovian chainìœ¼ë¡œ generalizeí•˜ê³ , ì¢€ ë” ë¹ ë¥´ê²Œ ì´ë¯¸ì§€ ìƒì„±ì´ ê°€ëŠ¥í•˜ë„ë¡ ìƒˆë¡œìš´ sampling ë°©ë²•ì„ ì œì‹œí–ˆë‹¤ëŠ” ì ì— ìˆë‹¤. 
- ìš”ì¦˜ íŠ¸ë Œë“œëŠ” DDPMìœ¼ë¡œ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì„ DDIMì˜ generation ë°©ì‹ìœ¼ë¡œ sampling í•˜ëŠ” ë°©ì‹ì´ë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸(DDPM)ì„ ì‚¬ìš©í•˜ë©´ì„œ, ì´ë¯¸ì§€ë¥¼ ë¹ ë¥´ê²Œ samplingí•  ìˆ˜ ìˆë‹¤.

</aside>

### 3.1 Denoising Diffusion Implicit Models

![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%205.png)

### 3.2 Accelerated Generation Process

generative processëŠ” reverse process ì™€ ë§¤ìš° ìœ ì‚¬í•˜ë‹¤. ë”°ë¼ì„œ ë§Œì•½ forward processê°€ T stepì„ ê±°ì¹œë‹¤ë©´ ì¼ë°˜ì ìœ¼ë¡œ generative processë„ T stepì„ ê°€ì§€ê³¤ í•˜ëŠ”ë°, DDIMì—ì„œëŠ” ì´ë¥¼ ë¹ ë¥´ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.

$q_{\sigma}\left(\boldsymbol{x}_{t} \mid \boldsymbol{x}_{0}\right)$ ì´ fix ë˜ì–´ ìˆëŠ” í•œ, denoising objective $L_1$ ì€ íŠ¹ë³„í•œ forward procedureì— ì˜ì¡´í•˜ì§€ ì•Šìœ¼ë©°, forward processë„ T ë³´ë‹¤ ì§§ì€ stepìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— generative processë„ ë” ê°€ì†í™”ê°€ ê°€ëŠ¥í•˜ë‹¤.

![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%206.png)

**Appendix C.1**

![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%207.png)

![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%208.png)

ìœ„ì™€ ê°™ì´ samplingì„ í•˜ë©´ ë¹ ë¥´ê²Œ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆì§€ë§Œ, forward stepì—ì„œ ì„ì˜ì˜ stepì— ëŒ€í•´ì„œë§Œ ëª¨ë¸ì„ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì—, generative processì—ì„œë„ ì¼ë¶€ë§Œ samplingí•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ë” ë§ì€ stepì— ëŒ€í•´ í•™ìŠµì„ í•  í•„ìš”ê°€ ìˆë‹¤.

- `DDIM`ì˜ ì´ ë°©ì‹ì²˜ëŸ¼ ì„ì˜ì˜ stepì˜ forward stepì—ì„œë§Œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒë³´ë‹¤ `DDPM`ì²˜ëŸ¼ ìˆ˜ë§ì€ stepì— ëŒ€í•´ì„œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ”ê²Œ ë” íš¨ê³¼ì ì´ë‹¤.
- ê·¸ë˜ì„œ ìš”ì¦˜ `DDPM`ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ê³  `DDIM`ìœ¼ë¡œ samplingí•˜ëŠ” ë“¯?

### 3.3 Relevance to Neural ODEs

![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%209.png)

## 4. Experiments

- DDIMì€ DDPMë³´ë‹¤ í›¨ì”¬ ë” ì ì€ iterationìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„±ì´ ê°€ëŠ¥í•˜ë‹¤ (10~100ë°° ë” ë¹ ë¦„)
- DDPMê³¼ ë‹¤ë¥´ê²Œ í•œë²ˆ initial latent variables $x_T$ ê°€ fix ë˜ë©´, generation trajectoryì™€ ìƒê´€ì—†ì´ í•­ìƒ high-levelì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìœ¼ë©° latent space ìƒì—ì„œì˜ interpolationë„ ê°€ëŠ¥í•˜ë‹¤
- latent codeì—ì„œì˜ ì´ë¯¸ì§€ reconë„ ê°€ëŠ¥ (DDPMì€ stochastic sampling processë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í–ˆê¸° ë•Œë¬¸ì— ë¶ˆê°€ëŠ¥í–ˆìŒ)

![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%2010.png)

### 4.1 Sample Quality and Efficiency

![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%2011.png)

- $\eta=0.0$ ì´ë©´ `DDIM`, $\eta=1.0$ ì´ë©´ `DDPM`
    - 
- $\operatorname{dim}(\tau)$ ì„ í‚¤ìš¸ìˆ˜ë¡ sample qualityê°€ ì¢‹ì•„ì§: sample qualityì™€ computational costsëŠ” trande-off ê´€ê³„
- DDIMì´ DDPMë³´ë‹¤ í›¨ì”¬ ê²°ê³¼ê°€ ì¢‹ìŒ

![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%2012.png)

- ê¸°ì¡´ì— DDPMìœ¼ë¡œëŠ” 1000 step ì •ë„ ê±°ì³ì•¼ ì–»ì„ ìˆ˜ ìˆì—ˆë˜ ê²°ê³¼ë¥¼ DDIMì—ì„œëŠ” 20~100 step ì•ˆì— ì–»ì„ ìˆ˜ ìˆê²Œë¨
    - DDPM ëŒ€ë¹„ 10~50ë°° ë¹¨ë¼ì§

### 4.2 Sample Consistency in DDIMs

DDIMì€ generative processê°€ deterministicí•˜ë‹¤. $x_0$ ëŠ” ì˜¤ì§ initial state $x_T$ ì— ì˜ì¡´

![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%2013.png)

ë” ë§ì€ time stepì„ ë°Ÿì„ìˆ˜ë¡(sample trajetoriesë¥¼ ê¸¸ê²Œ ì¡ì„ìˆ˜ë¡) high-qualityì˜ ì •ë³´ê°€ ìƒì„±ë˜ì§€ë§Œ, ë³¸ì§ˆì ìœ¼ë¡œ ê°™ì€ $x_T$ ë¥¼ ì‚¬ìš©í–ˆë‹¤ë©´ sampleì˜ ê²°ê³¼ëŠ” ë™ì¼í•˜ë‹¤.

- ì¦‰, $x_T$ ê°€ imageì˜ informative latent encoding ì—­í• ì„ í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.

### 4.3 Interpolation in Deterministic Generative Processes

![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%2014.png)

- high-level featureëŠ” $x_T$ ì— ì˜í•´ encoding ëœë‹¤. ë”°ë¼ì„œ ë§ˆì¹˜ GAN ì²˜ëŸ¼ latent variableì¸ $x_T$ ë¥¼ interpolationí•˜ë©´ ì´ë¯¸ì§€ ì—­ì‹œ interpolationì´ ê°€ëŠ¥í•´ì§„ë‹¤.
- DDPMì—ì„œëŠ” ì´ê²ƒì´ ë¶ˆê°€ëŠ¥

### 4.4 Reconstruction From Latent Spcae

DDIMì—ì„œëŠ” ODEë¡œ Euler intergrationì„ í•˜ê¸° ë•Œë¬¸ì— ì´ë¯¸ì§€ $x_0$ ì„ $x_T$ ë¡œ encodingí•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤. ì´í›„ ë‹¤ì‹œ $x_0$ ìœ¼ë¡œë„ reconstruction í• ìˆ˜ë„ ìˆë‹¤.

![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%2015.png)

- Sê°€ ì»¤ì§ˆìˆ˜ë¡ reconstructionì´ ì˜ë¨

---

ì •ë¦¬

![                                                                 Denoising Diffusion Implicit Models](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%2016.png)

                                                                 Denoising Diffusion Implicit Models

- **Vs DDPM**
    - non-markovianìœ¼ë¡œ ê°€ì •
        - determinstic íŠ¹ì§•ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•´, random noiseë¥¼ ì—†ì• ë²„ë¦¼ 
        â†’ reverse processë¥¼ ì§„í–‰í• ë•Œ, noiseë¥¼ ì œê±°í•˜ê³ , ë‹¤ì‹œ ì¡°ê¸ˆ ë”í•´ì£¼ëŠ”ë°, `ë”í•´ì£¼ëŠ” ê³¼ì •ì„ ì—†ì•°`
            
            â†’ ê·¸ë˜ì„œ ìŠ¤í…ì— ë”°ë¼, ì´ë¯¸ì§€ê°€ ë‹¬ë¼ì§€ì§€ ì•ŠìŒ
            
        - depend on $x_{t-1}$ and $\mathbf{x}_0$
            - $\mathbf{x}_2$ë¥¼ ì˜ˆì¸¡í•˜ëŠ”ë°, $\mathbf{x}_1$ë¿ë§Œ ì•„ë‹ˆë¼ $\mathbf{x}_0$ë„ í•„ìš”í•¨.
    - ë”°ë¼ì„œ, forwardí• ë•Œ $\mathbf{x}_0$ë¥¼ predictí•  ìˆ˜ ìˆì–´ì•¼í•¨.
    - í‘œê¸° ì°¨ì´
        - DDIM $\alpha$ = DDPM $\bar{\alpha}$
- **Forward process $`q$ (image â†’ noise)`**
    - $q_\sigma(\mathbf{x}_t \mid \mathbf{x}_0)=\mathcal{N}(\sqrt{\alpha}\mathbf{x}_0, \sqrt{1-\alpha_t}I) \, \text{for all t}$
    - $\mathbf{x}_t=\sqrt{\alpha}_t\mathbf{x}_0+\sqrt{1-\alpha_t}\epsilon$
        - $\epsilon-\epsilon_{\theta}(\mathbf{x}_t)$ = $\epsilon-\epsilon_{\theta}(\sqrt{\alpha}_t\mathbf{x}_0+\sqrt{1-\alpha_t}\epsilon)$
- **Reverse process $`p$ (noise â†’ image)`**
    - $q_\sigma(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)=\mathcal{N}(\sqrt{\alpha_{t-1}}\mathbf{x}_0+\sqrt{1-\alpha_{t-1}-\sigma^2_t}\cdot\underbrace{\frac{\mathbf{x}_t-\sqrt{\alpha_t}\mathbf{x}_0}{\sqrt{1-\alpha_t}}}_{\epsilon_\theta(\mathbf{x}_t)}, \, \sigma^2_tI)$
    - $\mathbf{x}_{t-1}=\sqrt{\alpha_{t-1}}\underbrace{\left(\frac{\mathbf{x}_t-\sqrt{1-\alpha_t}\epsilon_\theta(\mathbf{x}_t)}{\sqrt{\alpha_t}}\right)}_{\text{predicted} \, \mathbf{x}_0=f_\theta(\mathbf{x}_t)}+\underbrace{\sqrt{1-\alpha_{t-1}-\sigma^2_t}\cdot\epsilon_\theta(\mathbf{x}_t)}_{\text{direction pointing to} \, \mathbf{x}_t}+\underbrace{\sigma_t\epsilon}_{\text{noise}}$
        - deterministic when $\sigma_t = 0$ â†’ consistency
    - **General form**
        
        ![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%2017.png)
        
        ![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%2018.png)
        
        - $**\eta=1$  â†’ DDPM (stochastic)**
        - $**\eta=0$ â†’ DDIM (determinstic)**
        - **Consistency results**
            
            ![Untitled](DDIM%2068634f4ac6c340619ef3756553c660e9/Untitled%2019.png)
            
- **Summary**
    - **DDIM $\alpha$ = DDPM $\bar{\alpha}$**
    - $\mathbf{x}_t=\sqrt{\alpha}_t\mathbf{x}_0+\sqrt{1-\alpha_t}\epsilon$ **`(Forward)`**
        - $\epsilon-\epsilon_{\theta}(\mathbf{x}_t)$ = $\epsilon-\epsilon_{\theta}(\sqrt{\alpha}_t\mathbf{x}_0+\sqrt{1-\alpha_t}\epsilon)$ **`(Loss)`**
            - $\epsilon_\theta$ = prediction network
    - $\mathbf{x}_{t-1}=\sqrt{\alpha_{t-1}}\underbrace{\left(\frac{\mathbf{x}_t-\sqrt{1-\alpha_t}\epsilon_\theta(\mathbf{x}_t)}{\sqrt{\alpha_t}}\right)}_{\text{predicted} \, \mathbf{x}_0=f_\theta(\mathbf{x}_t)}+\underbrace{\sqrt{1-\alpha_{t-1}-\sigma^2_t}\cdot\epsilon_\theta(\mathbf{x}_t)}_{\text{direction pointing to} \, \mathbf{x}_t}+\underbrace{\sigma_t\epsilon}_{\text{noise}}$ **`(Reverse)`**
        - deterministic when $\sigma_t = 0$ â†’ consistency