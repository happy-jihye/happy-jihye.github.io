{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26nADTPFDL6F"
   },
   "source": [
    "# 1 - Simple Sentiment Analysis\n",
    "\n",
    "- Pytorch / TorchText\n",
    "- RNN network를 사용한 간단한 Sentiment Analysis 예제\n",
    "> - 2021/03/12 Happy-jihye\n",
    "> - **Reference** : [pytorch-sentiment-analysis/1 - Simple Sentiment Analysis](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0jFZX0dEmMB"
   },
   "source": [
    "## 1. Preparing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFd2lRS-G89C"
   },
   "source": [
    "#### 1) Text/Label\n",
    "- **[spaCy](https://spacy.io/)** : nlp를 쉽게 할 수 있도록 도와주는 python package로, tokenizaing, parsing, pos tagging 등을 지원합니다.\n",
    "- **[Field](https://pytorch.org/text/_modules/torchtext/data/field.html)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Lj415BkW090"
   },
   "outputs": [],
   "source": [
    "!apt install python3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0cr1n7oE-pM"
   },
   "outputs": [],
   "source": [
    "!pip install -U torchtext==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cArSLZmKEydH"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cn9QsVChE48-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy',\n",
    "                  tokenizer_language = 'en')\n",
    "LABEL = data.LabelField(dtype = torch.float) # pos -> 1 / neg -> 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_R9bRwoyIia9"
   },
   "source": [
    "#### 2) IMDb Dataset\n",
    "- 5만개의 영화 리뷰로 구성된 dataset\n",
    "- IMDb dataset을 다운로드 받은 후, 이전에 정의한 Field(TEXT, LABEL)를 사용해서 데이터를 처리하였습니다.\n",
    "- torchtext.datasets의 [IMDB](https://pytorch.org/text/stable/datasets.html#imdb) 객체로 train data와 test data을 분할하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PadXXIFIKXg_"
   },
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXkUEhDjK-zn"
   },
   "outputs": [],
   "source": [
    "print(f'training examples 수 : {len(train_data)}') #25,000\n",
    "print(f'testing examples 수 : {len(test_data)}') #25,000\n",
    "\n",
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YULs86qbL4L9"
   },
   "source": [
    "- IMDb dataset은 train/test data만 있고, validation set이 없으므로 train dataset을 split해서 validation dataset을 만들어주었습니다.\n",
    "  - 이때, split 함수의 default split_ratio = 0.7 이므로 7:3의 비율로 각각의 데이터들이 나눠집니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9-dgz-PMQp6"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gK61YhDIMgPm",
    "outputId": "f597958d-d2c7-46ec-b9a0-dadf93578b27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training examples 수 : 17500\n",
      "validations examples 수 : 7500\n",
      "testing examples 수 : 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'training examples 수 : {len(train_data)}')\n",
    "print(f'validations examples 수 : {len(valid_data)}')\n",
    "print(f'testing examples 수 : {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMQDknQkNKGY"
   },
   "source": [
    "#### 3) Build Vocabulary\n",
    "- one-hot encoding 방식을 사용해서 단어들을 indexing 합니다.\n",
    "![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/Simple_RNN_model1.png?raw=1)\n",
    "\n",
    "- training dataset에 있는 단어들은 10만개가 넘는데, 이 모든 단어들에 대해 indexing을 하면 one-hot vector의 dimension이 10만개가 되므로 연산하기 좋지 않습니다.\n",
    "  - 따라서 어휘의 수를 MAX_VOCAB_SIZE로 제한하였고,이 예제에서는 **25,000 words**를 사용하였습니다.\n",
    "  - \"This film is great and I love it\" 라는 문장에서 \"love\"라는 단어가 vocabulary에 없다면, \"This film is great and I $<unk>$ it\"로 문장을 학습시키게 됩니다.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8r09NpIPPXH"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE, min_freq = 5)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ty-HvZnzR2MB"
   },
   "source": [
    "- vocab size가 25,000개가 아닌 25,002개인 이유는 $<unk>$ token과 $<pad>$ token이 추가되었기 때문입니다.\n",
    "- $<pad>$ token : 문장의 길이를 맞추기 위해 있는 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SnRLbdsMRKLX",
    "outputId": "6f5dcca4-6c97-4acf-ee72-8bad661f8f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMCUWH1YSHnv",
    "outputId": "e922bb76-84f4-4f25-a5a4-24e71ad5e7da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 자주 나오는 단어들 20개 출력 :\n",
      "[('the', 204412), (',', 192936), ('.', 166941), ('a', 110304), ('and', 109590), ('of', 101675), ('to', 94170), ('is', 76946), ('in', 61671), ('I', 54581), ('it', 53843), ('that', 49317), ('\"', 44555), (\"'s\", 43644), ('this', 42548), ('-', 37200), ('/><br', 35695), ('was', 35052), ('as', 30433), ('with', 30218)]\n",
      "\n",
      "['<unk>', '<pad>', 'the', ',', '.']\n",
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(f\"가장 자주 나오는 단어들 20개 출력 :\\n{TEXT.vocab.freqs.most_common(20)}\\n\")\n",
    "\n",
    "# itos(int to string)\n",
    "print(TEXT.vocab.itos[:5])\n",
    "\n",
    "# stoi(string to int)\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVSBULSUSUjW"
   },
   "source": [
    "#### 4) Iterator\n",
    "\n",
    "- GPU를 사용할 수 있다면 GPU 사용 (colab이라면 런타임 유형을 GPU로 설정하기)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wm0iCra-aEk0",
    "outputId": "c91ec806-f49f-4002-d209-214b1bca220f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0+cu101\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwJS7CKZT6jc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QucJLf94s9-q"
   },
   "source": [
    "- BucketIterator를 사용하여 interators를 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Khgb9NUFYXXI"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_NfL5WwTn7Fp",
    "outputId": "b49d553c-87df-4caf-d638-e3056993f01f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 배치의 text 크기: torch.Size([1411, 64])\n",
      "tensor([   35,    11,  2570,   305,   458,   448,   658,     8, 10220,     3,\n",
      "          494,   127,   303,    83,   277,    22,   541,    80,   390,    80,\n",
      "         3599,   166,  1302,     4,    80,   178,    65,    24,     7,    16,\n",
      "          311,   465,   827,    83,    19,   168,   805,    16,   478,   409,\n",
      "            6,    10,     5,    16,   182,    22,     5,  1804,    15,    21,\n",
      "          490,     8,   832,    22,   264,    65,   137,   173,     7,  7289,\n",
      "          103,     5,    38,    23], device='cuda:0')\n",
      "torch.Size([64])\n",
      "첫 번째 배치의 label 크기: torch.Size([64])\n",
      "tensor([0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 1., 1., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# iterator 출력\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    text = batch.text\n",
    "    label = batch.label\n",
    "\n",
    "    print(f\"첫 번째 배치의 text 크기: {text.shape}\")\n",
    "    print(text[3])\n",
    "    print(text[3].shape)\n",
    "    print(f\"첫 번째 배치의 label 크기: {label.shape}\")\n",
    "    print(label)\n",
    "\n",
    "    # 첫 번째 batch만 출력\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBtU7KsqZ12O"
   },
   "source": [
    "## 2. Build Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB-j7VO8ayts"
   },
   "source": [
    "**Embedding layer**\n",
    "![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/Simple_RNN_model3.png?raw=1)\n",
    "- [Embedding layer](https://wikidocs.net/64779) : input을 dense vector(embedding vector)로 mapping 해주는 일종의 look-up table\n",
    "  - Embedding vector는 인공 신경망의 학습과정에서 가중치가 학습되는 것과 같은 방식으로 훈련됩니다. (역전파 과정에서 embedding vector값이 학습)\n",
    "  - 이 예제에서는 input sentence를 one-hot encoding하는 부분을 찾아보기 어려운데, 이는 pytorch의 성질 때문입니다. pytorch에서는 단어를 정수 index로 바꾸고 one-hot vector로 한번 더 바꾸고 나서 embedding layer의 입력으로 사용하는 것이 아니라, 단어를 정수 index로만 바꾼 채로 embedding layer에 입력합니다. \n",
    "  - [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMiDiSqofOZ0"
   },
   "source": [
    "**RNN Layer**\n",
    "- 이 model은 RNN layer를 사용합니다.\n",
    "- RNN은 문장($X=\\{x_1, ..., x_T\\}$) 속 단어들을 한번에 하나씩 계산하여 각 단어당 *hidden state*(h)를 구합니다.\n",
    "$$h_t = \\text{RNN}(x_t, h_{t-1})$$\n",
    "- 이때 각 단어당 hidden state를 구하기 위해서는 이전 hidden state $h_{t-1}$와 단어의 정보를 가지고 있는 dense vector가 필요합니다.\n",
    "\n",
    "  ![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/Simple_RNN_model2.png?raw=1)\n",
    "\n",
    "- final hidden state인 $h_T$를 linear layer에 통과시킴으로써 prediction 결과를 얻을 수 있습니다. ($\\hat{y} = f(h_T)$)\n",
    "\n",
    "- 이 예제에서는 부정적인 감정을 가지면 0을 예측하도록 RNN을 학습시켰습니다.\n",
    "  ![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/Simple_RNN_model4.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0v8A8X6CbeuJ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "  def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "    self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "  \n",
    "  def forward(self, text):\n",
    "    \n",
    "    # text = [sentence length, batch size]\n",
    "\n",
    "    embedded = self.embedding(text)\n",
    "\n",
    "    # embedded = [sentence length, batch size, embedding dim]\n",
    "\n",
    "    output, hidden = self.rnn(embedded)\n",
    "\n",
    "    # output = [sentence length, batch size, hidden dim]\n",
    "    # hidden = [1, batch size, hidden dim]\n",
    "\n",
    "    return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ui0Y4b0g1ax"
   },
   "source": [
    "- **Input dim** : one-hot vector의 dimension과 같음(vocabulary size)\n",
    "- **Embedding dim** : 보통 50-250 dimensions\n",
    "- **Hidden dim** :보통 100-500 dim\n",
    "- **Output dim** : class의 수, 위 예제에서는 0아니면 1이므로 1-dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XN_0DpCqgQNx"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab) #25,002\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6uSeSNhhdju",
    "outputId": "f84a2542-5320-4780-88b9-e0b44769e77c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,592,105 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fy-6lg_HhfSb"
   },
   "source": [
    "## 3. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtXPbSXchiIy"
   },
   "source": [
    "#### optimizer\n",
    "- **stochastic gradient descent (SGD)** 를 이용해서 model을 update하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0vLv4drhyXL"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer =optim.SGD(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XxWUCDih_0z"
   },
   "source": [
    "#### loss function\n",
    "- loss function 으로는 **binary cross entropy with logits**을 사용하였습니다.\n",
    "- 0아니면 1의 label을 예측해야하므로 **sigmoid**나 **logit** function을 사용하였습니다.\n",
    "- [BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)는 sigmoid와 the binary cross entropy steps를 모두 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1A9f5ysXikuR"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZosE0TCpis_9"
   },
   "outputs": [],
   "source": [
    "# GPU\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1w4aPDkeixl_"
   },
   "source": [
    "**accuracy function**\n",
    "- sigmoid layer를 지나면 0과 1사이의 값이 나오는데, 우리가 필요한 값은 0,1의 label이므로 [nn.round](https://pytorch.org/docs/stable/generated/torch.round.html)를 이용하여 반올림하였습니다.\n",
    "- prediction 값과 label 값이 같은 것들이 얼마나 있는지를 계산하여 정확도를 측정하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruGAS62rjQoy"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "\n",
    "  rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "  # rounded_preds : [batch size]\n",
    "  # y : batch.label\n",
    "  correct = (rounded_preds == y).float()\n",
    "  acc = correct.sum() / len(correct)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-Y-o_4kjkw2"
   },
   "source": [
    "### 1) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLVdffp-jojk"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "\n",
    "  # model을 \"training mode\"로 -> dropout이나 batch normalization이 가능해짐\n",
    "  # 이 모델에서는 이를 사용하지는 않음\n",
    "  model.train()\n",
    "\n",
    "  for batch in iterator:\n",
    "\n",
    "    # 모든 batch마다 gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # batch of sentences인 batch.text를 model에 입력 (저절로 forward가 됨)\n",
    "    # predictions의 크기가 [batch size, 1]이므로 squeeze해서 [batch size]로 size를 변경해줘야 함 \n",
    "    predictions = model(batch.text).squeeze(1)\n",
    "\n",
    "    # prediction결과와 batch.label을 비교하여 loss값 계산 \n",
    "    loss = criterion(predictions, batch.label)\n",
    "\n",
    "    # 정확도 계산\n",
    "    acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "    # backward()를 사용하여 역전파 수행\n",
    "    loss.backward()\n",
    "\n",
    "    # 최적화 알고리즘을 사용하여 parameter를 update\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "    epoch_acc += acc.item()\n",
    "\n",
    "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7aRqf7Cl9pu"
   },
   "source": [
    "### 2) Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBP2c1Q0mCXp"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "\n",
    "  # \"evaluation mode\" : dropout이나 batch nomalizaation을 끔\n",
    "  model.eval()\n",
    "\n",
    "  # pytorch에서 gradient가 계산되지 않도록 해서 memory를 적게 쓰고 computation 속도를 높임\n",
    "  with torch.no_grad():\n",
    "    \n",
    "    for batch in iterator :\n",
    "      predictions = model(batch.text).squeeze(1)\n",
    "      \n",
    "      loss = criterion(predictions, batch.label)\n",
    "      acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "      epoch_loss += loss.item()\n",
    "      epoch_acc += acc.item()\n",
    "\n",
    "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wt3HQYEp3b2"
   },
   "source": [
    "- epoch 시간을 계산하기 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erTMajygmvuo"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "  elapsed_time = end_time - start_time\n",
    "  elapsed_mins = int(elapsed_time / 60)\n",
    "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "  return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTZ9k2EhvO_6"
   },
   "source": [
    "### Train the model through multiple epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGrnQs4FnHtE",
    "outputId": "6e0c2d63-1ea8-4f79-a350-e90a89986b32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.694 | Train Acc: 49.82%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 50.19%\n",
      "Epoch: 02 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.41%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 50.51%\n",
      "Epoch: 03 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.11%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 50.54%\n",
      "Epoch: 04 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.22%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 50.19%\n",
      "Epoch: 05 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.58%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 50.05%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nfCNhdYznr5l",
    "outputId": "049e791a-dffd-4112-ffcd-72b279b48031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.710 | Test Acc: 46.36%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "1 - Simple Sentiment Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
