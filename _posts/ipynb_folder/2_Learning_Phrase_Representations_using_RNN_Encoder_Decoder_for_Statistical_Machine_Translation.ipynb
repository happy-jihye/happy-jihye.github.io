{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsCHC88ibsh1"
      },
      "source": [
        "# 2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
        "\n",
        "\n",
        "> 2021/03/29 Happy-jihye 🌺\n",
        "> \n",
        "> **Reference** : [pytorch-seq2seq/2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://github.com/bentrevett/pytorch-seq2seq)\n",
        "\n",
        "--- \n",
        "\n",
        "## 0. Introduction\n",
        "\n",
        "- 이번 노트북에서는 [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation(2014)](https://arxiv.org/abs/1406.1078) paper의 모델을 간단하게 구현할 예정입니다.\n",
        "\n",
        "- 이 논문은 두 가지 내용으로 유명합니다. <u>하나는 기계번역 Neural Machine Translation(NMT) 분야에서 널리 쓰이고 있는 Seq2Seq architecture의 제안이고, 두번째는 LSTM의 대안인 Gated Recurrent Unit(GRU)의 도입입니다.</u>\n",
        "  \n",
        "  - 이 논문은 Seq2Seq model을 제시한 논문이지, 이를 NMT 분야에 사용한 논문은 아닙니다. 이 논문에서는 당시 활용되던 Statical Machine Translation(SMT)분야의 한 파트로서 **RNN Encoder-Decoder model**을 제안하였습니다. \n",
        "  - 실제로 이 모델을 NMT 분야에 적용한 논문은 [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)입니다.\n",
        "  - [SMT vs NMT](https://smartlion.co.kr/news-%EC%8B%A0%EA%B2%BD%EA%B8%B0%EA%B3%84%EB%B2%88%EC%97%ADnmt%EC%9D%98%EC%8B%9C%EB%8C%80/)\n",
        "  \n",
        "- Sequence to Sequence Learning with Neural Networks, LSTM 등에 대해 공부하고 싶으시다면 이 글들([Seq2Seq-NMT](https://happy-jihye.github.io/nlp/1_Sequence_to_Sequence_Learning_with_Neural_Networks/)과 [Understanding LSTM Network](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))을 참고하시면 좋을 것 같습니다 :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6cHb1ugpMAa"
      },
      "source": [
        "### RNN Encoder-Decoder\n",
        "\n",
        "![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/seq2seq1.png?raw=1)\n",
        "\n",
        "이번 시간에 배울 모델의 architecture는 간단합니다. \n",
        "\n",
        "**RNN Encoder-Decoder** 은 encoder와 decoder 역할을 하는 2개의 Recurrent Neural Network(RNN)으로 구성되어 있으며, **Encoder**는 가변 길이의 `source sequence`를 고정된 크기의 `context vector`로 만들고 **Decoder**는 이 `context vector`를 다시 가변 길이의 `target sequence`로 변환합니다.\n",
        "\n",
        "\n",
        "context vector는 모든 decoder의 노드들에 관여를 하며, 번역이 문장 단위가 아닌, 단어나 구문 단위로 쪼개서 되기 때문에 이 모델은 통계 기계 번역(Statistical Machine Translation, SMT)를 따른다고 볼 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG8aw82pwUfS"
      },
      "source": [
        "## 1. Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MngZOT9T3jC2"
      },
      "source": [
        "!apt install python3.7\n",
        "!pip install -U torchtext==0.6.0\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKucp-663qub"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL5Yr3e63rDv"
      },
      "source": [
        "### **Tokenizers**\n",
        "- tokenizers는 문장을 개별 token으로 변환해주는 데 사용됩니다.\n",
        "  - e.g. \"good morning!\" becomes [\"good\", \"morning\", \"!\"]\n",
        "- nlp를 쉽게 할 수 있도록 도와주는 python package인 `spaCy`를 이용하여, token화를 할 예정입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI8Csi-13rG4"
      },
      "source": [
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E-XOILd3rMt"
      },
      "source": [
        "def tokenize_de(text):\n",
        "  return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "  return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeGj4lbd55Al"
      },
      "source": [
        "다음으로는 **Field** 라이브러리를 사용하여 데이터를 처리합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjgQFUgu5_6C"
      },
      "source": [
        "SRC = Field(tokenize= tokenize_de,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize= tokenize_en,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYlgumKSZkq8"
      },
      "source": [
        "- dataset으로는 [Multi30k dataset](https://github.com/multi30k/dataset)을 사용하였습니다. 이는 약 3만개의 영어, 독일어, 프랑스어 문장이 있는 데이터이며 각 문장 당 12개의 단어가 있습니다.\n",
        "- `exts`는 source와 target으로 사용할 언어를 지정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jnnu07gTZgQz",
        "outputId": "f65e6532-7e0f-48ab-a251-4df74f4030d8"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts= ('.de', '.en'),\n",
        "                                                    fields = (SRC, TRG))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:01<00:00, 705kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 174kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 159kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DezyyLzCaOXs",
        "outputId": "b06fe511-beba-46bc-ce96-6363853e08e4"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDcZx_nNaYas"
      },
      "source": [
        "- data를 출력해본 결과, source문장은 역순으로 저장되어있음을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrjrpOA4aWVY",
        "outputId": "5b47f37b-8317-4dad-a047-2d9ec41b46c2"
      },
      "source": [
        "print(len(vars(train_data.examples[0])['src']))\n",
        "print(len(vars(train_data.examples[1])['src']))\n",
        "\n",
        "print(vars(train_data.examples[0]))\n",
        "print(vars(train_data.examples[1]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13\n",
            "8\n",
            "{'src': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n",
            "{'src': ['mehrere', 'männer', 'mit', 'schutzhelmen', 'bedienen', 'ein', 'antriebsradsystem', '.'], 'trg': ['several', 'men', 'in', 'hard', 'hats', 'are', 'operating', 'a', 'giant', 'pulley', 'system', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0Vh7OEkal9X"
      },
      "source": [
        "### Build Vocabulary\n",
        "- `build_vocab`함수를 이용하여 각 token을 indexing해줍니다. 이때, source와 target의 vocabulary는 다릅니다.\n",
        "- `min_freq`를 사용하여 최소 2번 이상 나오는 단어들만 vocabulary에 넣어주었습니다. 이때, 한번만 나오는 단어는 `<unk>` token으로 변환됩니다.\n",
        "- 이때, vocabulary는 **training set**에서만 만들어져야합니다. *(validation/test set에 대해서는 만들어지면 안됨!!)* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKCsid6tbl_x"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJB1I4Rmbs5h",
        "outputId": "74a21b01-e6dc-4dcd-82c2-a31787231802"
      },
      "source": [
        "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in source (de) vocabulary: 7855\n",
            "Unique tokens in target (en) vocabulary: 5893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH8PWXeRbxz0"
      },
      "source": [
        "### Create the iterators\n",
        "- `BucketIterator`를 이용하여 batch size별로 token들을 묶고, 어휘를 읽을 수 있는 token에서 index로 변환해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmHuEKeygLnl"
      },
      "source": [
        "# for using GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4tWF2FNgTg_"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXjXGCCYG3X8"
      },
      "source": [
        "- 다음은 batch size가 무엇인지에 대해 이해해보기 위해 첫번째 batch를 출력해본 예제입니다. `BucketIterator`를 통해 batch끼리 묶으면 [sequence length, batch size]라는 tensor가 생성되며, 이 tensor는 train_data를 batch_size로 나눈 결과값만큼 생성됩니다.\n",
        "  - 이 예제에서는 128의 크기를 가진 batch가 총 227개 생깁니다.\n",
        "- 또한, batch에서 `sequence length`는 그 batch 내의 가장 긴 문장의 길이로 결정되며 그보다 짧은 문장들에 대해서는 `<pad>` token으로 남은 tensor값이 채워집니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNhZ55rHJgd_",
        "outputId": "6cc372bd-8700-4bf3-bc30-da9b1d805fcd"
      },
      "source": [
        "print(TRG.vocab.stoi[TRG.pad_token]) #<pad> token의 index = 1\n",
        "\n",
        "for i, batch in enumerate(train_iterator):\n",
        "    src = batch.src\n",
        "    trg = batch.trg\n",
        "\n",
        "    src = src.transpose(1,0)\n",
        "    print(f\"첫 번째 배치의 text 크기: {src.shape}\")\n",
        "    print(src[0])\n",
        "    print(src[1])\n",
        "\n",
        "    break\n",
        "\n",
        "print(len(train_iterator))\n",
        "print(len(train_iterator)*128)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "첫 번째 배치의 text 크기: torch.Size([128, 33])\n",
            "tensor([   2,    8,   67,  217,   12,   33,  214,    9,   35,   17,  101,   17,\n",
            "         998,   20, 1787,   93,    4,    3,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1], device='cuda:0')\n",
            "tensor([  2,  43,  41,  57, 215,   9,  14,   7, 555,   9,  18, 101,   7, 234,\n",
            "          9,  22, 354,  14, 337, 119,  69,   4,   3,   1,   1,   1,   1,   1,\n",
            "          1,   1,   1,   1,   1], device='cuda:0')\n",
            "227\n",
            "29056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMO69_r3geyB"
      },
      "source": [
        "## Building the Seq2Seq Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROkRbTVngkFH"
      },
      "source": [
        "### Encoder\n",
        "- Encoder는 1개의 GRU layer로 구성되어 있습니다. LSTM과는 달리 GRU에서는 각 dropout이 RNN의 각 layer간에 사용되기 때문에 dropout을 GRU의 인수로 주지 않아도 됩니다.\n",
        "\n",
        "- 또한, <u>GRU는 LSTM과 달리 cell state를 RNN network의 입출력으로 사용하지 않습니다.</u>\n",
        "\n",
        "  $h_t = \\text{GRU}(e(x_t), h_{t-1})\\\\\n",
        "(h_t, c_t) = \\text{LSTM}(e(x_t), h_{t-1}, c_{t-1})\\\\\n",
        "h_t = \\text{RNN}(e(x_t), h_{t-1})$\n",
        "\n",
        "- Encoder의 최종식을 표현하면 다음과 같습니다. \n",
        "\n",
        "  $(h_t) = \\text{EncoderGRU}^1(e(x_t), h_{t-1})$\n",
        "\n",
        "- 마지막 RNN을 거치고 나면, context vector인 $z=h_T$를 얻게 됩니다.\n",
        "\n",
        "![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/seq2seq7.png?raw=1)\n",
        "\n",
        "\n",
        "-  GRU는 LSTM과 비슷한 성능을 내지만, 메모리를 보다 효율적으로 사용할 수 있는 모듈로 현재에도 LSTM의 대용으로 많이 사용되고 있습니다 :) GRU의 아키텍처에 대해서는 [이 글](https://blog.floydhub.com/gru-with-pytorch/)을 참고하세요 :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl32aWGThzhY"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hid_dim = hid_dim\n",
        "\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "    self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src):\n",
        "\n",
        "    # src = [src len, batch size]\n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "    # embedded = [src len, batch size, emb dim]\n",
        "\n",
        "    ## cell state가 없습니다 !\n",
        "    outputs, hidden = self.rnn(embedded)\n",
        "\n",
        "    # outputs = [src len, batch size, hid dim * n directions]\n",
        "    # hidden = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "    ## output은 언제나 hidden layer의 top에 있습니다.\n",
        "\n",
        "    return hidden"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RItpYm8jzzj"
      },
      "source": [
        "### Decoder\n",
        "- decoder도 encoder와 유사하지만, 한가지 다른 점은 모든 네트워크에 `context vector`가 관여한다는 점입니다.\n",
        "- GRU에 embedding vector뿐만 아니라 context vector도 입력으로 들어가기 때문에, GRU의 input dimension은 `emb_dim + hid_dim`가 됩니다.\n",
        "- 또한 최종 output의 입력에는 context vector, hidden state, embedding vector가 관여하므로 dimension이 `emb_dim + hid_dim * 2`입니다.\n",
        "\n",
        "  ![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/seq2seq8.png?raw=1)\n",
        "\n",
        "- 다음은 Decoder의 layer를 수식으로 나타낸 식입니다.\n",
        "\n",
        "  $s_t = \\text{DecoderGRU}(d(y_t), s_{t-1}, z))$\n",
        "\n",
        "  $\\hat{y}_{t+1} = f(d(y_t), s_t, z)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_ZanUngHECa"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        # input : context vec + embedding vec\n",
        "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, input, hidden, context):\n",
        "        \n",
        "        # input = [batch size]\n",
        "        ## 한번에 하나의 token만 decoding하므로 forward에서의 input token의 길이는 1입니다.\n",
        "        \n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "        # cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n layers and n directions in the decoder will both always be 1, therefore:\n",
        "        # hidden = [1, batch size, hid dim]\n",
        "        # context = [1, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        # input을 0차원에 대해 unsqueeze해서 1의 sentence length dimension을 추가합니다.\n",
        "        # input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        # embedding layer를 통과한 후에 dropout을 합니다.\n",
        "        # embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        emb_con = torch.cat((embedded, context), dim = 2)\n",
        "        \n",
        "        # emb_con = [1, batch size, emb dim + hid dim]\n",
        "\n",
        "        output, hidden = self.rnn(emb_con, hidden)\n",
        "\n",
        "        # output = [seq len, batch size, hid dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        # seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        # output = [1, batch size, hid dim]\n",
        "        # hidden = [1, batch size, hid dim]\n",
        "\n",
        "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim = 1)\n",
        "        \n",
        "        # output = [batch size, emb dim + hid dim * 2]\n",
        "\n",
        "        prediction = self.fc_out(output)\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfGRH6zVLvk6"
      },
      "source": [
        "## Seq2Seq\n",
        "\n",
        "seq2seq model을 정리하면 다음과 같습니다.\n",
        "- encoder에 source(input) sentence를 입력한다.\n",
        "- encoder를 학습시켜 고정된 크기의 context vector를 출력한다.\n",
        "- context vector를 decoder에 넣어 예측된 target(output) sentence를 생성한다.\n",
        "\n",
        "  ![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/seq2seq9.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybFYGRt5I2hB"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # output을 저장할 tensor를 만듭니다.(처음에는 전부 0으로)\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # src문장을 encoder에 넣은 후 context vector를 구합니다.\n",
        "        context = self.encoder(src)\n",
        "        \n",
        "        # decoder의 initial hidden state는 context vector입니다.\n",
        "        hidden = context\n",
        "\n",
        "        # decoder에 입력할 첫번째 input입니다.\n",
        "        # 첫번째 input은 모두 <sos> token입니다.\n",
        "        # trg[0,:].shape = BATCH_SIZE \n",
        "        input = trg[0,:]  \n",
        "        \n",
        "        \n",
        "        '''한번에 batch_size만큼의 token들을 독립적으로 계산\n",
        "        즉, 총 trg_len번의 for문이 돌아가며 이 for문이 다 돌아가야지만 하나의 문장이 decoding됨\n",
        "        또한, 1번의 for문당 128개의 문장의 각 token들이 다같이 decoding되는 것'''\n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            # input token embedding과 이전 hidden state와 context state를 decoder에 입력합니다.\n",
        "            # 새로운 hidden state와 예측 output값이 출력됩니다.\n",
        "            output, hidden = self.decoder(input, hidden, context)\n",
        "\n",
        "            #output = [batch size, output dim]\n",
        "\n",
        "            # 각각의 출력값을 outputs tensor에 저장합니다.\n",
        "            outputs[t] = output\n",
        "            \n",
        "            # decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            # predictions들 중에 가장 잘 예측된 token을 top에 넣습니다.\n",
        "            # 1차원 중 가장 큰 값만을 top1에 저장하므로 1차원은 사라집니다.\n",
        "            top1 = output.argmax(1) \n",
        "            # top1 = [batch size]\n",
        "            \n",
        "            # teacher forcing기법을 사용한다면, 다음 input으로 target을 입력하고\n",
        "            # 아니라면 이전 state의 예측된 출력값을 다음 input으로 사용합니다.\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4WJZu-iCoCw"
      },
      "source": [
        "\n",
        "## Training the Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jB9OpTrI3L-"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "739iwlMiCv-Z"
      },
      "source": [
        "- 초기 가중치값은 $\\mathcal{N}(0, 0.01)$의 정규분포로부터 얻었습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-58y-hbI_aS",
        "outputId": "29227a2f-4531-4ad1-b4e0-86940ca96830"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean = 0, std = 0.01)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7855, 256)\n",
              "    (rnn): GRU(256, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(5893, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=5893, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ypW8ITHI5HC",
        "outputId": "9ed8dac6-47bc-4480-e5c8-b48450cc2268"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 14,220,293 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kx4GJX-DGMO"
      },
      "source": [
        "- optimizer함수로는 `Adam`을 사용하였고, loss function으로는 `CrossEntropyLoss`를 사용하였습니다. 또한, `<pad>` token에 대해서는 loss 계산을 하지 않도록 조건을 부여했습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvnbqQPTMJ1K"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QZl4LTYMK6m"
      },
      "source": [
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1sBTQvvDdPy"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itnpa_9XMK9Z"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doH3lCKcDgA5"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efcpaqOtMK_V"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAVjT1R-MLBz"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIEaj7GuDpNG"
      },
      "source": [
        "### Train the model through multiple epochsPermalink"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3gyumXIMT9S",
        "outputId": "ba22601d-a15f-4467-b4fe-0b72fa3d1356"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 36s\n",
            "\tTrain Loss: 5.041 | Train PPL: 154.550\n",
            "\t Val. Loss: 5.141 |  Val. PPL: 170.908\n",
            "Epoch: 02 | Time: 0m 36s\n",
            "\tTrain Loss: 4.377 | Train PPL:  79.604\n",
            "\t Val. Loss: 5.104 |  Val. PPL: 164.637\n",
            "Epoch: 03 | Time: 0m 36s\n",
            "\tTrain Loss: 4.060 | Train PPL:  58.001\n",
            "\t Val. Loss: 4.731 |  Val. PPL: 113.397\n",
            "Epoch: 04 | Time: 0m 37s\n",
            "\tTrain Loss: 3.766 | Train PPL:  43.194\n",
            "\t Val. Loss: 4.479 |  Val. PPL:  88.112\n",
            "Epoch: 05 | Time: 0m 36s\n",
            "\tTrain Loss: 3.473 | Train PPL:  32.222\n",
            "\t Val. Loss: 4.165 |  Val. PPL:  64.397\n",
            "Epoch: 06 | Time: 0m 36s\n",
            "\tTrain Loss: 3.213 | Train PPL:  24.857\n",
            "\t Val. Loss: 3.995 |  Val. PPL:  54.303\n",
            "Epoch: 07 | Time: 0m 37s\n",
            "\tTrain Loss: 2.993 | Train PPL:  19.937\n",
            "\t Val. Loss: 3.856 |  Val. PPL:  47.268\n",
            "Epoch: 08 | Time: 0m 37s\n",
            "\tTrain Loss: 2.726 | Train PPL:  15.267\n",
            "\t Val. Loss: 3.880 |  Val. PPL:  48.448\n",
            "Epoch: 09 | Time: 0m 37s\n",
            "\tTrain Loss: 2.543 | Train PPL:  12.714\n",
            "\t Val. Loss: 3.810 |  Val. PPL:  45.146\n",
            "Epoch: 10 | Time: 0m 36s\n",
            "\tTrain Loss: 2.352 | Train PPL:  10.511\n",
            "\t Val. Loss: 3.768 |  Val. PPL:  43.309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yzt3w5UbMT_s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aafcb525-fd99-47c5-b0ce-6b21a1f1887f"
      },
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.703 | Test PPL:  40.569 |\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}