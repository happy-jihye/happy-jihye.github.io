{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsCHC88ibsh1"
      },
      "source": [
        "# 2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
        "\n",
        "\n",
        "> 2021/03/29 Happy-jihye ğŸŒº\n",
        "> \n",
        "> **Reference** : [pytorch-seq2seq/2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://github.com/bentrevett/pytorch-seq2seq)\n",
        "\n",
        "--- \n",
        "\n",
        "## 0. Introduction\n",
        "\n",
        "- ì´ë²ˆ ë…¸íŠ¸ë¶ì—ì„œëŠ” [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation(2014)](https://arxiv.org/abs/1406.1078) paperì˜ ëª¨ë¸ì„ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•  ì˜ˆì •ì…ë‹ˆë‹¤.\n",
        "\n",
        "- ì´ ë…¼ë¬¸ì€ ë‘ ê°€ì§€ ë‚´ìš©ìœ¼ë¡œ ìœ ëª…í•©ë‹ˆë‹¤. <u>í•˜ë‚˜ëŠ” ê¸°ê³„ë²ˆì—­ Neural Machine Translation(NMT) ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì“°ì´ê³  ìˆëŠ” Seq2Seq architectureì˜ ì œì•ˆì´ê³ , ë‘ë²ˆì§¸ëŠ” LSTMì˜ ëŒ€ì•ˆì¸ Gated Recurrent Unit(GRU)ì˜ ë„ì…ì…ë‹ˆë‹¤.</u>\n",
        "  \n",
        "  - ì´ ë…¼ë¬¸ì€ Seq2Seq modelì„ ì œì‹œí•œ ë…¼ë¬¸ì´ì§€, ì´ë¥¼ NMT ë¶„ì•¼ì— ì‚¬ìš©í•œ ë…¼ë¬¸ì€ ì•„ë‹™ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ë‹¹ì‹œ í™œìš©ë˜ë˜ Statical Machine Translation(SMT)ë¶„ì•¼ì˜ í•œ íŒŒíŠ¸ë¡œì„œ **RNN Encoder-Decoder model**ì„ ì œì•ˆí•˜ì˜€ìŠµë‹ˆë‹¤. \n",
        "  - ì‹¤ì œë¡œ ì´ ëª¨ë¸ì„ NMT ë¶„ì•¼ì— ì ìš©í•œ ë…¼ë¬¸ì€ [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)ì…ë‹ˆë‹¤.\n",
        "  - [SMT vs NMT](https://smartlion.co.kr/news-%EC%8B%A0%EA%B2%BD%EA%B8%B0%EA%B3%84%EB%B2%88%EC%97%ADnmt%EC%9D%98%EC%8B%9C%EB%8C%80/)\n",
        "  \n",
        "- Sequence to Sequence Learning with Neural Networks, LSTM ë“±ì— ëŒ€í•´ ê³µë¶€í•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ ì´ ê¸€ë“¤([Seq2Seq-NMT](https://happy-jihye.github.io/nlp/1_Sequence_to_Sequence_Learning_with_Neural_Networks/)ê³¼ [Understanding LSTM Network](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))ì„ ì°¸ê³ í•˜ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤ :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6cHb1ugpMAa"
      },
      "source": [
        "### RNN Encoder-Decoder\n",
        "\n",
        "![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/seq2seq1.png?raw=1)\n",
        "\n",
        "ì´ë²ˆ ì‹œê°„ì— ë°°ìš¸ ëª¨ë¸ì˜ architectureëŠ” ê°„ë‹¨í•©ë‹ˆë‹¤. \n",
        "\n",
        "**RNN Encoder-Decoder** ì€ encoderì™€ decoder ì—­í• ì„ í•˜ëŠ” 2ê°œì˜ Recurrent Neural Network(RNN)ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, **Encoder**ëŠ” ê°€ë³€ ê¸¸ì´ì˜ `source sequence`ë¥¼ ê³ ì •ëœ í¬ê¸°ì˜ `context vector`ë¡œ ë§Œë“¤ê³  **Decoder**ëŠ” ì´ `context vector`ë¥¼ ë‹¤ì‹œ ê°€ë³€ ê¸¸ì´ì˜ `target sequence`ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "context vectorëŠ” ëª¨ë“  decoderì˜ ë…¸ë“œë“¤ì— ê´€ì—¬ë¥¼ í•˜ë©°, ë²ˆì—­ì´ ë¬¸ì¥ ë‹¨ìœ„ê°€ ì•„ë‹Œ, ë‹¨ì–´ë‚˜ êµ¬ë¬¸ ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ ë˜ê¸° ë•Œë¬¸ì— ì´ ëª¨ë¸ì€ í†µê³„ ê¸°ê³„ ë²ˆì—­(Statistical Machine Translation, SMT)ë¥¼ ë”°ë¥¸ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG8aw82pwUfS"
      },
      "source": [
        "## 1. Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MngZOT9T3jC2"
      },
      "source": [
        "!apt install python3.7\n",
        "!pip install -U torchtext==0.6.0\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKucp-663qub"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL5Yr3e63rDv"
      },
      "source": [
        "### **Tokenizers**\n",
        "- tokenizersëŠ” ë¬¸ì¥ì„ ê°œë³„ tokenìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "  - e.g. \"good morning!\" becomes [\"good\", \"morning\", \"!\"]\n",
        "- nlpë¥¼ ì‰½ê²Œ í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” python packageì¸ `spaCy`ë¥¼ ì´ìš©í•˜ì—¬, tokení™”ë¥¼ í•  ì˜ˆì •ì…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI8Csi-13rG4"
      },
      "source": [
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E-XOILd3rMt"
      },
      "source": [
        "def tokenize_de(text):\n",
        "  return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "  return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeGj4lbd55Al"
      },
      "source": [
        "ë‹¤ìŒìœ¼ë¡œëŠ” **Field** ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjgQFUgu5_6C"
      },
      "source": [
        "SRC = Field(tokenize= tokenize_de,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize= tokenize_en,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYlgumKSZkq8"
      },
      "source": [
        "- datasetìœ¼ë¡œëŠ” [Multi30k dataset](https://github.com/multi30k/dataset)ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” ì•½ 3ë§Œê°œì˜ ì˜ì–´, ë…ì¼ì–´, í”„ë‘ìŠ¤ì–´ ë¬¸ì¥ì´ ìˆëŠ” ë°ì´í„°ì´ë©° ê° ë¬¸ì¥ ë‹¹ 12ê°œì˜ ë‹¨ì–´ê°€ ìˆìŠµë‹ˆë‹¤.\n",
        "- `exts`ëŠ” sourceì™€ targetìœ¼ë¡œ ì‚¬ìš©í•  ì–¸ì–´ë¥¼ ì§€ì •í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jnnu07gTZgQz",
        "outputId": "f65e6532-7e0f-48ab-a251-4df74f4030d8"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts= ('.de', '.en'),\n",
        "                                                    fields = (SRC, TRG))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.21M/1.21M [00:01<00:00, 705kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46.3k/46.3k [00:00<00:00, 174kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66.2k/66.2k [00:00<00:00, 159kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DezyyLzCaOXs",
        "outputId": "b06fe511-beba-46bc-ce96-6363853e08e4"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDcZx_nNaYas"
      },
      "source": [
        "- dataë¥¼ ì¶œë ¥í•´ë³¸ ê²°ê³¼, sourceë¬¸ì¥ì€ ì—­ìˆœìœ¼ë¡œ ì €ì¥ë˜ì–´ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrjrpOA4aWVY",
        "outputId": "5b47f37b-8317-4dad-a047-2d9ec41b46c2"
      },
      "source": [
        "print(len(vars(train_data.examples[0])['src']))\n",
        "print(len(vars(train_data.examples[1])['src']))\n",
        "\n",
        "print(vars(train_data.examples[0]))\n",
        "print(vars(train_data.examples[1]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13\n",
            "8\n",
            "{'src': ['zwei', 'junge', 'weiÃŸe', 'mÃ¤nner', 'sind', 'im', 'freien', 'in', 'der', 'nÃ¤he', 'vieler', 'bÃ¼sche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n",
            "{'src': ['mehrere', 'mÃ¤nner', 'mit', 'schutzhelmen', 'bedienen', 'ein', 'antriebsradsystem', '.'], 'trg': ['several', 'men', 'in', 'hard', 'hats', 'are', 'operating', 'a', 'giant', 'pulley', 'system', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0Vh7OEkal9X"
      },
      "source": [
        "### Build Vocabulary\n",
        "- `build_vocab`í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ê° tokenì„ indexingí•´ì¤ë‹ˆë‹¤. ì´ë•Œ, sourceì™€ targetì˜ vocabularyëŠ” ë‹¤ë¦…ë‹ˆë‹¤.\n",
        "- `min_freq`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì†Œ 2ë²ˆ ì´ìƒ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ë§Œ vocabularyì— ë„£ì–´ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ë•Œ, í•œë²ˆë§Œ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ëŠ” `<unk>` tokenìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.\n",
        "- ì´ë•Œ, vocabularyëŠ” **training set**ì—ì„œë§Œ ë§Œë“¤ì–´ì ¸ì•¼í•©ë‹ˆë‹¤. *(validation/test setì— ëŒ€í•´ì„œëŠ” ë§Œë“¤ì–´ì§€ë©´ ì•ˆë¨!!)* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKCsid6tbl_x"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJB1I4Rmbs5h",
        "outputId": "74a21b01-e6dc-4dcd-82c2-a31787231802"
      },
      "source": [
        "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in source (de) vocabulary: 7855\n",
            "Unique tokens in target (en) vocabulary: 5893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH8PWXeRbxz0"
      },
      "source": [
        "### Create the iterators\n",
        "- `BucketIterator`ë¥¼ ì´ìš©í•˜ì—¬ batch sizeë³„ë¡œ tokenë“¤ì„ ë¬¶ê³ , ì–´íœ˜ë¥¼ ì½ì„ ìˆ˜ ìˆëŠ” tokenì—ì„œ indexë¡œ ë³€í™˜í•´ì¤ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmHuEKeygLnl"
      },
      "source": [
        "# for using GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4tWF2FNgTg_"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXjXGCCYG3X8"
      },
      "source": [
        "- ë‹¤ìŒì€ batch sizeê°€ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•´ ì´í•´í•´ë³´ê¸° ìœ„í•´ ì²«ë²ˆì§¸ batchë¥¼ ì¶œë ¥í•´ë³¸ ì˜ˆì œì…ë‹ˆë‹¤. `BucketIterator`ë¥¼ í†µí•´ batchë¼ë¦¬ ë¬¶ìœ¼ë©´ [sequence length, batch size]ë¼ëŠ” tensorê°€ ìƒì„±ë˜ë©°, ì´ tensorëŠ” train_dataë¥¼ batch_sizeë¡œ ë‚˜ëˆˆ ê²°ê³¼ê°’ë§Œí¼ ìƒì„±ë©ë‹ˆë‹¤.\n",
        "  - ì´ ì˜ˆì œì—ì„œëŠ” 128ì˜ í¬ê¸°ë¥¼ ê°€ì§„ batchê°€ ì´ 227ê°œ ìƒê¹ë‹ˆë‹¤.\n",
        "- ë˜í•œ, batchì—ì„œ `sequence length`ëŠ” ê·¸ batch ë‚´ì˜ ê°€ì¥ ê¸´ ë¬¸ì¥ì˜ ê¸¸ì´ë¡œ ê²°ì •ë˜ë©° ê·¸ë³´ë‹¤ ì§§ì€ ë¬¸ì¥ë“¤ì— ëŒ€í•´ì„œëŠ” `<pad>` tokenìœ¼ë¡œ ë‚¨ì€ tensorê°’ì´ ì±„ì›Œì§‘ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNhZ55rHJgd_",
        "outputId": "6cc372bd-8700-4bf3-bc30-da9b1d805fcd"
      },
      "source": [
        "print(TRG.vocab.stoi[TRG.pad_token]) #<pad> tokenì˜ index = 1\n",
        "\n",
        "for i, batch in enumerate(train_iterator):\n",
        "    src = batch.src\n",
        "    trg = batch.trg\n",
        "\n",
        "    src = src.transpose(1,0)\n",
        "    print(f\"ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ text í¬ê¸°: {src.shape}\")\n",
        "    print(src[0])\n",
        "    print(src[1])\n",
        "\n",
        "    break\n",
        "\n",
        "print(len(train_iterator))\n",
        "print(len(train_iterator)*128)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ text í¬ê¸°: torch.Size([128, 33])\n",
            "tensor([   2,    8,   67,  217,   12,   33,  214,    9,   35,   17,  101,   17,\n",
            "         998,   20, 1787,   93,    4,    3,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1], device='cuda:0')\n",
            "tensor([  2,  43,  41,  57, 215,   9,  14,   7, 555,   9,  18, 101,   7, 234,\n",
            "          9,  22, 354,  14, 337, 119,  69,   4,   3,   1,   1,   1,   1,   1,\n",
            "          1,   1,   1,   1,   1], device='cuda:0')\n",
            "227\n",
            "29056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMO69_r3geyB"
      },
      "source": [
        "## Building the Seq2Seq Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROkRbTVngkFH"
      },
      "source": [
        "### Encoder\n",
        "- EncoderëŠ” 1ê°œì˜ GRU layerë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. LSTMê³¼ëŠ” ë‹¬ë¦¬ GRUì—ì„œëŠ” ê° dropoutì´ RNNì˜ ê° layerê°„ì— ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì— dropoutì„ GRUì˜ ì¸ìˆ˜ë¡œ ì£¼ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.\n",
        "\n",
        "- ë˜í•œ, <u>GRUëŠ” LSTMê³¼ ë‹¬ë¦¬ cell stateë¥¼ RNN networkì˜ ì…ì¶œë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</u>\n",
        "\n",
        "  $h_t = \\text{GRU}(e(x_t), h_{t-1})\\\\\n",
        "(h_t, c_t) = \\text{LSTM}(e(x_t), h_{t-1}, c_{t-1})\\\\\n",
        "h_t = \\text{RNN}(e(x_t), h_{t-1})$\n",
        "\n",
        "- Encoderì˜ ìµœì¢…ì‹ì„ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \n",
        "\n",
        "  $(h_t) = \\text{EncoderGRU}^1(e(x_t), h_{t-1})$\n",
        "\n",
        "- ë§ˆì§€ë§‰ RNNì„ ê±°ì¹˜ê³  ë‚˜ë©´, context vectorì¸ $z=h_T$ë¥¼ ì–»ê²Œ ë©ë‹ˆë‹¤.\n",
        "\n",
        "![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/seq2seq7.png?raw=1)\n",
        "\n",
        "\n",
        "-  GRUëŠ” LSTMê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë‚´ì§€ë§Œ, ë©”ëª¨ë¦¬ë¥¼ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë“ˆë¡œ í˜„ì¬ì—ë„ LSTMì˜ ëŒ€ìš©ìœ¼ë¡œ ë§ì´ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤ :) GRUì˜ ì•„í‚¤í…ì²˜ì— ëŒ€í•´ì„œëŠ” [ì´ ê¸€](https://blog.floydhub.com/gru-with-pytorch/)ì„ ì°¸ê³ í•˜ì„¸ìš” :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl32aWGThzhY"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hid_dim = hid_dim\n",
        "\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "    self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src):\n",
        "\n",
        "    # src = [src len, batch size]\n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "    # embedded = [src len, batch size, emb dim]\n",
        "\n",
        "    ## cell stateê°€ ì—†ìŠµë‹ˆë‹¤ !\n",
        "    outputs, hidden = self.rnn(embedded)\n",
        "\n",
        "    # outputs = [src len, batch size, hid dim * n directions]\n",
        "    # hidden = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "    ## outputì€ ì–¸ì œë‚˜ hidden layerì˜ topì— ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "    return hidden"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RItpYm8jzzj"
      },
      "source": [
        "### Decoder\n",
        "- decoderë„ encoderì™€ ìœ ì‚¬í•˜ì§€ë§Œ, í•œê°€ì§€ ë‹¤ë¥¸ ì ì€ ëª¨ë“  ë„¤íŠ¸ì›Œí¬ì— `context vector`ê°€ ê´€ì—¬í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤.\n",
        "- GRUì— embedding vectorë¿ë§Œ ì•„ë‹ˆë¼ context vectorë„ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì—, GRUì˜ input dimensionì€ `emb_dim + hid_dim`ê°€ ë©ë‹ˆë‹¤.\n",
        "- ë˜í•œ ìµœì¢… outputì˜ ì…ë ¥ì—ëŠ” context vector, hidden state, embedding vectorê°€ ê´€ì—¬í•˜ë¯€ë¡œ dimensionì´ `emb_dim + hid_dim * 2`ì…ë‹ˆë‹¤.\n",
        "\n",
        "  ![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/seq2seq8.png?raw=1)\n",
        "\n",
        "- ë‹¤ìŒì€ Decoderì˜ layerë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ì‹ì…ë‹ˆë‹¤.\n",
        "\n",
        "  $s_t = \\text{DecoderGRU}(d(y_t), s_{t-1}, z))$\n",
        "\n",
        "  $\\hat{y}_{t+1} = f(d(y_t), s_t, z)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_ZanUngHECa"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        # input : context vec + embedding vec\n",
        "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, input, hidden, context):\n",
        "        \n",
        "        # input = [batch size]\n",
        "        ## í•œë²ˆì— í•˜ë‚˜ì˜ tokenë§Œ decodingí•˜ë¯€ë¡œ forwardì—ì„œì˜ input tokenì˜ ê¸¸ì´ëŠ” 1ì…ë‹ˆë‹¤.\n",
        "        \n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "        # cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n layers and n directions in the decoder will both always be 1, therefore:\n",
        "        # hidden = [1, batch size, hid dim]\n",
        "        # context = [1, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        # inputì„ 0ì°¨ì›ì— ëŒ€í•´ unsqueezeí•´ì„œ 1ì˜ sentence length dimensionì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "        # input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        # embedding layerë¥¼ í†µê³¼í•œ í›„ì— dropoutì„ í•©ë‹ˆë‹¤.\n",
        "        # embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        emb_con = torch.cat((embedded, context), dim = 2)\n",
        "        \n",
        "        # emb_con = [1, batch size, emb dim + hid dim]\n",
        "\n",
        "        output, hidden = self.rnn(emb_con, hidden)\n",
        "\n",
        "        # output = [seq len, batch size, hid dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        # seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        # output = [1, batch size, hid dim]\n",
        "        # hidden = [1, batch size, hid dim]\n",
        "\n",
        "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim = 1)\n",
        "        \n",
        "        # output = [batch size, emb dim + hid dim * 2]\n",
        "\n",
        "        prediction = self.fc_out(output)\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfGRH6zVLvk6"
      },
      "source": [
        "## Seq2Seq\n",
        "\n",
        "seq2seq modelì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "- encoderì— source(input) sentenceë¥¼ ì…ë ¥í•œë‹¤.\n",
        "- encoderë¥¼ í•™ìŠµì‹œì¼œ ê³ ì •ëœ í¬ê¸°ì˜ context vectorë¥¼ ì¶œë ¥í•œë‹¤.\n",
        "- context vectorë¥¼ decoderì— ë„£ì–´ ì˜ˆì¸¡ëœ target(output) sentenceë¥¼ ìƒì„±í•œë‹¤.\n",
        "\n",
        "  ![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/seq2seq9.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybFYGRt5I2hB"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # outputì„ ì €ì¥í•  tensorë¥¼ ë§Œë“­ë‹ˆë‹¤.(ì²˜ìŒì—ëŠ” ì „ë¶€ 0ìœ¼ë¡œ)\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # srcë¬¸ì¥ì„ encoderì— ë„£ì€ í›„ context vectorë¥¼ êµ¬í•©ë‹ˆë‹¤.\n",
        "        context = self.encoder(src)\n",
        "        \n",
        "        # decoderì˜ initial hidden stateëŠ” context vectorì…ë‹ˆë‹¤.\n",
        "        hidden = context\n",
        "\n",
        "        # decoderì— ì…ë ¥í•  ì²«ë²ˆì§¸ inputì…ë‹ˆë‹¤.\n",
        "        # ì²«ë²ˆì§¸ inputì€ ëª¨ë‘ <sos> tokenì…ë‹ˆë‹¤.\n",
        "        # trg[0,:].shape = BATCH_SIZE \n",
        "        input = trg[0,:]  \n",
        "        \n",
        "        \n",
        "        '''í•œë²ˆì— batch_sizeë§Œí¼ì˜ tokenë“¤ì„ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°\n",
        "        ì¦‰, ì´ trg_lenë²ˆì˜ forë¬¸ì´ ëŒì•„ê°€ë©° ì´ forë¬¸ì´ ë‹¤ ëŒì•„ê°€ì•¼ì§€ë§Œ í•˜ë‚˜ì˜ ë¬¸ì¥ì´ decodingë¨\n",
        "        ë˜í•œ, 1ë²ˆì˜ forë¬¸ë‹¹ 128ê°œì˜ ë¬¸ì¥ì˜ ê° tokenë“¤ì´ ë‹¤ê°™ì´ decodingë˜ëŠ” ê²ƒ'''\n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            # input token embeddingê³¼ ì´ì „ hidden stateì™€ context stateë¥¼ decoderì— ì…ë ¥í•©ë‹ˆë‹¤.\n",
        "            # ìƒˆë¡œìš´ hidden stateì™€ ì˜ˆì¸¡ outputê°’ì´ ì¶œë ¥ë©ë‹ˆë‹¤.\n",
        "            output, hidden = self.decoder(input, hidden, context)\n",
        "\n",
        "            #output = [batch size, output dim]\n",
        "\n",
        "            # ê°ê°ì˜ ì¶œë ¥ê°’ì„ outputs tensorì— ì €ì¥í•©ë‹ˆë‹¤.\n",
        "            outputs[t] = output\n",
        "            \n",
        "            # decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            # predictionsë“¤ ì¤‘ì— ê°€ì¥ ì˜ ì˜ˆì¸¡ëœ tokenì„ topì— ë„£ìŠµë‹ˆë‹¤.\n",
        "            # 1ì°¨ì› ì¤‘ ê°€ì¥ í° ê°’ë§Œì„ top1ì— ì €ì¥í•˜ë¯€ë¡œ 1ì°¨ì›ì€ ì‚¬ë¼ì§‘ë‹ˆë‹¤.\n",
        "            top1 = output.argmax(1) \n",
        "            # top1 = [batch size]\n",
        "            \n",
        "            # teacher forcingê¸°ë²•ì„ ì‚¬ìš©í•œë‹¤ë©´, ë‹¤ìŒ inputìœ¼ë¡œ targetì„ ì…ë ¥í•˜ê³ \n",
        "            # ì•„ë‹ˆë¼ë©´ ì´ì „ stateì˜ ì˜ˆì¸¡ëœ ì¶œë ¥ê°’ì„ ë‹¤ìŒ inputìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4WJZu-iCoCw"
      },
      "source": [
        "\n",
        "## Training the Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jB9OpTrI3L-"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "739iwlMiCv-Z"
      },
      "source": [
        "- ì´ˆê¸° ê°€ì¤‘ì¹˜ê°’ì€ $\\mathcal{N}(0, 0.01)$ì˜ ì •ê·œë¶„í¬ë¡œë¶€í„° ì–»ì—ˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-58y-hbI_aS",
        "outputId": "29227a2f-4531-4ad1-b4e0-86940ca96830"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean = 0, std = 0.01)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7855, 256)\n",
              "    (rnn): GRU(256, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(5893, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=5893, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ypW8ITHI5HC",
        "outputId": "9ed8dac6-47bc-4480-e5c8-b48450cc2268"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 14,220,293 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kx4GJX-DGMO"
      },
      "source": [
        "- optimizerí•¨ìˆ˜ë¡œëŠ” `Adam`ì„ ì‚¬ìš©í•˜ì˜€ê³ , loss functionìœ¼ë¡œëŠ” `CrossEntropyLoss`ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, `<pad>` tokenì— ëŒ€í•´ì„œëŠ” loss ê³„ì‚°ì„ í•˜ì§€ ì•Šë„ë¡ ì¡°ê±´ì„ ë¶€ì—¬í–ˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvnbqQPTMJ1K"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QZl4LTYMK6m"
      },
      "source": [
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1sBTQvvDdPy"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itnpa_9XMK9Z"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doH3lCKcDgA5"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efcpaqOtMK_V"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAVjT1R-MLBz"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIEaj7GuDpNG"
      },
      "source": [
        "### Train the model through multiple epochsPermalink"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3gyumXIMT9S",
        "outputId": "ba22601d-a15f-4467-b4fe-0b72fa3d1356"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 36s\n",
            "\tTrain Loss: 5.041 | Train PPL: 154.550\n",
            "\t Val. Loss: 5.141 |  Val. PPL: 170.908\n",
            "Epoch: 02 | Time: 0m 36s\n",
            "\tTrain Loss: 4.377 | Train PPL:  79.604\n",
            "\t Val. Loss: 5.104 |  Val. PPL: 164.637\n",
            "Epoch: 03 | Time: 0m 36s\n",
            "\tTrain Loss: 4.060 | Train PPL:  58.001\n",
            "\t Val. Loss: 4.731 |  Val. PPL: 113.397\n",
            "Epoch: 04 | Time: 0m 37s\n",
            "\tTrain Loss: 3.766 | Train PPL:  43.194\n",
            "\t Val. Loss: 4.479 |  Val. PPL:  88.112\n",
            "Epoch: 05 | Time: 0m 36s\n",
            "\tTrain Loss: 3.473 | Train PPL:  32.222\n",
            "\t Val. Loss: 4.165 |  Val. PPL:  64.397\n",
            "Epoch: 06 | Time: 0m 36s\n",
            "\tTrain Loss: 3.213 | Train PPL:  24.857\n",
            "\t Val. Loss: 3.995 |  Val. PPL:  54.303\n",
            "Epoch: 07 | Time: 0m 37s\n",
            "\tTrain Loss: 2.993 | Train PPL:  19.937\n",
            "\t Val. Loss: 3.856 |  Val. PPL:  47.268\n",
            "Epoch: 08 | Time: 0m 37s\n",
            "\tTrain Loss: 2.726 | Train PPL:  15.267\n",
            "\t Val. Loss: 3.880 |  Val. PPL:  48.448\n",
            "Epoch: 09 | Time: 0m 37s\n",
            "\tTrain Loss: 2.543 | Train PPL:  12.714\n",
            "\t Val. Loss: 3.810 |  Val. PPL:  45.146\n",
            "Epoch: 10 | Time: 0m 36s\n",
            "\tTrain Loss: 2.352 | Train PPL:  10.511\n",
            "\t Val. Loss: 3.768 |  Val. PPL:  43.309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yzt3w5UbMT_s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aafcb525-fd99-47c5-b0ce-6b21a1f1887f"
      },
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.703 | Test PPL:  40.569 |\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}