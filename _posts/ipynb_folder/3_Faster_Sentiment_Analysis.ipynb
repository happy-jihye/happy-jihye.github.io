{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3 - Faster Sentiment Analysis",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26nADTPFDL6F"
      },
      "source": [
        "# 3 - Faster Sentiment Analysis\r\n",
        "\r\n",
        "- Pytorch / TorchText\r\n",
        "- 기존의 tutorial 2에서는 정확도를 높이기 위한 다양한 model들을 제시했다면, 이번 tutorial에서는 computation 속도를 향상시킬 수 있는 [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759) 논문의 **FastText model**을 학습할 예정입니다.\r\n",
        "\r\n",
        "> 2021/03/16 Happy-jihye\r\n",
        "> \r\n",
        "> **Reference** : [pytorch-sentiment-analysis/3 - Faster Sentiment Analysis](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/3%20-%20Faster%20Sentiment%20Analysis.ipynb)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0jFZX0dEmMB"
      },
      "source": [
        "## 1. Preparing Data\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Lj415BkW090"
      },
      "source": [
        "!apt install python3.7\r\n",
        "!pip install -U torchtext==0.6.0\r\n",
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LymgkkdrkNc-"
      },
      "source": [
        "### 1) FastText\r\n",
        "- FastText 논문의 핵심 idea 중 하나는 input 문장의 n-gram을 계산하여 문장 끝에 추가하는 것입니다. bi-gram, tri-gram 등 다양한 n-gram이 있지만, 이 tutorial에서는 bi-gram을 사용하였습니다.\r\n",
        "- **bi-gram**은 문장 내에서 연속적으로 나타나는 word/token들의 쌍입니다.\r\n",
        "  - \"how are you ?\"를 예시로 들면, bi-gram은 \"how are\", \"are you\", \"you ?\"입니다.\r\n",
        "\r\n",
        "- 아래의 **generate_bigrams** 함수에서는 이미 토큰화된 문장에서 bi-gram을 한 내역들을 tokenized list 끝에 추가해주었습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNgYrzIllW4u"
      },
      "source": [
        "def generate_bigrams(x):\r\n",
        "  n_grams = set(zip(*[x[i:] for i in range(2)]))\r\n",
        "  for n_gram in n_grams:\r\n",
        "    x.append(' '.join(n_gram))\r\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pjrpc_ZAlj3n",
        "outputId": "e5ebb014-cda7-48ce-d4af-dbf9db2c9501"
      },
      "source": [
        "generate_bigrams(['This', 'film', 'is', 'terrible'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'film', 'is', 'terrible', 'is terrible', 'This film', 'film is']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "honYLyZOmh-7"
      },
      "source": [
        "- TorchText Field에는 **preprocessing argument**가 있습니다. 이 인자에 함수를 전달하면 token화된 문장들이 indexing 되기 전에 적용됩니다.\r\n",
        "- 이번 tutorial에서는 RNN을 사용하지 않으므로 include_closed 를 True로 설정할 필요가 없습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn9QsVChE48-"
      },
      "source": [
        "import torch\r\n",
        "from torchtext import data\r\n",
        "\r\n",
        "TEXT = data.Field(tokenize = 'spacy',\r\n",
        "                  tokenizer_language = 'en',\r\n",
        "                  preprocessing = generate_bigrams)\r\n",
        "\r\n",
        "LABEL = data.LabelField(dtype = torch.float) # pos -> 1 / neg -> 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R9bRwoyIia9"
      },
      "source": [
        "#### 2) IMDb Dataset\r\n",
        "- 5만개의 영화 리뷰로 구성된 dataset\r\n",
        "- IMDb dataset을 다운로드 받은 후, 이전에 정의한 Field(TEXT, LABEL)를 사용해서 데이터를 처리하였습니다.\r\n",
        "- torchtext.datasets의 [IMDB](https://pytorch.org/text/stable/datasets.html#imdb) 의 dataset에서 train_data, valid_data, test_data를 나눠주었습니다.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PadXXIFIKXg_"
      },
      "source": [
        "from torchtext import datasets\r\n",
        "import random\r\n",
        "\r\n",
        "SEED = 1234\r\n",
        "\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\r\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK61YhDIMgPm",
        "outputId": "8e803922-3d67-4bae-8793-041fe9a47b71"
      },
      "source": [
        "print(f'training examples 수 : {len(train_data)}')\r\n",
        "print(f'validations examples 수 : {len(valid_data)}')\r\n",
        "print(f'testing examples 수 : {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training examples 수 : 17500\n",
            "validations examples 수 : 7500\n",
            "testing examples 수 : 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMQDknQkNKGY"
      },
      "source": [
        "#### 3) Build Vocabulary and load the pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8r09NpIPPXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd9557c-4eb1-418f-a209-202b96bcb9f3"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\r\n",
        "\r\n",
        "TEXT.build_vocab(train_data, \r\n",
        "                 max_size = MAX_VOCAB_SIZE,\r\n",
        "                 vectors = \"glove.6B.100d\",\r\n",
        "                 unk_init = torch.Tensor.normal_)\r\n",
        "\r\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.36MB/s]                          \n",
            "100%|█████████▉| 399107/400000 [00:15<00:00, 27491.10it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnRLbdsMRKLX",
        "outputId": "dac4afaa-3551-4838-fcf8-36fe16c3c95c"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\r\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 25002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QucJLf94s9-q"
      },
      "source": [
        "### 4) Create the iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khgb9NUFYXXI"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "BATCH_SIZE = 64\r\n",
        "\r\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\r\n",
        "    (train_data, valid_data, test_data),\r\n",
        "    batch_size = BATCH_SIZE,\r\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBtU7KsqZ12O"
      },
      "source": [
        "## 2. Build Model\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjTQufXqoIBs"
      },
      "source": [
        "- 이번 tutorial에서는 RNN-model을 사용하지 않고 embedding layer와 linear layer, 2개의 layer만을 사용하기 때문에 parameter가 적습니다.\r\n",
        "\r\n",
        "![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/Faster_Sentiment_Analysis1.png?raw=1)\r\n",
        "\r\n",
        "- 위 그림에서 파란색에 해당하는 embedding layer에서 각 단어들을 embedding한 후, 분홍색 부분에서 모든 단어의 임베딩 값의 평균을 취합니다. 이후 계산한 평균값을 은색의 linear layer에 전달하면 됩니다.\r\n",
        "- 이때 평균은 avg_pool2d 함수를 이용하여 구할 수 있습니다. 문장들 자체는 1차원이지만, word embedding은 2차원의 그리드로 생각할 수 있으므로 avg_pool2d함수를 사용하여 단어의 평균값을 구할 수 있습니다.\r\n",
        "\r\n",
        "![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/Faster_Sentiment_Analysis2.png?raw=1)\r\n",
        "\r\n",
        "- **avg_pool2d** 함수는 embedded.shape[1] size의 filter를 사용합니다.\r\n",
        "\r\n",
        "![](https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/Faster_Sentiment_Analysis3.png?raw=1)\r\n",
        "\r\n",
        "- filter를 오른쪽으로 한칸씩 이동시켜가면서 평균을 계산할 수 있습니다.\r\n",
        "- 위의 예제에서의 element가 [4x5]의 tensor였다면, 평균을 구하고 난 후에는 [1x5]의 tensor를 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v8A8X6CbeuJ"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "class FastText(nn.Module):\r\n",
        "  def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\r\n",
        "    \r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\r\n",
        "\r\n",
        "    self.fc = nn.Linear(embedding_dim, output_dim)\r\n",
        "\r\n",
        "  def forward(self, text):\r\n",
        "\r\n",
        "    # text = [sentence length, batch size]\r\n",
        "    \r\n",
        "    embedded = self.embedding(text)\r\n",
        "\r\n",
        "    # embedded = [sentence length, batch size, embedding dim]\r\n",
        "\r\n",
        "    embedded = embedded.permute(1, 0, 2)\r\n",
        "\r\n",
        "    #embedded = [batch size, sentence length, embedding dim]\r\n",
        "\r\n",
        "    pooled = F.avg_pool2d(embedded, (embedded.shape[1],1)).squeeze(1)\r\n",
        "\r\n",
        "    # pooled = [batch size, embedding_dim]\r\n",
        "\r\n",
        "    return self.fc(pooled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN_0DpCqgQNx"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\r\n",
        "EMBEDDING_DIM = 100\r\n",
        "OUTPUT_DIM = 1\r\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\r\n",
        "\r\n",
        "model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6uSeSNhhdju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ffba061-bef4-436b-985f-0b498074dc47"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 2,500,301 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0id7ronsq5M"
      },
      "source": [
        "- tutorial2와 마찬가지로 미리 학습되어져있는 embedding vector를 사용하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDNIyfG8XVbo",
        "outputId": "4b13150d-395f-4a68-e0a5-7b43cf35d6d4"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\r\n",
        "\r\n",
        "print(pretrained_embeddings.shape)\r\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25002, 100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
              "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [-1.0530, -1.0757,  0.3903,  ...,  0.0792, -0.3059,  1.9734],\n",
              "        [ 0.5571, -0.3352,  0.0417,  ...,  0.0257,  1.1868, -0.8791],\n",
              "        [ 0.0484, -0.1010, -0.1440,  ..., -0.6414,  0.5251,  0.3069]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6YlSR57YSjl"
      },
      "source": [
        "- unknown token과 padding token은 embedding weight를 0으로 초기화합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTzvOsVCXtvO",
        "outputId": "70b65f2d-ea7c-46be-920b-145f45a15a31"
      },
      "source": [
        "# PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] : 1\r\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token] #0\r\n",
        "\r\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\r\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\r\n",
        "\r\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [-1.0530, -1.0757,  0.3903,  ...,  0.0792, -0.3059,  1.9734],\n",
            "        [ 0.5571, -0.3352,  0.0417,  ...,  0.0257,  1.1868, -0.8791],\n",
            "        [ 0.0484, -0.1010, -0.1440,  ..., -0.6414,  0.5251,  0.3069]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy-6lg_HhfSb"
      },
      "source": [
        "## 3. Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtXPbSXchiIy"
      },
      "source": [
        "#### optimizer\r\n",
        "- **Adam** 를 이용해서 model을 update하였습니다.\r\n",
        "  - 이전 tutorial에서 사용했던 **SGD**는 동일한 학습속도로 parameter를 업데이트하기 때문에 학습속도를 선택하기 어렵지만, Adam은 각 매개변수에 대해 학습속도를 조정해주기 때문에 자주 학습되는 parameter에 낮은 learning rate를 update하고 자주 학습되지 않는 parameter에 높은 learning rate를 update할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0vLv4drhyXL"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "optimizer =optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XxWUCDih_0z"
      },
      "source": [
        "#### loss function\r\n",
        "- loss function 으로는 **binary cross entropy with logits**을 사용하였습니다.\r\n",
        "- 0아니면 1의 label을 예측해야하므로 **sigmoid**나 **logit** function을 사용하였습니다.\r\n",
        "- [BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)는 sigmoid와 the binary cross entropy steps를 모두 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A9f5ysXikuR"
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZosE0TCpis_9"
      },
      "source": [
        "# GPU\r\n",
        "model = model.to(device)\r\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w4aPDkeixl_"
      },
      "source": [
        "**accuracy function**\r\n",
        "- sigmoid layer를 지나면 0과 1사이의 값이 나오는데, 우리가 필요한 값은 0,1의 label이므로 [nn.round](https://pytorch.org/docs/stable/generated/torch.round.html)를 이용하여 반올림하였습니다.\r\n",
        "- prediction 값과 label 값이 같은 것들이 얼마나 있는지를 계산하여 정확도를 측정하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruGAS62rjQoy"
      },
      "source": [
        "def binary_accuracy(preds, y):\r\n",
        "\r\n",
        "  rounded_preds = torch.round(torch.sigmoid(preds))\r\n",
        "  # rounded_preds : [batch size]\r\n",
        "  # y : batch.label\r\n",
        "  correct = (rounded_preds == y).float()\r\n",
        "  acc = correct.sum() / len(correct)\r\n",
        "  return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-Y-o_4kjkw2"
      },
      "source": [
        "### 1) Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLVdffp-jojk"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\r\n",
        "\r\n",
        "  epoch_loss = 0\r\n",
        "  epoch_acc = 0\r\n",
        "\r\n",
        "  model.train()\r\n",
        "\r\n",
        "  for batch in iterator:\r\n",
        "\r\n",
        "    # 모든 batch마다 gradient를 0으로 초기화\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    # batch of sentences인 batch.text를 model에 입력 (저절로 forward가 됨)\r\n",
        "    # predictions의 크기가 [batch size, 1]이므로 squeeze해서 [batch size]로 size를 변경해줘야 함 \r\n",
        "    predictions = model(batch.text).squeeze(1)\r\n",
        "\r\n",
        "    # prediction결과와 batch.label을 비교하여 loss값 계산 \r\n",
        "    loss = criterion(predictions, batch.label)\r\n",
        "\r\n",
        "    # 정확도 계산\r\n",
        "    acc = binary_accuracy(predictions, batch.label)\r\n",
        "\r\n",
        "    # backward()를 사용하여 역전파 수행\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    # 최적화 알고리즘을 사용하여 parameter를 update\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    epoch_loss += loss.item()\r\n",
        "    epoch_acc += acc.item()\r\n",
        "\r\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7aRqf7Cl9pu"
      },
      "source": [
        "### 2) Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBP2c1Q0mCXp"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "  epoch_loss = 0\r\n",
        "  epoch_acc = 0\r\n",
        "\r\n",
        "  # \"evaluation mode\" : dropout이나 batch nomalizaation을 끔\r\n",
        "  model.eval()\r\n",
        "\r\n",
        "  # pytorch에서 gradient가 계산되지 않도록 해서 memory를 적게 쓰고 computation 속도를 높임\r\n",
        "  with torch.no_grad():\r\n",
        "    \r\n",
        "    for batch in iterator :\r\n",
        "\r\n",
        "      predictions = model(batch.text).squeeze(1)\r\n",
        "      \r\n",
        "      loss = criterion(predictions, batch.label)\r\n",
        "      acc = binary_accuracy(predictions, batch.label)\r\n",
        "\r\n",
        "      epoch_loss += loss.item()\r\n",
        "      epoch_acc += acc.item()\r\n",
        "\r\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wt3HQYEp3b2"
      },
      "source": [
        "- epoch 시간을 계산하기 위한 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erTMajygmvuo"
      },
      "source": [
        "import time\r\n",
        "\r\n",
        "def epoch_time(start_time, end_time):\r\n",
        "  elapsed_time = end_time - start_time\r\n",
        "  elapsed_mins = int(elapsed_time / 60)\r\n",
        "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "  return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTZ9k2EhvO_6"
      },
      "source": [
        "### Train the model through multiple epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju0pXT9Ls6rk"
      },
      "source": [
        "- training을 한 결과 학습시간이 매우 줄어든 것을 확인할 수 있습니다. \r\n",
        "- 또한, 정확도를 통해 이번 모델이 이전 모델과 비슷한 성능을 내고 있음을 확인할 수 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGrnQs4FnHtE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d2808c5-90fd-4b09-ad31-d239a612a25c"
      },
      "source": [
        "N_EPOCHS = 5\r\n",
        "\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "\r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\r\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "\r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut3-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.516 | Train Acc: 82.01%\n",
            "\t Val. Loss: 0.459 |  Val. Acc: 83.84%\n",
            "Epoch: 02 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.391 | Train Acc: 87.25%\n",
            "\t Val. Loss: 0.378 |  Val. Acc: 86.19%\n",
            "Epoch: 03 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.315 | Train Acc: 89.50%\n",
            "\t Val. Loss: 0.331 |  Val. Acc: 87.60%\n",
            "Epoch: 04 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.268 | Train Acc: 91.05%\n",
            "\t Val. Loss: 0.307 |  Val. Acc: 88.27%\n",
            "Epoch: 05 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.233 | Train Acc: 92.13%\n",
            "\t Val. Loss: 0.288 |  Val. Acc: 88.79%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfCNhdYznr5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691bca9b-9105-49b2-ec48-77bf0ca6e4d3"
      },
      "source": [
        "model.load_state_dict(torch.load('tut3-model.pt'))\r\n",
        "\r\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\r\n",
        "\r\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.292 | Test Acc: 88.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LfVM9FTfD4w"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svY4CHEafGRs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd66a796-5de0-42af-f098-f2fdf412898a"
      },
      "source": [
        "import torch\r\n",
        "model.load_state_dict(torch.load('tut3-model.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9j3KhgufHAs"
      },
      "source": [
        "import spacy\r\n",
        "nlp = spacy.load('en_core_web_sm')\r\n",
        "\r\n",
        "def predict_sentiment(model, sentence):\r\n",
        "    model.eval()\r\n",
        "    tokenized = generate_bigrams([tok.text for tok in nlp.tokenizer(sentence)])\r\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\r\n",
        "    tensor = torch.LongTensor(indexed).to(device)\r\n",
        "    tensor = tensor.unsqueeze(1)\r\n",
        "    prediction = torch.sigmoid(model(tensor))\r\n",
        "    return prediction.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7VNWda9fHI3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238663b0-f504-4faa-ad72-0f8e5e29bd77"
      },
      "source": [
        "predict_sentiment(model, \"This film is terrible\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.9721719013432448e-07"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CErm_c3affiW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c8d21ed-f608-4a75-cab9-f51d5162e71b"
      },
      "source": [
        "predict_sentiment(model, \"This film is great\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWv1sugCfHP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd27fb98-85b2-4f34-bf5d-68c36f9f8a50"
      },
      "source": [
        "predict_sentiment(model, \"This movie is fantastic\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999998807907104"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    }
  ]
}