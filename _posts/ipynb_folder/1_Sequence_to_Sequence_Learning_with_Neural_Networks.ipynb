{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 - Sequence to Sequence Learning with Neural Networks",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsCHC88ibsh1"
      },
      "source": [
        "# 1 - Sequence to Sequence Learning with Neural Networks\n",
        "\n",
        "- Seq2Seq ì‹œë¦¬ì¦ˆì—ì„œëŠ” Pytorchì™€ torch textë¥¼ ì´ìš©í•˜ì—¬ í•˜ë‚˜ì˜ `seq`ë¥¼ ë‹¤ë¥¸ `seq`ë¡œ ë°”ê¾¸ëŠ” ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•  ì˜ˆì •ì…ë‹ˆë‹¤. \n",
        "- tutorial-1ì—ì„œëŠ” `ë…ì¼ì–´`ë¥¼ `ì˜ì–´`ë¡œ ë²ˆì—­í•˜ëŠ” translation modelì„ í•™ìŠµí•©ë‹ˆë‹¤. Seq2Seq model ëª¨ë¸ì€ ë²ˆì—­ ì™¸ì—ë„ ë‚´ìš© ìš”ì•½(Text Summarization), STT(Speech to Text)ë“±ì— ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "\n",
        "- ì´ë²ˆ ë…¸íŠ¸ë¶ì—ì„œëŠ” [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) paperì˜ ëª¨ë¸ì„ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•  ì˜ˆì •ì…ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ encoder-decoder modelì„ ì œì‹œí•œ ëª¨ë¸ë¡œ, ìì—°ì–´ ì²˜ë¦¬ì— ìˆì–´ êµ‰ì¥íˆ ì¤‘ìš”í•œ ë…¼ë¬¸ì´ë‹ˆ í•œë²ˆì¯¤ì€ ì½ì–´ë³´ì‹œëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤ :)\n",
        "\n",
        "> 2021/03/26 Happy-jihye ğŸŒº\n",
        "> \n",
        "> **Reference** : [pytorch-seq2seq/1 - Sequence to Sequence Learning with Neural Networks](https://github.com/bentrevett/pytorch-seq2seq)\n",
        "\n",
        "--- \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6cHb1ugpMAa"
      },
      "source": [
        "## Seq2Seq\n",
        "\n",
        "- ê°€ì¥ ì¼ë°˜ì ì¸ Seq2Seq ëª¨ë¸ì€ `encoder-decoder` ëª¨ë¸ì…ë‹ˆë‹¤. input ë¬¸ì¥ì„ RNNìœ¼ë¡œ single vectorë¡œ ì¸ì½”ë”©í•œ í›„, ì´ single vectorë¥¼ ë‹¤ì‹œ RNN ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ë””ì½”ë”©í•©ë‹ˆë‹¤.\n",
        "- single vectorëŠ” **context vector**ë¼ê³ ë„ ë¶ˆë¦¬ë©°, ì „ì²´ ì…ë ¥ ë¬¸ì¥ì˜ ì¶”ìƒì ì¸ í‘œí˜„ìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "![](/images/seq2seq1.png)\n",
        "\n",
        "**Encoder**\n",
        "- ìœ„ì˜ ì´ë¯¸ì§€ëŠ” ëŒ€í‘œì ì¸ ë²ˆì—­ ì˜ˆì œë¡œ, \"guten morgen\"ì´ë¼ëŠ” source ë¬¸ì¥ì€ ë…¸ë€ìƒ‰ì˜ `embedding layer`ë¥¼ ê±¸ì³  ì´ˆë¡ìƒ‰ì˜ `encoder`ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤. \n",
        "- `<sos>` tokenì€ *start of sequence*, <eos> tokenì€ *end of sequence*ì˜ ì•½ìë¡œ ë¬¸ì¥ì˜ ì‹œì‘ê³¼ ëì„ ì•Œë¦¬ëŠ” tokenì…ë‹ˆë‹¤. \n",
        "- Encoder RNNì€ ì´ì „ time stepì˜ hidden stateì™€ í˜„ì¬ time stepì˜ ebeddingê°’ì„ inputìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤. ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "  $h_t = \\text{EncoderRNN}(e(x_t), h_{t-1})$\n",
        "  - ì—¬ê¸°ì„œ input sentenceëŠ” $X = \\{x_1, x_2, ..., x_T\\}$ë¡œ í‘œí˜„ë˜ë©°, $x_1$ ì€ `<sos>`, $x_2$ ëŠ” `guten`ì´ ë©ë‹ˆë‹¤. \n",
        "  - ë˜í•œ ì´ˆê¸° hidden state, $h_0$ëŠ” 0ì´ ë˜ê±°ë‚˜ í•™ìŠµëœ parameterë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.\n",
        "\n",
        "- RNNë¡œëŠ” LSTM (Long Short-Term Memory)ë‚˜ GRU (Gated Recurrent Unit)ì™€ ê°™ì€ architectureë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "**context vector**\n",
        "- ìµœì¢… ë‹¨ì–´ì¸ $x_T$, `<eos>`ê°€ embedding layerë¥¼ í†µí•´ RNNì— ì „ë‹¬ë˜ë©´, ìš°ë¦¬ëŠ” ë§ˆì§€ë§‰ hidden stateì¸ $h_T$ì„ ì–»ì„ ìˆ˜ ìˆìœ¼ë©°, ì´ë¥¼ context vectorë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. \n",
        "- context vectorëŠ” ì „ì²´ ë¬¸ì¥ì„ ëŒ€í‘œí•˜ë©°, $h_T = z$ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "**Decoder**\n",
        "- ì´ì œ ìš°ë¦¬ëŠ” context vectorì¸ $z$ë¥¼ output/target sentenceë¡œ ë””ì½”ë”©í•´ì•¼í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ë¬¸ì¥ì˜ ì• ë’¤ì— `<sos>`ì™€ `<eos>` tokenì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "- ë””ì½”ë”© ê³¼ì •ì„ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "  \n",
        "  $s_t = \\text{DecoderRNN}(d(y_t), s_{t-1})$\n",
        "\n",
        "  - ì—¬ê¸°ì„œ í˜„ì¬ ë‹¨ì–´ë¥¼ embedding, $y$í•œ ê°’ì´ $d(y_t)$ì´ë©°, context vector $z = h_T$ëŠ” ì²«ë²ˆì§¸ hidden stateì¸ $s_0$ê³¼ë„ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "- ìš°ë¦¬ëŠ” decoderì˜ hidden state $s_t$ë¥¼ ë³´ë¼ìƒ‰ì˜ `Linear layer`ì— ë„£ìŒìœ¼ë¡œì¨ predictionê°’ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "  $\\hat{y}_t = f(s_t)$\n",
        "\n",
        "- ì´ë•Œ, decoderì˜ ë‹¨ì–´ëŠ” ê° time stepë‹¹ í•˜ë‚˜ì”© ì°¨ë¡€ëŒ€ë¡œ ìƒì„±ë©ë‹ˆë‹¤. decoderë¥¼ ê±°ì¹˜ë©´ì„œ ë§ì€ ë‹¨ì–´ë“¤ì´ ìƒì„±ì´ ë˜ëŠ”ë°, `<eos>` tokenì´ ì¶œë ¥ë˜ë©´ decodingì„ ë©ˆì¶¥ë‹ˆë‹¤.\n",
        "- ì˜ˆì¸¡ê°’  $\\hat{Y} = \\{ \\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_T \\}$ì„ ì‹¤ì œ target senteceì˜ ê°’ $Y = \\{ y_1, y_2, ..., y_T \\}$ê³¼ ë¹„êµí•˜ì—¬ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG8aw82pwUfS"
      },
      "source": [
        "## 1. Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MngZOT9T3jC2"
      },
      "source": [
        "!apt install python3.7\n",
        "!pip install -U torchtext==0.6.0\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKucp-663qub"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL5Yr3e63rDv"
      },
      "source": [
        "### **Tokenizers**\n",
        "- tokenizersëŠ” ë¬¸ì¥ì„ ê°œë³„ tokenìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "  - e.g. \"good morning!\" becomes [\"good\", \"morning\", \"!\"]\n",
        "- nlpë¥¼ ì‰½ê²Œ í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” python packageì¸ `spaCy`ë¥¼ ì´ìš©í•˜ì—¬, tokení™”ë¥¼ í•  ì˜ˆì •ì…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI8Csi-13rG4"
      },
      "source": [
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KugnoMoE3rJz"
      },
      "source": [
        "**Reversing the order of the words**\n",
        "\n",
        "  \n",
        "ì´ ë…¼ë¬¸ì—ì„œëŠ” ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ë°”ê¾¸ë©´ ìµœì í™”ê°€ ë” ì‰¬ì›Œì ¸ ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì§„ë‹¤ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ìœ„í•´ source ë¬¸ì¥ì¸ `ë…ì¼ì–´`ë¥¼ tokení™”ë¥¼ í•œ í›„ ì—­ìˆœìœ¼ë¡œ listì— ì €ì¥í–ˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E-XOILd3rMt"
      },
      "source": [
        "def tokenize_de(text):\n",
        "  return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
        "\n",
        "def tokenize_en(text):\n",
        "  return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeGj4lbd55Al"
      },
      "source": [
        "ë‹¤ìŒìœ¼ë¡œëŠ” **Field** ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjgQFUgu5_6C"
      },
      "source": [
        "SRC = Field(tokenize= tokenize_de,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize= tokenize_en,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYlgumKSZkq8"
      },
      "source": [
        "- datasetìœ¼ë¡œëŠ” [Multi30k dataset](https://github.com/multi30k/dataset)ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” ì•½ 3ë§Œê°œì˜ ì˜ì–´, ë…ì¼ì–´, í”„ë‘ìŠ¤ì–´ ë¬¸ì¥ì´ ìˆëŠ” ë°ì´í„°ì´ë©° ê° ë¬¸ì¥ ë‹¹ 12ê°œì˜ ë‹¨ì–´ê°€ ìˆìŠµë‹ˆë‹¤.\n",
        "- `exts`ëŠ” sourceì™€ targetìœ¼ë¡œ ì‚¬ìš©í•  ì–¸ì–´ë¥¼ ì§€ì •í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jnnu07gTZgQz",
        "outputId": "5b2e2a71-c129-491e-ee52-1e7a40455f9c"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts= ('.de', '.en'),\n",
        "                                                    fields = (SRC, TRG))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.21M/1.21M [00:02<00:00, 544kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46.3k/46.3k [00:00<00:00, 173kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66.2k/66.2k [00:00<00:00, 165kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DezyyLzCaOXs",
        "outputId": "e55c6bf7-31c9-4d67-bf98-73329849c6fe"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDcZx_nNaYas"
      },
      "source": [
        "- dataë¥¼ ì¶œë ¥í•´ë³¸ ê²°ê³¼, sourceë¬¸ì¥ì€ ì—­ìˆœìœ¼ë¡œ ì €ì¥ë˜ì–´ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrjrpOA4aWVY",
        "outputId": "b78ac968-10c4-4493-e3d5-64f658edf2e4"
      },
      "source": [
        "print(len(vars(train_data.examples[0])['src']))\n",
        "print(len(vars(train_data.examples[1])['src']))\n",
        "\n",
        "print(vars(train_data.examples[0]))\n",
        "print(vars(train_data.examples[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13\n",
            "8\n",
            "{'src': ['.', 'bÃ¼sche', 'vieler', 'nÃ¤he', 'der', 'in', 'freien', 'im', 'sind', 'mÃ¤nner', 'weiÃŸe', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n",
            "{'src': ['.', 'antriebsradsystem', 'ein', 'bedienen', 'schutzhelmen', 'mit', 'mÃ¤nner', 'mehrere'], 'trg': ['several', 'men', 'in', 'hard', 'hats', 'are', 'operating', 'a', 'giant', 'pulley', 'system', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0Vh7OEkal9X"
      },
      "source": [
        "### Build Vocabulary\n",
        "- `build_vocab`í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ê° tokenì„ indexingí•´ì¤ë‹ˆë‹¤. ì´ë•Œ, sourceì™€ targetì˜ vocabularyëŠ” ë‹¤ë¦…ë‹ˆë‹¤.\n",
        "- `min_freq`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì†Œ 2ë²ˆ ì´ìƒ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ë§Œ vocabularyì— ë„£ì–´ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ë•Œ, í•œë²ˆë§Œ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ëŠ” `<unk>` tokenìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.\n",
        "- ì´ë•Œ, vocabularyëŠ” **training set**ì—ì„œë§Œ ë§Œë“¤ì–´ì ¸ì•¼í•©ë‹ˆë‹¤. *(validation/test setì— ëŒ€í•´ì„œëŠ” ë§Œë“¤ì–´ì§€ë©´ ì•ˆë¨!!)* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKCsid6tbl_x"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJB1I4Rmbs5h",
        "outputId": "e4d4b01c-3147-49ca-b9a4-ae2b34ea0c78"
      },
      "source": [
        "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in source (de) vocabulary: 7855\n",
            "Unique tokens in target (en) vocabulary: 5893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH8PWXeRbxz0"
      },
      "source": [
        "### Create the iterators\n",
        "- `BucketIterator`ë¥¼ ì´ìš©í•˜ì—¬ batch sizeë³„ë¡œ tokenë“¤ì„ ë¬¶ê³ , ì–´íœ˜ë¥¼ ì½ì„ ìˆ˜ ìˆëŠ” tokenì—ì„œ indexë¡œ ë³€í™˜í•´ì¤ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmHuEKeygLnl"
      },
      "source": [
        "# for using GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr3oNGp_FX0W",
        "outputId": "c78de6f0-dfaa-463c-bd1a-bed466b073db"
      },
      "source": [
        "print(train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torchtext.datasets.translation.Multi30k object at 0x7f0410c961d0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4tWF2FNgTg_"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXjXGCCYG3X8"
      },
      "source": [
        "- ë‹¤ìŒì€ batch sizeê°€ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•´ ì´í•´í•´ë³´ê¸° ìœ„í•´ ì²«ë²ˆì§¸ batchë¥¼ ì¶œë ¥í•´ë³¸ ì˜ˆì œì…ë‹ˆë‹¤. `BucketIterator`ë¥¼ í†µí•´ batchë¼ë¦¬ ë¬¶ìœ¼ë©´ [sequence length, batch size]ë¼ëŠ” tensorê°€ ìƒì„±ë˜ë©°, ì´ tensorëŠ” train_dataë¥¼ batch_sizeë¡œ ë‚˜ëˆˆ ê²°ê³¼ê°’ë§Œí¼ ìƒì„±ë©ë‹ˆë‹¤.\n",
        "  - ì´ ì˜ˆì œì—ì„œëŠ” 128ì˜ í¬ê¸°ë¥¼ ê°€ì§„ batchê°€ ì´ 227ê°œ ìƒê¹ë‹ˆë‹¤.\n",
        "- ë˜í•œ, batchì—ì„œ `sequence length`ëŠ” ê·¸ batch ë‚´ì˜ ê°€ì¥ ê¸´ ë¬¸ì¥ì˜ ê¸¸ì´ë¡œ ê²°ì •ë˜ë©° ê·¸ë³´ë‹¤ ì§§ì€ ë¬¸ì¥ë“¤ì— ëŒ€í•´ì„œëŠ” `<pad>` tokenìœ¼ë¡œ ë‚¨ì€ tensorê°’ì´ ì±„ì›Œì§‘ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNhZ55rHJgd_",
        "outputId": "97d1d980-f06e-4c75-b7b7-38232f41831a"
      },
      "source": [
        "print(TRG.vocab.stoi[TRG.pad_token]) #<pad> tokenì˜ index = 1\n",
        "\n",
        "for i, batch in enumerate(train_iterator):\n",
        "    src = batch.src\n",
        "    trg = batch.trg\n",
        "\n",
        "    src = src.transpose(1,0)\n",
        "    print(f\"ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ text í¬ê¸°: {src.shape}\")\n",
        "    print(src[0])\n",
        "    print(src[1])\n",
        "\n",
        "    break\n",
        "\n",
        "print(len(train_iterator))\n",
        "print(len(train_iterator)*128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ text í¬ê¸°: torch.Size([128, 31])\n",
            "tensor([   2,    4, 4334,   14,   22,   69,   25,   66,    5,    3,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1], device='cuda:0')\n",
            "tensor([   2,    4, 1700,  118,  254,   23,  443,   10,  589,    0,   18,   98,\n",
            "          60,   16,    8,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1], device='cuda:0')\n",
            "torch.Size([128])\n",
            "227\n",
            "29056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMO69_r3geyB"
      },
      "source": [
        "## Building the Seq2Seq Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROkRbTVngkFH"
      },
      "source": [
        "### Encoder\n",
        "- EncoderëŠ” 2ê°œì˜ LSTM layerë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ë…¼ë¬¸ì—ì„œëŠ” 4ê°œì˜ layerë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ, ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” í•™ìŠµì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•´ 2ê°œì˜ layerë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.)\n",
        "- RNNì—ì„œëŠ” ì²«ë²ˆì§¸ layerì˜ hidden stateë¥¼ $h_t^1 = \\text{EncoderRNN}^1(e(x_t), h_{t-1}^1)$ë¡œ, ë‘ë²ˆì§¸ layerì˜ hidden stateë¥¼ $h_t^2 = \\text{EncoderRNN}^2(h_t^1, h_{t-1}^2)$ë¡œ í‘œí˜„í–ˆë‹¤ë©´, LSTMì€ `cell state`ì¸  $c_t$ë„ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤.\n",
        "\n",
        "![](/images/seq2seq2.png)\n",
        "\n",
        "- ë”°ë¼ì„œ LSTMì—ì„œì˜ multi-layer equationì„ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "  $(h_t^1, c_t^1) = \\text{EncoderLSTM}^1(e(x_t), (h_{t-1}^1, c_{t-1}^1))$\n",
        "  $(h_t^2, c_t^2) = \\text{EncoderLSTM}^2(h_t^1, (h_{t-1}^2, c_{t-1}^2))$\n",
        " \n",
        "- RNN architectureì— ëŒ€í•œ ì„¤ëª…ì€ [ì´ ê¸€](https://happy-jihye.github.io/nlp/2_Updated_Sentiment_Analysis/#lstm-long-short-term-memory)ì— ìì„¸íˆ ì ì–´ë†“ì•˜ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl32aWGThzhY"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src):\n",
        "\n",
        "    # src = [src len, batch size]\n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "    # embedded = [src len, batch size, emb dim]\n",
        "\n",
        "    outputs, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "    # hidden = [n layers * n directions, batch size, hid dim]\n",
        "    # cell = [n layer * n directions, batch size, hid dim]\n",
        "\n",
        "    # outputs = [src len, batch size, hid dim * n directions]\n",
        "    ## outputì€ ì–¸ì œë‚˜ hidden layerì˜ topì— ìˆìŒ\n",
        "\n",
        "    return hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RItpYm8jzzj"
      },
      "source": [
        "### Decoder\n",
        "- decoderë„ encoderì™€ ë§ˆì°¬ê°€ì§€ë¡œ 2ê°œì˜ LSTM layerë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. (ë…¼ë¬¸ì—ì„œëŠ” 4ê°œì˜ layerë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.)\n",
        "  ![](/images/seq2seq3.png)\n",
        "\n",
        "- ë‹¤ìŒì€ Decoderì˜ layerë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ì‹ì…ë‹ˆë‹¤.\n",
        "\n",
        "  $(s_t^1, c_t^1) = \\text{DecoderLSTM}^1(d(y_t), (s_{t-1}^1, c_{t-1}^1))\\\\\n",
        "  (s_t^2, c_t^2) = \\text{DecoderLSTM}^2(s_t^1, (s_{t-1}^2, c_{t-1}^2))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_ZanUngHECa"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, input, hidden, cell):\n",
        "        \n",
        "        # input = [batch size]\n",
        "        ## í•œë²ˆì— í•˜ë‚˜ì˜ tokenë§Œ decodingí•˜ë¯€ë¡œ forwardì—ì„œì˜ input tokenì˜ ê¸¸ì´ëŠ” 1ì…ë‹ˆë‹¤.\n",
        "        \n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "        # cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        # n directions in the decoder will both always be 1, therefore:\n",
        "        # hidden = [n layers, batch size, hid dim]\n",
        "        # context = [n layers, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        # inputì„ 0ì°¨ì›ì— ëŒ€í•´ unsqueezeí•´ì„œ 1ì˜ sentence length dimensionì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "        # input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        # embedding layerë¥¼ í†µê³¼í•œ í›„ì— dropoutì„ í•©ë‹ˆë‹¤.\n",
        "        # embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        \n",
        "        # output = [seq len, batch size, hid dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "        # cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        # seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        # output = [1, batch size, hid dim]\n",
        "        # hidden = [n layers, batch size, hid dim]\n",
        "        # cell = [n layers, batch size, hid dim]\n",
        "        \n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfGRH6zVLvk6"
      },
      "source": [
        "## Seq2Seq\n",
        "\n",
        "seq2seq modelì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "- encoderì— source(input) sentenceë¥¼ ì…ë ¥í•œë‹¤.\n",
        "- encoderë¥¼ í•™ìŠµì‹œì¼œ ê³ ì •ëœ í¬ê¸°ì˜ context vectorë¥¼ ì¶œë ¥í•œë‹¤.\n",
        "- context vectorë¥¼ decoderì— ë„£ì–´ ì˜ˆì¸¡ëœ target(output) sentenceë¥¼ ìƒì„±í•œë‹¤.\n",
        "\n",
        "![](/images/seq2seq4.png)\n",
        "\n",
        "- ì´ë²ˆ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Encoderì™€ Decoderì—ì„œì˜ layerì˜ ìˆ˜ì™€ hidden/cell dimensionsì„ ë™ì¼í•˜ê²Œ ë§ì¶°ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” í•­ìƒ ê·¸ë˜ì•¼í•˜ëŠ” í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆì§€ë§Œ, layerì˜ ê°œìˆ˜ë‚˜ ì°¨ì›ì„ ë‹¤ë¥´ê²Œ í•´ì¤€ë‹¤ë©´ ì¶”ê°€ì ìœ¼ë¡œ ìƒê°í•´ì¤„ ë¬¸ì œë“¤ì´ ë§ì•„ì§ˆ ê²ƒì…ë‹ˆë‹¤. \n",
        "  - ex) ì¸ì½”ë“œì˜ ë ˆì´ì–´ëŠ” 2ê°œ, ë””ì½”ë”ì˜ ë ˆì´ì–´ëŠ” 1ê°œë¼ë©´ context vectorì˜ í‰ê· ì„ ë””ì½”ë”ì— ë„˜ê²¨ì¤˜ì•¼í•˜ë‚˜?\n",
        "- targetë¬¸ì¥ê³¼ outputë¬¸ì¥ì˜ tensorëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "  ![](/images/seq2seq5.png)\n",
        "\n",
        "**Teacher Forcing**\n",
        "![](/images/seq2seq6.png)\n",
        "- teacher forcingì€ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ë””ì½”ë”ì˜ ì˜ˆì¸¡ì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹  ì‹¤ì œ ëª©í‘œ ì¶œë ¥ì„ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì»¨ì…‰ì…ë‹ˆë‹¤. ([ì°¸ê³ ](https://tutorials.pytorch.kr/intermediate/seq2seq_translation_tutorial.html)) ì¦‰, `target word`(Ground Truth)ë¥¼ ë””ì½”ë”ì˜ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì¤Œìœ¼ë¡œì¨ í•™ìŠµì‹œ ë” ì •í™•í•œ ì˜ˆì¸¡ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
        "- [ì°¸ê³ 2](https://blog.naver.com/PostView.nhn?blogId=sooftware&logNo=221790750668&categoryNo=0&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybFYGRt5I2hB"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # outputì„ ì €ì¥í•  tensorë¥¼ ë§Œë“­ë‹ˆë‹¤.(ì²˜ìŒì—ëŠ” ì „ë¶€ 0ìœ¼ë¡œ)\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # srcë¬¸ì¥ì„ encoderì— ë„£ì€ í›„ hidden, cellê°’ì„ êµ¬í•©ë‹ˆë‹¤.\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        # decoderì— ì…ë ¥í•  ì²«ë²ˆì§¸ inputì…ë‹ˆë‹¤.\n",
        "        # ì²«ë²ˆì§¸ inputì€ ëª¨ë‘ <sos> tokenì…ë‹ˆë‹¤.\n",
        "        # trg[0,:].shape = BATCH_SIZE \n",
        "        input = trg[0,:]  \n",
        "        \n",
        "        \n",
        "        '''í•œë²ˆì— batch_sizeë§Œí¼ì˜ tokenë“¤ì„ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°\n",
        "        ì¦‰, ì´ trg_lenë²ˆì˜ forë¬¸ì´ ëŒì•„ê°€ë©° ì´ forë¬¸ì´ ë‹¤ ëŒì•„ê°€ì•¼ì§€ë§Œ í•˜ë‚˜ì˜ ë¬¸ì¥ì´ decodingë¨\n",
        "        ë˜í•œ, 1ë²ˆì˜ forë¬¸ë‹¹ 128ê°œì˜ ë¬¸ì¥ì˜ ê° tokenë“¤ì´ ë‹¤ê°™ì´ decodingë˜ëŠ” ê²ƒ'''\n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            # input token embeddingê³¼ ì´ì „ hidden/cell stateë¥¼ decoderì— ì…ë ¥í•©ë‹ˆë‹¤.\n",
        "            # ìƒˆë¡œìš´ hidden/cell statesì™€ ì˜ˆì¸¡ outputê°’ì´ ì¶œë ¥ë©ë‹ˆë‹¤.\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "            #output = [batch size, output dim]\n",
        "\n",
        "            # ê°ê°ì˜ ì¶œë ¥ê°’ì„ outputs tensorì— ì €ì¥í•©ë‹ˆë‹¤.\n",
        "            outputs[t] = output\n",
        "            \n",
        "            # decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            # predictionsë“¤ ì¤‘ì— ê°€ì¥ ì˜ ì˜ˆì¸¡ëœ tokenì„ topì— ë„£ìŠµë‹ˆë‹¤.\n",
        "            # 1ì°¨ì› ì¤‘ ê°€ì¥ í° ê°’ë§Œì„ top1ì— ì €ì¥í•˜ë¯€ë¡œ 1ì°¨ì›ì€ ì‚¬ë¼ì§‘ë‹ˆë‹¤.\n",
        "            top1 = output.argmax(1) \n",
        "            # top1 = [batch size]\n",
        "            \n",
        "            # teacher forcingê¸°ë²•ì„ ì‚¬ìš©í•œë‹¤ë©´, ë‹¤ìŒ inputìœ¼ë¡œ targetì„ ì…ë ¥í•˜ê³ \n",
        "            # ì•„ë‹ˆë¼ë©´ ì´ì „ stateì˜ ì˜ˆì¸¡ëœ ì¶œë ¥ê°’ì„ ë‹¤ìŒ inputìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4WJZu-iCoCw"
      },
      "source": [
        "\n",
        "## Training the Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jB9OpTrI3L-"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "739iwlMiCv-Z"
      },
      "source": [
        "- ì´ˆê¸° ê°€ì¤‘ì¹˜ê°’ì€ $\\mathcal{U}(-0.08, 0.08)$ì˜ ì •ê·œë¶„í¬ë¡œë¶€í„° ì–»ì—ˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-58y-hbI_aS",
        "outputId": "a83c7e5d-35ec-4d93-ca72-a1fde228700a"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7855, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(5893, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ypW8ITHI5HC",
        "outputId": "1ccbdffa-69c0-4582-b0ec-611b79125964"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 13,899,013 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kx4GJX-DGMO"
      },
      "source": [
        "- optimizerí•¨ìˆ˜ë¡œëŠ” `Adam`ì„ ì‚¬ìš©í•˜ì˜€ê³ , loss functionìœ¼ë¡œëŠ” `CrossEntropyLoss`ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, `<pad>` tokenì— ëŒ€í•´ì„œëŠ” loss ê³„ì‚°ì„ í•˜ì§€ ì•Šë„ë¡ ì¡°ê±´ì„ ë¶€ì—¬í–ˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvnbqQPTMJ1K"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QZl4LTYMK6m"
      },
      "source": [
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1sBTQvvDdPy"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itnpa_9XMK9Z"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doH3lCKcDgA5"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efcpaqOtMK_V"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAVjT1R-MLBz"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIEaj7GuDpNG"
      },
      "source": [
        "### Train the model through multiple epochsPermalink"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3gyumXIMT9S",
        "outputId": "1391dac8-bef5-4fa4-8bdd-16ad50dd58a3"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 38s\n",
            "\tTrain Loss: 5.052 | Train PPL: 156.330\n",
            "\t Val. Loss: 5.009 |  Val. PPL: 149.767\n",
            "Epoch: 02 | Time: 0m 37s\n",
            "\tTrain Loss: 4.483 | Train PPL:  88.471\n",
            "\t Val. Loss: 4.817 |  Val. PPL: 123.627\n",
            "Epoch: 03 | Time: 0m 37s\n",
            "\tTrain Loss: 4.193 | Train PPL:  66.237\n",
            "\t Val. Loss: 4.675 |  Val. PPL: 107.187\n",
            "Epoch: 04 | Time: 0m 37s\n",
            "\tTrain Loss: 4.006 | Train PPL:  54.940\n",
            "\t Val. Loss: 4.543 |  Val. PPL:  93.994\n",
            "Epoch: 05 | Time: 0m 37s\n",
            "\tTrain Loss: 3.853 | Train PPL:  47.152\n",
            "\t Val. Loss: 4.419 |  Val. PPL:  83.004\n",
            "Epoch: 06 | Time: 0m 37s\n",
            "\tTrain Loss: 3.717 | Train PPL:  41.151\n",
            "\t Val. Loss: 4.419 |  Val. PPL:  83.041\n",
            "Epoch: 07 | Time: 0m 37s\n",
            "\tTrain Loss: 3.598 | Train PPL:  36.537\n",
            "\t Val. Loss: 4.235 |  Val. PPL:  69.030\n",
            "Epoch: 08 | Time: 0m 37s\n",
            "\tTrain Loss: 3.462 | Train PPL:  31.871\n",
            "\t Val. Loss: 4.120 |  Val. PPL:  61.552\n",
            "Epoch: 09 | Time: 0m 37s\n",
            "\tTrain Loss: 3.339 | Train PPL:  28.205\n",
            "\t Val. Loss: 4.060 |  Val. PPL:  57.994\n",
            "Epoch: 10 | Time: 0m 37s\n",
            "\tTrain Loss: 3.212 | Train PPL:  24.839\n",
            "\t Val. Loss: 4.076 |  Val. PPL:  58.898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yzt3w5UbMT_s"
      },
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}