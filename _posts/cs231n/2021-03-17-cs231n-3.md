---
date: 2021-03-08
title: "[CS231n] 03. Loss Functions and Optimization"

excerpt: "(1) Loss Functions(Multiclass SVM Loss/Regularization/Softmax Classifier) (2) Optimization(Gradient Descent/SGD)"

categories: 
  - cs231n
tags: 
  - cs231n
  - vision
# ëª©ì°¨
toc: true  
toc_sticky: true 
---

---

**Reference**

- [CS231n ê°•ì˜ë…¸íŠ¸ Convolutional Neural Networks](http://cs231n.github.io/convolutional-networks/)

- Lecture 03 - [( Slide Link,](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture3.pdf) [,Youtube Link )](https://www.youtube.com/watch?v=h7iBpEHGVNc&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&index=4)

- [ğŸŒº Happy-Jihye / CS 231n ê°•ì˜ ë…¸íŠ¸](https://happy-jihye.github.io/cs231n/cs231n-0/)


---



## 1) Loss Functions



> - ê°€ì¥ ì¢‹ì€ ê°€ì¤‘ì¹˜ í–‰ë ¬ Wë¥¼ ì–»ê¸°ìœ„í•´ ì–´ë–»ê²Œ íŠ¸ë ˆì´ë‹ ë°ì´í„°ë¥¼ í™œìš©í•´ì•¼í•˜ë‚˜?
> - ì´ë¥¼ ìœ„í•´ì„œëŠ” Wê°€ ì–´ë–¤ì§€ë¥¼ í‰ê°€í•  ìˆ˜ ìˆì–´ì•¼í•˜ê³ , ì§€ê¸ˆì˜ Wê°€ ì–¼ë§ˆë‚˜ ë‚˜ìœì§€ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ë§í•  ìˆ˜ ìˆì–´ì•¼í•¨. 
>   - ì´ë¥¼ ìœ„í•œ í•¨ìˆ˜ê°€  loss function
> - ê°€ì¥ ê´œì°®ì€ Wë¥¼ ì°¾ì•„ë‚˜ê°€ëŠ” ê³¼ì •ì´ optimization



<p align="center"><img title = "image-20210308160552155](https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/cs231n/images/lec3/image-20210308160552155.png?raw=1" width = "400" ></p>

-  L : datasetì—ì„œ ê° Nê°œì˜ ìƒ˜í”Œë“¤ì˜ Loss ë“¤ì˜ í‰ê· 
- ìœ„ í•¨ìˆ˜ëŠ” ì»´í“¨í„° ë¹„ì ¼ì™¸ì—ë„ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ



### (1) Multiclass SVM Loss

- binary SVM (Support Vector Machines)ì€ ë‘ê°€ì§€ì˜ classë¥¼ ë‹¤ë£¨ê¸° ìœ„í•œ ì•Œê³ ë¦¬ì¦˜ì´ì—ˆë‹¤ë©´, multiclass SVMì€ ì—¬ëŸ¬ê°œì˜ í´ë˜ìŠ¤ë¥¼ ë‹¤ë£¸
- L_ië¥¼ êµ¬í•˜ë ¤ë©´ ìš°ì„  trueì¸ ì¹´í…Œê³ ë¦¬ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì¹´í…Œê³ ë¦¬ Yì˜ í•©ì„ êµ¬í•¨  

- <p align="center"><img title = "image-20210308161722850" src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/cs231n/images/lec3/image-20210308161722850.png?raw=1" width = "400" ></p>

  - S_Y_i : training setì˜ ië²ˆì§¸ ì´ë¯¸ì§€ì˜ ì •ë‹µ í´ë˜ìŠ¤ì˜ ìŠ¤ì½”ì–´
  - S_j : ë¶„ë¥˜ê¸°ì˜ ì¶œë ¥ìœ¼ë¡œ ë‚˜ì˜¨ ì˜ˆì¸¡ëœ ìŠ¤ì½”ì–´

- <p align="center"><img title = "image-20210308161758436" src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/cs231n/images/lec3/image-20210308161758436.png?raw=1" width = "400" ></p>
  - L_i : ì´ ë¶„ë¥˜ê¸°ê°€ ì–¼ë§ˆë‚˜ ì´ìƒí•˜ê²Œ ë¶„ë¥˜ë¥¼ í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ
  - L = (2.9 + 0 + 12.9) / 3 = 5.27 ë§Œí¼ ì´ìƒí•˜ê²Œ ë¶„ë¥˜ë¥¼ í•¨
  - safety margin : 1ì€ ë­”ì§€? -> ì‚¬ì‹¤ 1ì´ë¼ëŠ” ìƒìˆ˜ê°€ ë³„ë¡œ ì¤‘ìš”í•œ ìˆ«ìëŠ” ì•„ë‹˜. ì†ì‹¤í•¨ìˆ˜ì˜ ìŠ¤ì½”ì–´ì˜ ì ˆëŒ€ì ì¸ ê°’ì´ ì¤‘ìš”í•œ ê²ƒì´ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ìŠ¤ì½”ì–´ê°„ì˜ ìƒëŒ€ì ì¸ ì°¨ì´ê°€ ì¤‘ìš”í•œ ê²ƒ.
  - ì¦‰, ì •ë‹µ ìŠ¤ì½”ì–´ê°€ ë‹¤ë¥¸ ìŠ¤ì½”ì–´ì— ë¹„í•´ ì–¼ë§ˆë‚˜ ë” í° ìŠ¤ì½”ì–´ë¥¼ ê°€ì§€ê³  ìˆëƒê°€ ì¤‘ìš”í•¨.
  - ê²°êµ­ì— 1ì´ë¼ëŠ” íŒŒë¼ë¯¸í„°ëŠ” ì—†ì–´ì§€ê³ , Wì˜ ìŠ¤ì¼€ì¼ì— ì˜í•´ ìƒì‡„ë  ê²ƒ

  

> Q1. What happens to loss if car scores change a bit?
>
> - Car scoreëŠ” ì´ë¯¸ ë‹¤ë¥¸ scoreë“¤ì— ë¹„í•´ ë§ì´ ë†’ìŒ. ì¦‰, ìŠ¤ì½”ì–´ë¥¼ ì¡°ê¸ˆ ë°”ê¾¼ë‹¤ í•´ë„ ì„œë¡œ ê°„ì˜ ê°„ê²©(margin)ì€ ìœ ì§€ë  ê²ƒì´ê³  lossëŠ” ê³„ì†í•´ì„œ 0ì¼ ê²ƒ
>
> Q2: what is the min/max possible loss?
>
> - ìµœì†Ÿê°’ì€ 0 (ëª¨ë“  classë“¤ì—ì„œ ì •ë‹µ ìŠ¤ì½”ì–´ê°€ ê°€ì¥ í¬ë©´ lossê°’ì€ 0ì¼ ê²ƒ)
> - ì†ì‹¤í•¨ìˆ˜ê°€ hinge lossëª¨ì–‘. ë§Œì•½ ì •ë‹µ ìŠ¤ì½”ì–´ê°€ ì—„ì²­ ì‘ì€ ìŒìˆ˜ê°’ì´ë¼ë©´ lossëŠ” ë¬´í•œëŒ€ì¼ ê²ƒ(ìµœëŒ“ê°’ì€ ë¬´í•œëŒ€)
>
> Q3: At initialization W is small so all s â‰ˆ 0. What is the loss? 
>
> - C-1 ì˜ í´ë˜ìŠ¤ë¥¼ ìˆœíšŒí•¨. ê·¼ë° ë¹„êµí•˜ëŠ” ë‘ ìŠ¤ì½”ì–´ê°€ ê±°ì˜ ë¹„ìŠ·í•˜ê¸° ë•Œë¬¸ì— marginê°’ê³¼ ìœ ì‚¬í•œ 1ì˜ ìŠ¤ì½”ì–´ë¥¼ ì–»ê²Œ ë  ê²ƒì´ê³  ì´ë“¤ì´ ìŒ“ì´ë‹¤ ë³´ë©´ lossëŠ” C-1ì´ ë¨
> - ë””ë²„ê¹… ì „ëµ,, traainingì„ ì²˜ìŒ ì‹œì‘í•  ë•Œ lossê°€ C-1ê°€ ì•„ë‹ˆë¼ë©´ ë²„ê·¸ê°€ ìˆëŠ” ê²ƒ -> ìˆ˜ì •ì„ í•´ì•¼í•¨ !!
>
> Q4: What if the sum was over all classes? (including j = y_i)
>
> - ë§Œì•½ì— ì •ë‹µì¸ ì•„ì´ê¹Œì§€ ë‹¤ í•˜ë©´ ì–´ë–»ê²Œ ë˜ë‚˜? -> lossê°€ 1 ëŠ˜ì–´ë‚¨.
> - ì‹¤ì œë¡œ ì •ë‹µ í´ë˜ìŠ¤ë§Œ ë¹¼ê³  ê³„ì‚°í•˜ëŠ” ì´ìœ ëŠ” lossê°€ 0ì´ì–´ì•¼ ì•„ë¬´ê²ƒë„ ìƒëŠ” ê²ƒì´ ì—†ë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸
>
> Q5: What if we used mean instead of sum?
>
> - í‰ê· ì„ ì·¨í•œë‹¤ëŠ” ê±´ ìŠ¤ì¼€ì¼ë§ì„ í•˜ëŠ” ê²ƒì¼ ë¿, ê²°ê³¼ëŠ” ë˜‘ê°™ìŒ
>
> Q6: What if we used<p align="center"><img title = "image-20210308221530532" src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/cs231n/images/lec3/image-20210308221530532.png?raw=1" width = "400" ></p>
>
> - ì†ì‹¤í•¨ìˆ˜ì˜ ê³„ì‚° ìì²´ê°€ ë‹¬ë¼ì§. ì´ë ‡ê²Œ ì‚¬ìš©í•˜ê¸°ë„ í•¨.(ì¥ë‹¨ì ì´ ìˆìŒ)
>
> Q7 :  Suppose that we found a W such that L = 0. Is this W unique?
>
> - ì—¬ëŸ¬ê°œì˜ Wê°€ ìƒê¸¸ ìˆ˜ë„ ìˆìŒ. Wì— scalingì„ í•´ë„ ë¹„ìŠ·í•  ê²ƒ



```python
def L_i_vectorized(X, y, W):
    scores = W.dot(X)
    margins = np.maximum(0, scores - scores[y] + 1)
    margins[y] = 0 #ì •ë‹µ classë§Œ 0ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ ì´ë¥¼ ë¬´ì‹œ
    loss_i = np.sum(margins)
    return loss_i
```



### (2) Regularization

> - training dataì— ê¼­ ë§ëŠ” Wë¥¼ ì°¾ëŠ”ê²Œ ì¢‹ì€ ê²ƒë§Œì€ ì•„ë‹˜,,
>
>   â€‹	-> test dataì— ì ìš©í–ˆì„ ë•Œ ê´œì°®ì€ ë¶„ë¥˜ê¸°ë¥¼ ë§Œë“œëŠ” ê²ƒì´ ëª©ì ì´ê¸° ë•Œë¬¸
>
> - ì•„ë˜ì˜ ì˜ˆì‹œì—ì„œ training dataì— fití•˜ê²Œ íŒŒë€ìƒ‰ì˜ êµ¬ë¶ˆêµ¬ë¶ˆí•œ ì„ ìœ¼ë¡œ í•™ìŠµì„ ì‹œì¼°ëŠ”ë°
>
>   ì‹¤ì œ testë°ì´í„°ì—ì„œ ì›í•˜ëŠ” ì„ ì€ ì´ˆë¡ìƒ‰ì˜ ì„ ì´ë¼ë©´ ì˜ëª» í•™ìŠµì„ ì‹œí‚¨ ê²ƒ!
>
> <p align="center"><img title = "image-20210308223459411" src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/cs231n/images/lec3/image-20210308223459411.png?raw=1" width = "400" ></p>
>
> - ë”°ë¼ì„œ ì´ë¥¼ ìœ„í•´ **Regularization term**í•­ì„ ì¶”ê°€!!
> - **Data Loss term** : ë¶„ë¥˜ê¸°ê°€ íŠ¸ë ˆì´ë‹ ë°ì´í„°ì— fitë˜ë„ë¡ í•¨
> - **Regularization term** : ëª¨ë¸ì´ ì¡°ê¸ˆ ë” ë‹¨ìˆœí•œ Wë¥¼ ì„ íƒí•˜ë„ë¡ ë„ì™€ì¤Œ 
> - ëŒë‹¤ : hyper-parameterë¡œì„œ ìš°ë¦¬ê°€ í•™ìŠµ ì „ì— ì„¤ì •ì„ í•´ì•¼í•¨ 

<p align="center"><img title = "image-20210308225533610" src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/cs231n/images/lec3/image-20210308225533610.png?raw=1" width = "400" ></p>



#### - L2 Regularization

- <p align="center"><img title = "image-20210308230509509" src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/cs231n/images/lec3/image-20210308230509509.png?raw=1" width = "400" ></p>
- ***L2 Regularization*** ëŠ” ëª¨ë“  x ì˜ ìš”ì†Œê°€ ê³¨ê³ ë£¨ ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ì›í•¨ -> w1ë³´ë‹¤ w2ê°€ ë” ì¢‹ì€ ê°€ì¤‘ì¹˜ë¼ íŒë‹¨
- ë°˜ë©´, ***L1 Regularization*** ì€ w1ê°€ w2ë³´ë‹¤ ì¢‹ì€ ê°€ì¤‘ì¹˜ë¼ê³  íŒë‹¨ (L1ì€ ì¼ë°˜ì ìœ¼ë¡œ sparseí•œ solutionì„ ì„ í˜¸í•¨. wì¤‘ 0ì˜ ê°’ì´ ë§ì„ ìˆ˜ë¡ ì„ í˜¸)
- **"ë³µì¡í•˜ë‹¤" : (L1 = 0ì´ ì•„ë‹Œ ìš”ì†Œë“¤ì´ ë§ë‹¤), (L2 = Wê°€ ê³ ë¥´ì§€ ì•Šë‹¤, ì „ì²´ì ìœ¼ë¡œ í¼ì ¸ìˆë‹¤ë©´ ë³µì¡í•˜ì§€ ì•Šì€ ê²ƒ)**



### (3) Softmax Classifier (Multinomial Logistic Regression)

<p align="center"><img title = "image-20210308231310504" src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/cs231n/images/lec3/image-20210308231310504.png?raw=1" width = "400" ></p>

- softmax func : í™•ë¥ ì´ê¸° ë•Œë¬¸ì— 0~1ì˜ ê°’ì„ ê°€ì§€ê³ , í•©ì¹˜ë©´ 1ì´ ë¨
- ëª©í‘œ : ì •ë‹µ í´ë˜ìŠ¤ì— í•´ë‹¹í•˜ëŠ” í™•ë¥ ì´ 1ì— ê°€ê¹Œì›Œì§€ë„ë¡ ë§Œë“œëŠ” ê²ƒ â­
- ê·¸ëƒ¥ í™•ë¥ ê°’ì„ ìµœëŒ€í™”ì‹œí‚¤ëŠ” ê²ƒë³´ë‹¤ í™•ë¥ ê°’ì˜ ë¡œê·¸ë¥¼ ì·¨í•œ ê°’ì„ ìµœëŒ€í™”ì‹œí‚¤ëŠ” ê²ƒì´ í¸í•˜ê¸° ë•Œë¬¸ì— ë¡œê·¸ë¥¼ ì“°ëŠ” ê²ƒ
- ì†ì‹¤í•¨ìˆ˜ëŠ” ì–¼ë§ˆë‚˜ ë‚˜ìœì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì´ë¯€ë¡œ ë¡œê·¸ê°’ì— ë§ˆì´ë„ˆìŠ¤ë¥¼ ë¶™ì„
- <p align="center"><img title = "image-20210308231805917" src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/cs231n/images/lec3/image-20210308231805917.png?raw=1" width = "400" ></p>

> Q1: What is the min/max possible loss L_i?
>
> - min : 0 (log 1ì´ ë˜ëŠ” ê²ƒì´ ëª©í‘œì´ë¯€ë¡œ, ì™„ë²½í•˜ê²Œ ë¶„ë¥˜ë¥¼ í–ˆë‹¤ë©´ Lossê°’ì€ 0ì´ ë  ê²ƒ)
> - max : ë¬´í•œëŒ€
> - but, ì»´í“¨í„°ì—ì„œ ìœ í•œ ì •ë°€ë„ë¥¼ ê°€ì§€ê³ ëŠ” ìµœëŒ“ê°’ì´ë‚˜ ìµœì†Ÿê°’ì„ ê°€ì§€ì§€ ëª»í•¨
>
> Q2: Usually at initialization W is small so all s â‰ˆ 0. What is the loss?
>
> - log(C)ê°€ ë¨



#### - SVM  vs Softmax

<p align="center"><img title = "image-20210308232227399" src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/cs231n/images/lec3/image-20210308232227399.png?raw=1" width = "400" ></p>

- ì •ë‹µ ìŠ¤ì½”ì–´ê°€ ì¶©ë¶„íˆ ë†’ê³ , ë‹¤ë¥¸ í´ë˜ìŠ¤ ìŠ¤ì½”ì–´ê°€ ì¶©ë¶„íˆ ë‚®ë‹¤ë©´ SVMì…ì¥ì—ì„œëŠ” ë§¤ìš° ë¶„ë¥˜ë¥¼ ì˜í•œ ê²ƒ!!
- ë‹¤ë§Œ, softmaxëŠ” í™•ë¥ ì„ 1ë¡œ ë§Œë“œëŠ” ê²ƒì´ ëª©í‘œì´ê¸° ë•Œë¬¸ì— ìµœëŒ€í•œ ì •ë‹µ í´ë˜ìŠ¤ì— í™•ë¥ ì„ ëª°ì•„ë„£ìœ¼ë ¤ê³  í•  ê²ƒì´ê³  ê·¸ ì™¸ì˜ í´ë˜ìŠ¤ëŠ” ìŒì˜ ë¬´í•œëŒ€ë¡œ ë§Œë“¤ë ¥ í•  ê²ƒ

> **ì¦‰, SVMì€ ì¼ì • ì„ (margins)ì„ ë„˜ê¸°ë§Œ í•˜ë©´ ë”ì´ìƒ ì„±ëŠ¥ ê°œì„ ì— ì‹ ê²½ì“°ì§€ ì•Šì§€ë§Œ,**
>
> **SoftmaxëŠ” ë”ë”ë” ì¢‹ê²Œ ì„±ëŠ¥ì„ ë†’ì´ë ¤ê³  í•¨**
>
> - ì‹¤ì œ ë”¥ëŸ¬ë‹ì—ì„œ ì´ ì†ì‹¤í•¨ìˆ˜ ê°„ì˜ ì„±ëŠ¥ì°¨ì´ëŠ” í¬ì§€ ì•ŠìŒ

<p align="center"><img title = "image-20210308232606457" src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/cs231n/images/lec3/image-20210308232606457.png?raw=1" width = "400" ></p>



---



## 2) Optimization

### (1) Random Search

- ì„ì˜ë¡œ ìƒ˜í”Œë§í•œ Wë¥¼ ì—„ì²­ ë§ì´ ëª¨ì•„ë‘ê³  lossë¥¼ ì¼ì¼ì´ ê³„ì‚°í•œ í›„ì— ì–´ë–¤ Wê°€ ê°€ì¥ ì¢‹ì€ì§€ë¥¼ ì‚´í´ë´„
- bad,,,,,,,,,,,,,,, ì•Œê³ ë¦¬ì¦˜...

```python
bestloss = float("inf")

for num in xrange(1000):
    W = np.random.rand(10, 3073) * 0.0001
    loss = L(X_train, Y_train, W)
    if loss < bestloss:
        bestloss = loss
        bestW = W
	print 'in attempt %d the loss was %f, best $f' % (num, loss, bestloss)

scores = Wbest.dot(Xte_cols)
Yte_predict = np.argmax(scores, axis = 0)
np.mean(Yte_predict == Yte) #return 0.1555
```

- CIFA-10ìœ¼ë¡œ ì‹¤í—˜ì„ í–ˆì„ ë•Œ 15.5%ì˜ accuracy
- ìµœì‹  ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì´ 95%ë¼ëŠ” ê±¸ ìƒê°í•˜ë©´ ë§¤ìš° ì•ˆì¢‹ì€ ì„±ëŠ¥,,



### (2) Follow the slope

<p align="center"><img title = "image-20210308233442777" src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/cs231n/images/lec3/image-20210308233442777.png?raw=1" width = "400" ></p>

- <p align="center"><img title = "image-20210308233758691" src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/cs231n/images/lec3/image-20210308233758691.png?raw=1" width = "400" ></p>

- <p align="center"><img title = "image-20210308233828473" src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/cs231n/images/lec3/image-20210308233828473.png?raw=1" width = "400" ></p>
- ë§¤ìš° ì•ˆì¢‹ì€ ë°©ë²•,,, -> ì§±ì§± ëŠë¦¬ê¸° ë•Œë¬¸ì— !!!!
- ì—„ì²­ í¬ê³  ê¹Šì€ ëª¨ë¸ì´ë¼ë©´ íŒŒë¼ë¯¸í„°ê°€ ì—„ì²­ ë§ê¸° ë•Œë¬¸ì—



> â­ **ìˆ˜ì¹˜ì ìœ¼ë¡œ í’€ì§€ ë§ê³ , í•´ì„ì ìœ¼ë¡œ í’€ê¸° !!**
>
> - ì¦‰, Wì˜ ëª¨ë“  ì›ì†Œë¥¼ ì‚´í´ë³´ë©´ì„œ gradientë¥¼ êµ¬í•˜ì§€ ì•Šê³ , 
>
>   gradientë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì‹ì„ ì°¾ì€ í›„ì— gradient dWë¥¼ ê³„ì‚°í•˜ê¸° !! 
>
> <p align="center"><img title = "image-20210308233956359" src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/cs231n/images/lec3/image-20210308233956359.png?raw=1" width = "400" ></p><p align="center"><img title = "image-20210308234011080" src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/cs231n/images/lec3/image-20210308234011080.png?raw=1" width = "400" ></p>
>
> 
>
> â­ In summary:
>
> - Numerical gradient: approximate, slow, easy to write 
>
> - Analytic gradient: exact, fast, error-prone 
>
>   => In practice: Always use analytic gradient, but check implementation with numerical gradient. This is called a gradient check.



### (3) Gradient Descent

- ì—„ì²­ í¬ê³  ë³µì¡í•œ ì‹ ê²½ë§ ì•Œê³ ë¦¬ì¦˜ì„ ì–´ë–»ê²Œ í•™ìŠµì‹œí‚¬ì§€ì— ëŒ€í•œ í•µì‹¬ ì•„ì´ë””ì–´
- Wì„ ì„ì˜ì˜ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•œ í›„, lossì™€ gradientë¥¼ ê³„ì‚°
- ì´í›„, ê°€ì¤‘ì¹˜ë¥¼ gradientì˜ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•¨(gradientê°€ ì¦ê°€í•˜ëŠ” ë°©í–¥ì´ê¸° ë•Œë¬¸ì— -ë°©í–¥ìœ¼ë¡œ í•´ì•¼ ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ìˆ˜ë ´ì„ í•˜ê²Œ ë¨)
- ì´ë•Œ step_size ëŠ” hyper-parameter, learning rateë¼ê³ ë„ í•¨ (ê°€ì¥ ì¤‘ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ ì¤‘ í•˜ë‚˜)



```python
# Vanilla Gradient Descent

while True : 
    weights_grad = evaluate_gradient(loss_fun, data, weights)
    weights += -step_size * weights_grad 
```



### (4) Stochastic Gradient Descent (SGD)

<p align="center"><img title = "image-20210308234931281" src="https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/cs231n/images/lec3/image-20210308234931281.png?raw=1" width = "400" ></p>

- imagenetì—ì„œ Nì€ 130ë§Œê°œì˜€ìŒ.. ì´ì²˜ëŸ¼ Nì´ ì•„ì£¼ í° ê²½ìš°ì—ëŠ” Lossë¥¼ ê³„ì‚°í•˜ëŠ” ë°ì— ì—„ì²­ ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦¼
- Loss : ê° ë°ì´í„° Lossì˜ Gradientì˜ í•©ì„
- ì¦‰, gradientë¥¼ ê³„ì‚°í•˜ë ¤ë©´ Në²ˆë§Œí¼ ë” ê³„ì‚°ì„ í•´ì•¼í•¨
- ê·¸ë˜ì„œ SGDì˜ ë°©ë²•ì„ ì”€ -> Minibatchë¼ëŠ” ì‘ì€ ì§‘í•©ì„ ë‚˜ëˆ ì„œ ê³„ì‚°

```python
# Vanilla Minibatch Gradient Descent

while True : 
    data_batch = sample_training_data(data, 256)
    weights_grad = evaluate_gradient(loss_fun, data_batch, weights)
    weights += -step_size * weights_grad 
```

- [Web Demo](http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/)

