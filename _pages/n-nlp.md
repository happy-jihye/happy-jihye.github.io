---
title: "Natural-Language-Processing"
permalink: /n-nlp/
toc_sticky: true
toc_ads : true
layout: single
---

---

### Papers

자연어처리 관련 paper들과 해당 논문 review입니다. (2022년 이전에 작성된 글은 다음 링크에서 확인하실 수 있습니다. [[링크]](https://happy-jihye.github.io/nlp/))

|  Date  |       keywords       |    Institute    | Paper                                                                                                                                                                               | review | Publication |
| :-----: | :------------------: | :--------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------: | :---------: |
| 2023-03 |  | |[A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)| [link-1](https://happy-jihye.github.io/nlp/nlp-22/), [link-2](https://happy-jihye.github.io/nlp/nlp-23/)|  |
| 2023-02 | LLaMA-1 |Meta|[LLaMA: Open and Efficient Foundation Language Models](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)| [link](https://happy-jihye.github.io/nlp/nlp-12/)|  |
| 2023-07	 | LLaMA-2 |Meta|[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288)| [link](https://happy-jihye.github.io/nlp/nlp-14/)| |
| 2024-04 | LLaMA-3 |Meta|[LLaMA3](https://llama.meta.com/llama3/)| [link](https://happy-jihye.github.io/nlp/nlp-14/)|  |
| 2024-01 | TinyLlama | |[TinyLlama: An Open-Source Small Language Model](https://arxiv.org/abs/2401.02385)| [link](https://happy-jihye.github.io/nlp/nlp-18/)|  |
| 2023-05 | GQA |Goggle Research|[GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245)| [link](https://happy-jihye.github.io/nlp/nlp-13/) | EMNLP 2023
| 2024-05 |  | Anthropic |[Mapping the Mind of a Large Language Model](https://www.anthropic.com/news/mapping-mind-language-model)| [link](https://happy-jihye.github.io/nlp/nlp-15/) | 
| 2023-05 | TinyStories | Microsoft |[TinyStories: How Small Can Language Models Be and Still Speak Coherent English?](https://arxiv.org/abs/2305.07759)| [link](https://happy-jihye.github.io/nlp/nlp-19/) | 
| 2023-06 | Phi-1 | Microsoft |[Phi-1: Textbooks Are All You Need](https://arxiv.org/abs/2306.11644)| [link](https://happy-jihye.github.io/nlp/nlp-16/) | 
| 2023-09 | Phi-1.5 | Microsoft |[Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463)| [link](https://happy-jihye.github.io/nlp/nlp-19/) | 
| 2023-12 | Phi-2 | Microsoft |[Phi-2: The surprising power of small language models](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)| [link](https://happy-jihye.github.io/nlp/nlp-19/) | 
| 2024-05 | Phi-3 | Microsoft |[Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219)| [link](https://happy-jihye.github.io/nlp/nlp-19/) | 
| 2023-07 |  | |[Chinchilla’s Death](https://espadrine.github.io/blog/posts/chinchilla-s-death.html)| [link](https://happy-jihye.github.io/nlp/nlp-17/)|  |
| 2024-03 |  | | [RAFT: Adapting Language Model to Domain Specific RAG](https://arxiv.org/abs/2403.10131) | [link](https://happy-jihye.github.io/nlp/nlp-25/)|  |

### **📒 LLM 시리즈: 기본 개념부터 최신 LLM 까지**

- [1] [DeepLearning 기본 이론](https://happy-jihye.github.io/nlp/nlp-20/)
- [2] [Network - CNN, RNN, LSTM, Transformer](https://happy-jihye.github.io/nlp/nlp-21/)
- [3-1] [A Survey of Large Language Models - 다양한 LLMs부터, Data, Architecture, Training 까지](https://happy-jihye.github.io/nlp/nlp-22/)
- [3-2] [A Survey of Large Language Models - Adaptation of LLMs](https://happy-jihye.github.io/nlp/nlp-23/)
- [4] [RAG (Retriever Augumented Generation)](https://happy-jihye.github.io/nlp/nlp-24/)
